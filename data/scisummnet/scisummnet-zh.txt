{"pid": "P99-1008", "zh_sum": "在非常大的语料库中查找部分我们提出了一种从整体中提取对象部分的方法（例如从“汽车”中提取“速度表”）。给定一个非常大的语料库，我们的方法在系统排名前50个单词中找到部分单词，准确率为55%。最终用户可以扫描部件列表，并将其添加到现有本体中（如WordNet），或者将其用作粗略语义词典的一部分。为了过滤出被视为品质（如驾驶能力）而非部件（如方向盘）的属性，我们删除了以后缀-ness、-ing和-ity结尾的单词。"}
{"pid": "J93-2003", "zh_sum": "统计机器翻译的数学：参数估计我们描述了一系列五个翻译过程的统计模型，并给出了算法，用于在一组相互翻译的句子对中估计这些模型的参数。我们定义了这样一对句子之间逐字对齐的概念。对于任何给定的一对这样的句子，我们的每个模型都为每个可能的逐字对齐分配了一个概率。我们给出了一个算法来寻找这些路线中最可能的路线。虽然算法不太理想，但由此获得的对齐很好地解释了这对句子中的逐字关系。我们有大量来自加拿大议会议事录的法语和英语数据。因此，我们的工作仅限于这两种语言；但我们认为，由于我们的算法具有最少的语言内容，因此它们可以很好地应用于其他语言对。我们还再次感到，由于我们算法的语言内容非常少，因此有理由认为，在任何足够大的双语语料库中，逐字对齐都是固有的。我们的统计机器翻译（SMT）模型侧重于词到词的翻译，并基于噪声信道方法。"}
{"pid": "W08-0336", "zh_sum": "优化中文分词以提高机器翻译性能以前的工作表明，中文分词对于机器翻译英语是有用的，但对不同的分词策略对机器翻译的影响仍然知之甚少。在本文中，我们证明了现有分割标准的优化分割并不总是产生更好的机器翻译性能。我们发现，其他因素，如汉语“词”的切分一致性和粒度，可能对机器翻译更为重要。基于这些发现，我们在条件随机场分割器中实现了直接优化机器翻译任务分割粒度的方法，提供了0.73 BLEU的改进。我们还表明，使用外部词汇和专有名词特征提高切分一致性可以使BLEU增加0.32。我们开发了基于CRF的斯坦福中文分词器，该分词器在中文树库的分词上进行训练，以保持一致性。我们通过调整单词粒度和提高切分一致性来增强机器翻译任务中的CRF s切分模型。"}
{"pid": "N10-1119", "zh_sum": "web衍生极性词典的可行性我们研究了从web半自动构建大型极性词典的可行性。我们首先描述了一个图传播框架，该框架的灵感来自于之前从词汇图构建极性词汇的工作（Kim和Hovy，2004；Hu和Liu，2004；Esuli和Sabastiani，2009；BlairGoldensohn等人，2008；Rao和Ravichandran，2009）。然后，我们运用这项技术构建了一个比之前研究的词汇要大得多的英语词汇。至关重要的是，这种网络衍生的词典不需要WordNet、词性标记器或其他情感分析系统典型的语言相关资源。因此，该词汇不仅限于特定的词类，例如出现在WordNet中的形容词，事实上还包含俚语、拼写错误、多词表达等。我们从定性和定量两个方面评估了来自英语文档的词汇，并表明它比之前研究过的词汇（包括来自WordNet的词汇）具有更好的性能。我们构建了一个图，其中节点是2000万个候选单词或短语，使用一组启发式方法选择，包括单词边界的频率和互信息。"}
{"pid": "P04-3022", "zh_sum": "将词汇句法和语义特征与最大熵模型相结合进行信息提取，提取实体之间的语义关系是一项具有挑战性的任务，因为注释数据较少，实体检测模块会导致错误。我们采用最大熵模型来结合来自文本的各种词汇、句法和语义特征。我们的系统在自动内容提取（ACE）评估中取得了有竞争力的结果。在这里，我们介绍了我们的一般方法，并描述了我们的ACE结果。我们使用两种特征：语法特征和基于单词的特征，例如，给定的一对网元在解析树中的路径和网元之间的单词n-gram。当我们结合各种特征时，结果会有所改善。我们在ACE RDC 2003语料库的24个关系亚型上获得了52.8的F测度。"}
{"pid": "J06-3003", "zh_sum": "语义关系的相似性至少有两种相似性。关系相似度是关系之间的对应关系，而属性相似度是属性之间的对应关系。当两个词具有高度的属性相似性时，我们称之为同义词。当两对词的关系相似度很高时，我们说它们的关系是相似的。例如，单词pair mason:stone类似于pair carpenter:wood。本文介绍了潜在关系分析（LRA），一种度量关系相似性的方法。LRA在许多领域都有潜在的应用，包括信息提取、词义消歧和信息检索。最近，信息检索的向量空间模型（VSM）已被用于衡量关系相似度，在374个大学级多项选择词类比问题上的得分达到47%。在VSM方法中，一对单词之间的关系以大型语料库中预定义模式的频率向量为特征。LRA从三个方面扩展了VSM方法：（1）自动从语料库中提取模式，（2）使用奇异值分解（SVD）平滑频率数据，（3）使用自动生成的同义词探索词对的变化。LRA在374个类比问题上的得分为56%，统计上相当于人类的平均得分57%。在相关的语义关系分类问题上，LRA取得了与VSM相似的效果。我们开发了一种基于语料库的方法来建模关系相似性，解决同义词和反义词之间的区别（以及其他任务）。我们描述了一种从大型语料库中提取名词对子序列模式的方法（潜在关系分析），使用查询扩展来提高搜索和特征选择的召回率，并使用降维来降低特征空间的复杂性。"}
{"pid": "W08-0309", "zh_sum": "机器翻译的进一步元评估本文分析了机器翻译系统在捷克语、英语、法语、德语、匈牙利语和西班牙语之间10种语言对的翻译质量。我们报告了30多个不同翻译系统的翻译质量，这些系统基于大规模手动评估，涉及数百小时的工作。我们使用系统的人类判断来分析翻译质量的自动评估指标，并报告了系统级和句子级与人类判断的相关性。我们通过测量注释者内部和注释者之间的一致性以及收集时间信息来验证我们的手动评估方法。因此，WMT 2008数据集的人类A符号是以二进制成对偏好的形式收集的，这非常容易制作。传统上，机器翻译质量的人类评分是以五分或七分Likert量表的绝对分数的形式收集的，但这种类型的注释的低可靠性数字引起了关注。"}
{"pid": "P08-1114", "zh_sum": "软语法约束基于层次短语的翻译在将语法添加到统计机器翻译中时，在利用语言分析与允许模型利用从并行训练数据中学习到的语言无动机映射之间存在权衡。之前的许多努力都解决了这一权衡问题，从致力于语言动机分析开始，然后找到适当的方法来软化这一承诺。我们提出了一种从另一个方向探索权衡的方法，从直接从对齐的平行文本学习的上下文无关翻译模型开始，然后添加基于源语言解析的软成分级约束。我们在中文和阿拉伯文到英文的翻译方面取得了实质性的改进。我们通过区分不同的组成句法类型来修改该方法，并为每种类型定义特征来计算短语是否匹配或跨越句法边界。我们发现它们的成分约束对语言对很敏感。"}
{"pid": "P87-1033", "zh_sum": "一种用于析取特征描述的统一方法虽然析取已经在几种基于统一的语法形式中使用，但现有的统一方法对于包含大量析取的描述并不令人满意，因为它们需要指数时间。本文描述了一种通过逐次逼近实现统一的方法，从而获得更好的平均性能。统一两个析取特征结构的一般问题是析取数的非多项式。我们提出了一种技术，对于每一组n个联合析取，首先检查单个析取与描述的确定部分的一致性，然后检查成对等的一致性，以获得完全一致性。"}
{"pid": "J98-1006", "zh_sum": "基于语料库的词义识别方法具有灵活性和通用性，但存在知识获取瓶颈。我们展示了如何使用基于知识的技术通过自动定位培训语料库来打开瓶颈。我们描述了一个统计分类器，它将主题上下文与局部线索相结合来识别词义。量词用于消除名词、动词和形容词的歧义。使用WordNet词汇关系形式的知识库在通用文本语料库中自动定位训练示例。将测试结果与手动标记的训练示例的结果进行比较。我们提出了一种使用单义亲属获取意义标记示例的方法。"}
{"pid": "P06-2066", "zh_sum": "轻度非投射依赖结构句法分析需要在表达性和复杂性之间取得良好的平衡，以便能够在不影响效率的情况下准确地分析自然出现的结构。在基于依赖关系的句法分析中，提出了一些限制允许结构类别的约束，如射影性、平面性、多平面性、良好嵌套性、间隙度和边度。虽然投射性通常被认为对自然语言语法的限制性太强，但不清楚其他哪种建议在表达性和复杂性之间取得了最佳平衡。在本文中，我们从理论上回顾和比较了不同的约束条件，并使用两个树库的数据进行了实验评估，研究了在不同约束条件下，树库中发现的结构比例有多大。结果表明，良好嵌套约束和不连续性参数约束的组合可以很好地拟合语言数据。"}
{"pid": "J02-3001", "zh_sum": "语义角色的自动标注我们提供了一个系统，用于识别语义关系或语义角色，由语义框架内的句子成分填充。给定一个输入句子和一个目标词和框架，系统用抽象语义角色（如Agent或Patient）或更多特定领域的语义角色（如Speaker、Message和Topic）标记成分。该系统基于对大约50000个句子进行训练的统计分类器，这些句子由FrameNet语义标记项目手工标注语义角色。然后，我们将每个训练句子解析成一棵句法树，并提取各种词汇和句法特征，包括每个成分的短语类型、语法功能及其在句子中的位置。这些特征结合了谓词动词、名词或形容词的知识，以及各种语义角色组合的先验概率等信息。我们使用各种词汇聚类算法来概括可能的角色填充。测试句子被解析，用这些特征注释，然后通过分类器传递。我们的系统在识别预编段成分的语义角色方面达到了82%的准确率。在同时分割成分和识别其语义角色这一更困难的任务中，该系统实现了65%的准确率和61%的召回率。我们的研究还允许我们比较不同特征和特征组合方法在语义角色标记任务中的有用性。我们还探索了角色标记与统计句法分析的集成，并尝试将其推广到训练数据中看不到的谓词。我们在FrameNet上提出了第一个SRL模型。"}
{"pid": "P02-1043", "zh_sum": "基于组合范畴语法的统计分析的生成模型本文比较了一种广泛覆盖的组合范畴语法（CCG）解析器的几种生成概率模型。这些模型在一个语料库上进行训练和测试，该语料库是通过将Penn树库树翻译成CCG范式派生得到的。根据对未标记单词-单词依赖性的评估，我们的最佳模型达到89.9%的性能，与Collins（1999）给出的语言表达性较差语法的数据相当。与Gildea（2001）相比，我们发现在建模单词-单词依赖关系方面有了显著的改进。CCG组合规则被编码为规则实例，以及许多处理标点和类型更改的附加规则。通过将组合类别的头添加到规则实例化特征中，根据本地规则实例化定义依赖性特征。"}
{"pid": "P95-1007", "zh_sum": "语料库统计符合名词复合词：一些实证结果对名词复合词的各种统计方法进行了实现和比较。结果支持两个主要结论。首先，概念关联的使用不仅可以实现广泛的覆盖，而且可以提高准确性。其次，基于依赖语法的分析模型比基于最深层成分的分析模型更准确，尽管后者在文献中更为普遍。我们提出了一种基于分类学或叙词表的无监督方法来估计相互竞争括号的频率。我们测试了从格罗利尔百科全书（一个800万单词的语料库）中提取的244个化合物的邻接和依赖模型。"}
{"pid": "J93-2004", "zh_sum": "建立一个大型的带注释的英语语料库：宾夕法尼亚树库"}
{"pid": "C00-1072", "zh_sum": "自动获取用于文本摘要的主题签名为了生成好的摘要，必须识别给定文本中最相关的部分。我们在本文中描述了一种自动训练主题签名的方法，即围绕主题组织的相关词集，以及相关权重，并使用我们在4个选定主题上创建的6194个TREC集合文本的签名进行说明。我们描述了主题签名与本体的可能集成及其在自动文本摘要系统上的评估。我们首先介绍了主题签名，这是一种与主题相关的摘要术语。"}
{"pid": "J05-4003", "zh_sum": "利用非平行语料库提高机器翻译性能我们提出了一种在可比较的非平行语料库中发现平行句子的新方法。我们训练了一个最大熵分类器，给定一对句子，该分类器可以可靠地确定它们是否是彼此的翻译。利用这种方法，我们从大型中文、阿拉伯文和英文非平行报纸语料库中提取平行数据。我们评估了提取数据的质量，结果表明它提高了最先进的统计机器翻译系统的性能。我们还表明，可以从一个非常小的平行语料库（100000个单词）开始，利用一个大型的非平行语料库，从头开始构建一个高质量的机器翻译系统。因此，我们的方法可以应用于只有稀缺资源可用的语言对。我们使用出版日期和基于向量的相似度（通过双语词典投影单词后）来识别相似的新闻文章。我们过滤掉长度差异大或单词重叠少的否定词（基于双语词典）。我们主要基于IBM Model 1 Alignment（Brown et al，1993）定义特性。"}
{"pid": "J98-1001", "zh_sum": "《词义消歧专刊：最新技术》导论我们简要介绍了词义消歧中使用的思想的历史。一般来说，过去的各种WSD方法可分为两类，即基于数据的方法和基于知识的方法。我们认为，对于许多已建立的HLT应用（例如机器翻译、信息提取和信息检索），词义歧义是一个中心问题。"}
{"pid": "P83-1007", "zh_sum": "在语篇语言学理论中，对名词短语进行统一描述，通常会将各种语言现象分为句法、语义或语用三类，似乎每一类中的现象相对独立于其他类别中的现象。然而，话语中的各种现象似乎都不能轻易地屈从于任何严格的句法、语义或语用解释。本文着重研究这类特殊现象——各种指称表达的使用，如名词短语和代词——并探讨它们与保持语篇连贯的机制之间的相互作用。即使是对有关明确描述和指称表达的文献进行一次偶然的调查，也会发现理论家（来自多个不同学科）提供的个人描述中存在缺陷，而且对句法、语义和语用因素在解释这些现象中所起的作用存在着深刻的混淆。我们进行的研究是为了理清其中的一些困惑，并为一个理论框架奠定基础，该框架可以解释语言使用的三个因素相互作用的各种话语现象。我们的研究所依赖的主要前提是，充分理解所讨论的现象所必需的概念不完全是句法、语义或语用的。本文的下一部分定义了两个层次的语篇连贯，并描述了它们在解释单数名词短语使用中的作用。为了说明在解释指称表达式的使用时各种因素的整合，第3节和第4节讨论了它们在其中一个层面上的使用，即局部层面上的使用。这一解释要求引入语篇中句子中心的概念，这一概念不能仅用句法、语义或语用因素来定义。第五节讨论了这两个层面与这些因素的相互作用及其对语篇中指称表达的影响。为了解决指称表达，我们发展了中心理论。"}
{"pid": "P09-1068", "zh_sum": "叙事图式及其参与者的无监督学习我们描述了一个无监督的系统，用于学习叙事图式，连贯的事件序列或集合（被捕（警察，嫌疑犯），定罪（法官，嫌疑犯）），其论点充满了在单词上定义的参与者语义角色（法官={法官，陪审团，法院}，警察={警察，代理人，当局}）。与之前在事件结构或语义角色学习方面的大多数工作不同，我们的系统不使用有监督的技术、手工构建的知识或预定义的事件或角色类。我们的无监督学习算法使用动词链中的共同传递参数来学习丰富的叙事事件结构和参数角色。通过联合处理这两项任务，我们改进了之前在叙事/框架学习方面的结果，并归纳出丰富的框架特定语义角色。我们描述了一个过程，通过使用无监督分布方法来学习共享共指参数的事件之间的关系，然后通过时间分类来诱导偏序，从而诱导一个由共同主角相关的偏序事件集。"}
{"pid": "N04-1043", "zh_sum": "基于词簇和区分性训练的名称标注我们提出了一种利用自动从大型未标注语料库中提取的分层词簇来扩充带注释的训练数据的技术。聚类成员在特征中进行编码，这些特征被合并到经过区分训练的标记模型中。主动学习用于选择培训示例。我们评估了命名实体标记技术。与最先进的基于HMM的名称查找器相比，该技术只需要13%的注释数据即可达到相同的性能水平。给定一个包含1000000个单词的大型带注释的训练集，与在相同材料上训练的最先进的HMM相比，该技术的错误减少了25%。我们使用Brown集群层次结构的前缀来生成不同粒度的集群。我们使用Brown算法进行聚类（Brown et al 1992）。"}
{"pid": "C00-1044", "zh_sum": "形容词方位和可分级性对句子主观性的影响主观性是一个语用、句子层面的特征，对信息抽取和信息检索等文本处理应用具有重要意义。我们研究了动态形容词、语义导向形容词和可分级形容词对简单主观性量词的影响，并确定它们是主观性的强预测因子。提出并评估了一种新的可训练方法，该方法在统计上结合了两个可分级性指标，补充了现有的自动分配方向标签的技术。与名词不同，许多形容词本质上是主观的，文本中形容词的数量与人类对其主观性的判断相关。我们报告了文本中形容词数量与人类主观判断之间的统计相关性。我们表明，自动检测可分级形容词是一种有用的主观性分类特征。"}
{"pid": "P09-1057", "zh_sum": "无监督词性标注的最小化模型我们描述了一种使用字典进行无监督词性标注的新方法，该方法使用整数规划明确搜索解释数据的最小模型，然后使用EM设置参数值。我们使用不同的标准标记集（一个45标记集和一个较小的17标记集）在标准测试语料库上评估了我们的方法，并表明我们的方法在这两种情况下都优于现有的最先进系统。我们通过使用整数程序（IP）的最小描述长度方法获得最佳结果（92.3%的单词标记准确率），该程序可以找到符合标记字典约束并覆盖观察数据的最小二元文法。我们提出了一种用于建模稀疏性的刚性机制，该机制通过转换类型的数量来最小化标记语法的大小。为了避免手动修剪标记字典的需要，我们建议通过应用于原始文本并受完整标记字典约束的模型最小化过程，从标记字典中自动过滤低概率标记。"}
{"pid": "P96-1021", "zh_sum": "统计机器翻译的多项式时间算法介绍了统计机器翻译的多项式时间算法。该算法可以代替当前统计翻译体系结构中昂贵、缓慢的最佳优先搜索策略。该方法采用了我们最近引入的随机括号转换语法（SBTG）模型来取代早期的单词对齐通道模型，同时保留了二元语言模型。根据我们的经验，新算法可以在不显著降低精度的情况下大幅提高速度。我们在汉英翻译中测试了我们的算法。"}
{"pid": "P06-1084", "zh_sum": "一种基于无监督语素HMM的希伯来语词法消歧方法词法消歧是将一组词法特征分配给文本中的每个单词的过程。当单词有歧义时（对该单词有几种可能的分析），必须应用基于单词上下文的消歧程序。本文讨论了希伯来语的形态消歧，它将语素以粘合和融合的方式组合成一个词。我们提出了一个无监督的随机模型-我们使用的唯一资源是形态学分析器-它处理由希伯来语词缀形态学引起的数据稀疏问题。我们提出了一种用于具有词缀形态学的语言的文本编码方法，其中对构词规则的知识（在希伯来语中非常有限）有助于消除歧义。我们采用HMM算法来学习和搜索这种文本表示，这样分割和标记就可以在一个步骤中并行学习。大规模评估的结果表明，这种学习改进了复杂标记集的消歧。我们的方法适用于其他具有词缀形态学的语言。我们不仅为每个单词提供词性，还提供完整的形态特征，如性别、数字、人称、结构、时态和词缀属性。我们提出了一种基于格的BaumWelch改进算法来处理分割歧义。"}
{"pid": "C04-1041", "zh_sum": "超标记在大范围CCG解析中的重要性本文描述了超标记在大范围CCG解析器中的作用，该解析器使用对数线性模型来选择分析。supertagger减少了执行模型估计的派生空间，减少了区分性训练所需的空间。它还显著提高了解析器的速度。我们表明，通过将supertagger与CCG语法和解析器紧密集成，可以大幅提高速度。这是我们所知的第一项成功地将supertagger与使用自动提取语法的完整解析器集成在一起的工作。我们还使用类别组合的约束进一步减少派生空间。结果是一个精确的覆盖范围广泛的CCG解析器，它比其他语言形式主义的可比系统快一个数量级。我们的分数表明了supertagging准确度与总体依赖恢复之间的对应关系。我们描述了CCG的两个对数线性解析模型：标准形式派生模型和依赖模型。CCG解析包括两个阶段：首先，supertagger将最可能的类别分配给每个单词，然后将少量组合规则，加上类型更改和标点符号规则，与CKY算法一起用于构建压缩图。我们提出了一种将supertagger与解析器集成的方法：最初为每个单词分配少量类别，如果解析器无法找到跨越分析，则请求更多类别。"}
{"pid": "W11-1801", "zh_sum": "2011年BioNLP共享任务概述2011年BioNLP共享任务是一项截至2011年3月的为期6个月的信息提取任务，社区广泛参与，共收到24个团队的46份最终提交。安排了五项主要任务和三项支持任务，其结果显示了细粒度生物医学领域信息提取的最新进展，并证明了提取方法在各个方面都得到了成功的推广。BioNLP 2011共享任务系列概括了这一点，定义了一系列涉及更多文本类型、域和目标事件类型的任务。"}
{"pid": "P91-1034", "zh_sum": "词义消歧使用统计方法我们描述了一种为单词分配词义的统计技术。一个词的实例通过询问该词出现的上下文来赋予意义。该问题被构造为与该实例在另一种语言中的翻译具有高度的互信息。当我们将这种分配意义的方法纳入统计机器翻译系统时，系统的错误率降低了13%。我们提出了一种词义消歧算法，该算法基于单个最不具信息性的上下文特征对法语目标词的英语翻译进行消歧。我们执行无监督的单词对齐，并从一组上下文特征中确定目标单词最合适的翻译。"}
{"pid": "P05-1067", "zh_sum": "基于语法的统计机器翻译（MT）旨在将统计模型应用于结构化数据。本文提出了一种基于概率同步依赖插入语法的统计机器翻译系统。同步依赖插入语法是在依赖树上定义的同步语法的一个版本。我们首先介绍了从平行语料库中归纳这种语法的方法。其次，我们描述了机器翻译任务的图形模型，它也可以看作是一个随机的树到树转换器。我们为该模型引入了一种多项式时间解码算法。我们使用NIST和Bleu自动机器翻译评估软件评估机器翻译系统的输出。结果表明，我们的系统在翻译速度和质量上都优于基于IBM模型的基线系统。我们的方法需要对两种语言之间的同构水平（词汇和/或结构）进行一些假设。我们提出了一个基于同步依赖插入语法（SDIG）的翻译模型，该模型处理一些非同构，但同时需要源和目标依赖结构。"}
{"pid": "W09-0424", "zh_sum": "Joshua：基于解析的机器翻译的开源工具包我们描述了Joshua，一个用于统计机器翻译的开源工具包。Joshua实现了同步上下文无关语法（SCFG）所需的所有算法：图表解析、n-gram语言模型集成、梁和立方体修剪以及k-best提取。该工具包还实现了后缀数组语法提取和最小错误率训练。它使用并行和分布式计算技术来实现可扩展性。我们证明了该工具包在WMT09法英翻译任务中达到了最先进的翻译性能。我们开发了基于语法的机器翻译系统Joshua，该系统实现了二阶期望半环的动态规划算法（Li和Eisner，2009），以有效地计算优化所需的梯度。"}
{"pid": "W04-0308", "zh_sum": "确定性依赖分析中的增量法确定性依赖分析是一种鲁棒有效的无限制自然语言文本句法分析方法。在本文中，我们分析了其增量处理的潜力，并得出结论，严格的增量处理在这个框架内是不可能实现的。然而，我们还表明，通过选择最佳的解析算法，可以最小化需要非增量处理的结构的数量。这一说法得到了实验证据的证实，实验证据表明，当在瑞典文本的随机样本上进行测试时，该算法对68.9%的输入实现了增量解析。"}
{"pid": "W07-2014", "zh_sum": "SemEval-2007任务15：临时时间关系识别临时任务提出了一种评估自动提取时间关系的简单方法。它通过定义允许对时间关系进行成对评估的三个子任务，避免了评估相互关联标签图的陷阱。该任务不仅允许直接的评估，还避免了完全时态解析的复杂性。时态信息处理是我们的评估活动TempEval推动的自然语言处理主题。TempEval07（Verhagen et al，2007）将14种TLINK关系整合为三种：之前、之后和。"}
{"pid": "H91-1060", "zh_sum": "定量比较英语语法语法覆盖率的过程我们定义了用于语法分析的PARSEVAL度量：标记精度、标记召回和标记F度量（分别为Prec、Rec和F1），它们基于语法分析输出中与金标准语法匹配的非终末项的数量。"}
{"pid": "J10-3003", "zh_sum": "生成短语和句子释义：数据驱动方法调查所有语言的使用者都熟悉释义任务。此外，自动生成或提取不同语言单元（单词、短语和句子）的语义等价物的任务是自然语言处理（NLP）的一个重要组成部分，并且越来越多地被用于提高几个NLP应用程序的性能。在这篇文章中，我们试图对数据驱动的短语和句子释义生成方法进行全面和独立于应用程序的调查，同时也表达了对释义在NLP研究领域的重要性和潜在用途的赞赏。本文还考察了近年来在人工和自动构建释义语料库方面所做的工作。我们还讨论了用于评估释义生成技术的策略，并简要探讨了释义生成的一些未来趋势。我们调查了各种数据驱动的释义技术，并根据它们使用的数据类型对其进行分类。"}
{"pid": "P06-1077", "zh_sum": "用于统计机器翻译的树到字符串对齐模板我们提出了一种新的基于树到字符串对齐模板（TAT）的翻译模型，该模板描述了源解析树和目标字符串之间的对齐。TAT能够生成终端和非终端，并在低级别和高级别执行重新排序。该模型基于语言语法，因为TAT是从单词对齐、源端解析的平行文本中自动提取的。为了翻译源语句，我们首先使用解析器生成源解析树，然后应用TATs将树转换为目标字符串。我们的实验表明，基于TAT的模型的性能明显优于用于基于短语模型的最先进解码器法老。我们执行派生级别的组合，以便在一个派生中混合不同类型的翻译规则。我们还将基于非句法PBSMT短语的统计机器翻译短语添加到树串翻译系统中。"}
{"pid": "C04-1046", "zh_sum": "机器翻译的置信度估计我们对机器翻译的置信度估计进行了详细的研究。研究了判断机器翻译输出是否正确的各种方法，包括整句话和单词。由于在这种情况下，正确性的概念并不直观明确，因此提出了不同的定义方法。我们展示了NIST 2003中文到英文机器翻译评估的数据结果。我们介绍了一个句子级QE系统，其中使用任意阈值将机器翻译输出分为好或坏。我们研究了句子和单词层面的特征，以预测翻译错误。"}
{"pid": "S12-1053", "zh_sum": "Semeval-2012任务8：跨语言文本蕴涵用于内容同步本文介绍了Semeval-2012中组织的关于跨语言文本蕴涵用于内容同步的第一轮任务。该任务旨在促进对不同语言文本语义推理的研究，同时针对实际应用场景。向参与者提供了不同语言对的数据集，其中必须确定多向蕴涵关系（向前、向后、双向、无蕴涵）。我们报告了用于评估的培训和测试数据、创建过程、参与系统（10个团队，92次跑步）、采用的方法和取得的结果。"}
{"pid": "P02-1019", "zh_sum": "改进拼写纠正的发音建模本文提出了一种在噪声信道模型中加入单词发音信息进行拼写纠正的方法。该方法建立了一个显式的单词发音错误模型。通过对单词之间的发音相似性进行建模，我们在拼写纠正方面比以前表现最好的模型有了实质性的性能改进。我们考虑一个发音变异模型，为发音词典中的每个规范发音生成多个发音。我们扩展了Brill和Moore（2000）的观点，考虑在单词发音和拼写错误方面对字母序列和电话序列进行编辑。我们使用噪声信道模型方法来确定编辑操作的类型和权重。由于拼写纠正模型需要对候选单词而不是候选发音进行排序，因此我们推导出一个错误模型，该模型根据单词的发音确定单词w拼写为非单词r的概率。"}
{"pid": "W99-0613", "zh_sum": "命名实体分类的无监督模型本文讨论了命名实体分类问题中未标记示例的使用。需要大量规则来覆盖域，这表明需要相当多的标记示例来训练分类器。然而，我们表明，使用未标记数据可以将监管要求减少到仅7条简单的“种子”规则。该方法利用了数据中的自然冗余：对于许多命名实体实例，名称的拼写和它出现的上下文都足以确定其类型。我们提出了两种算法。第一种方法使用了与（Yarowsky 95）相似的算法，并根据（Blum和Mitchell 98）进行了修改。第二种算法将为监督学习任务设计的boosting算法扩展到（Blum和Mitchell 98）提出的框架。我们通过向AdaBoost添加术语来扩展具有相互约束的分类器的使用，从而迫使分类器达成一致。"}
{"pid": "W97-0322", "zh_sum": "区分未标记文本中的词义本文描述了三种无监督学习算法的实验比较，它们可以区分未标记文本中歧义词的词义。本文描述的方法，McQuitty的相似性分析、Ward的最小方差方法和EM算法，仅基于文本中自动识别特征的值，将歧义词的每个实例分配给已知的意义定义。研究发现，这些方法和特征集在消除名词歧义方面比在消除形容词或动词歧义方面更为成功。总的来说，这些程序中最准确的是McQuitty的相似性分析与高维特征集的结合。我们提出了一种基于（dis）相似度的判别方法，计算目标词的每对实例之间的（dis）相似度。"}
{"pid": "P05-1033", "zh_sum": "基于短语的分层统计机器翻译模型我们提出了一个基于短语的统计翻译模型，该模型使用分层短语-包含子短语的短语。该模型形式上是一种同步上下文无关语法，但从没有任何语法信息的双文本中学习。因此，它可以被视为一种向基于句法的翻译系统的正式机制的转变，而无需任何语言承诺。在我们的实验中，使用BLEU作为度量标准，基于分层短语的模型比最先进的基于短语的系统法老（法老）相对提高了7.5%。我们在基于CFG的对数线性翻译模型中使用k-best解析算法来学习最大化BLEU的特征权重。我们注意到，每当我们组合两个动态规划项时，我们都需要通过合并跨越两个串联项的目标侧边界的任何语言模型特征的得分来对其表达的流畅性进行评分。为了更好地利用句法约束，同时仍然允许非句法翻译，我们为每个假设引入一个计数，并在假设与源端的句法边界完全匹配时将其累加。我们的机器翻译分级短语模型是从传统单词（Brown et al，1993）和基于短语（Koehn et al，2003a）模型的演变。"}
{"pid": "W07-0717", "zh_sum": "SMT的混合模型自适应我们描述了一种混合模型方法，该方法使用依赖于到混合组件的文本距离的权重来适应新领域的统计机器翻译系统。我们研究了这种方法的一些变体，包括跨域与动态适应；线性与对数线性混合物；语言与翻译模式适应；分配权重的不同方法；以及所适应的源单元的粒度。与最先进的非自适应基线系统相比，最佳方法可获得大约一个BLEU百分点的增益。我们得出结论，最好的方法是线性组合相同类型的子模型（例如，几个不同的TM或几个不同的LMs），而线性组合不同类型的模型（例如，混合TM和混合LM）。我们将域内短语表和通用短语表插在一起，在组合重叠条目之前，为表中的条目分配线性或对数线性权重；这现在是标准做法。"}
{"pid": "J04-1002", "zh_sum": "CorMet：一个基于计算语料库的传统隐喻提取系统CorMet是一个基于语料库的系统，用于发现概念之间的隐喻映射。它通过发现特定领域选择偏好的系统变化来实现这一点，这些选择偏好是从大型、动态挖掘的互联网语料库中推断出来的。隐喻将结构从源域转移到目标域，使目标域中的一些概念与源域中的概念隐喻等价。在源域中选择概念的动词倾向于在目标域中选择其隐喻对等物。这种规律性，通过浅层的语言分析可以发现，被用来发现隐喻的概念间映射，然后可以用来推断更高层次的常规隐喻的存在。大多数其他计算隐喻系统都使用小型的手工编码语义知识库，并处理一些示例。虽然CorMet的唯一知识库是WordNet（Fellbaum 1998），但它可以找到构成许多常规隐喻的映射，并且在某些情况下可以识别实例化这些映射的句子。CorMet测试了它找到主隐喻列表子集的能力（Lakoff、Espenson和Schwartz 1991）。CorMet系统动态挖掘特定领域的语料库，以发现不太频繁的用法并识别概念隐喻。我们展示了统计分析如何从语料库中自动检测和提取传统隐喻，尽管创造性隐喻仍然是一个诱人的挑战。"}
{"pid": "D09-1092", "zh_sum": "多语种主题模型主题模型是分析大型文本集合的有用工具，但以前仅在单语或最多双语上下文中应用。与此同时，维基百科（Wikipedia）等数十种语言的大量相互关联的文档现在已被广泛使用，需要能够描述多种语言内容的工具。我们引入了一个多语言主题模型，该模型可以发现跨多种语言的主题。我们使用两个大型语料库探索了该模型的特点，每个语料库都有十多种不同的语言，并展示了它在支持机器翻译和跟踪跨语言主题趋势方面的有用性。我们只需在两种语言中选择少量N个最可能的单词，然后将每个主题的这些集合的笛卡尔积添加到一组候选翻译中，即可检索到潜在翻译的列表。我们表明，只要主题对齐与非对齐文档的比例超过0.25，主题分布（以分布之间的Jensen-Shannon平均差异衡量）就不会显著降低。我们扩展了LDA的原始概念，以支持多语言主题模型（PLTM），既支持并行（如EuroParl），也支持部分可比较的文档（如Wikipedia文章）。多语言主题模型学习多种语言的主题，为每个主题的单语词汇创建特定于语言的分布元组。"}
{"pid": "N07-1038", "zh_sum": "多方面排序使用Good-greage算法，我们解决了在文本中分析多个相关观点的问题。例如，在餐馆审查中，这些意见可能包括食物、环境和服务。我们将此任务描述为一个多方面排序问题，目标是生成一组数字分数，每个方面一个。我们提出了一种算法，该算法通过对指定等级之间的依赖关系建模来联合学习各个方面的等级模型。该算法通过分析意见之间的元关系，如一致性和对比度，来指导个体愤怒的预测。我们证明了基于协议的联合模型比单独的排名模型更具表现力。我们的实证结果进一步证实了该模型的优势：该算法比个人排名和最先进的联合排名模型都有显著的改进。我们将基于对比RST关系的一致性模型与局部方面（或目标）模型相结合，以做出更明智的情绪分类总体决策。"}
{"pid": "D10-1048", "zh_sum": "用于共指消解的多通筛大多数共指消解模型使用一组约束或特征上的单个函数确定两个提及是否相互关联。这种方法可能会导致错误的决策，因为低精度特征往往会压倒数量较少的高精度特征。为了克服这个问题，我们提出了一种基于筛子的简单的共指体系结构，该结构应用确定性共指模型的层次，从最高精度到最低精度，一次一个。每一层构建在前一层的实体集群输出的基础上。此外，我们的模型通过共享同一集群中提及的属性（例如，性别和数量）来传播全局信息。这种谨慎的筛选保证了更强的功能优先于较弱的功能，并且每个决策都是使用当时可用的所有信息做出的。该框架是高度模块化的：可以插入新的共同引用模块，而无需对其他模块进行任何更改。尽管简单，但我们的方法在几个标准语料库上的表现优于许多最先进的有监督和无监督模型。这表明基于筛选的方法可以应用于其他NLP任务。我们基于规则的模型以较少的时间获得了有竞争力的结果。代词的候选先行词是基于话语显著性的概念排序的，这种概念有利于句法显著性和文档邻近性。我们开发了精确的无监督系统，利用简单但强大的语言原则。"}
{"pid": "J04-1005", "zh_sum": "Kappa统计：近年来，Kappa一致性系数已成为评估标记任务的指令间一致性的事实标准。在这个引爆装置中，我们强调了影响κ的问题，而社区在很大程度上忽视了这些问题。首先，我们讨论了κ的预期一致性成分的不同计算背后的假设。其次，我们讨论了患病率和偏倚如何影响κ测量。"}
{"pid": "P05-1034", "zh_sum": "依赖树翻译：句法信息短语SMT我们描述了一种新的统计机器翻译方法，它将源语言的句法信息与短语翻译的最新进展相结合。这种方法需要源语言依赖关系解析器、目标语言分词和无监督的词对齐组件。我们对齐一个平行语料库，将源依赖解析投影到目标句子上，提取依赖树翻译对，并训练一个基于树的排序模型。我们描述了一种高效的解码器，并表明将这些基于树的模型与传统的SMT模型结合使用提供了一种很有前景的方法，该方法将短语SMT的功能与解析器中可用的语言通用性结合起来。我们基于treelet的SMT系统基于大约460万个平行句子对进行培训，这些平行句子对来自不同来源，包括双语书籍、词典和网络出版物。我们将路径扩展到树元素，即依赖结构的任意连通子图，并提出了一个基于树元素对的模型。我们展示了使用目标语言语法片段treelets来提高短语翻译性能的成功经验。"}
{"pid": "C08-1022", "zh_sum": "基于分类器的二语英语介词和限定词错误纠正方法本文提出了一种非母语英语写作中介词和限定词错误的自动识别和纠正方法。我们表明，在母语文本上，这些词类的使用模型可以分别以70.06%和92.15%的准确率学习，并在二语写作的错误检测任务中给出了第一个结果。在二语英语介词和限定词自动纠错的背景下，我们注意到这个过程经常被拼写错误打断。"}
{"pid": "C02-1144", "zh_sum": "从文本广泛覆盖的词汇资源（如WordNet）中发现概念非常有用。然而，它们通常包含许多罕见的感觉，而缺少特定领域的感觉。我们提出了一种称为CBC（委员会聚类）的聚类算法，该算法可以自动从文本中发现概念。它最初发现了一组紧密的集群，称为委员会，它们分散在相似性空间中。将委员会成员的质心作为聚类的特征向量。我们继续将元素分配给它们最相似的集群。评估集群质量一直是一项艰巨的任务。我们提出了一种新的评估方法，该方法基于输出集群和从WordNet（答案键）提取的类之间的编辑距离。我们的实验表明，CBC在聚类质量上优于几种著名的聚类算法。互信息（MI）是一种信息论度量，已用于我们的词聚类方法。"}
{"pid": "J91-4003", "zh_sum": "生成词典在本文中，我将讨论与当前词汇语义学研究相关的四个主要主题：方法论、描述性覆盖、表示的充分性和表示的计算有用性。在解决这些问题时，我将讨论我认为词汇语义学界面临的一些核心问题，并提出解决这些问题的最佳方法。然后，我将提供一种分解词汇类别的方法，并概述一种词汇语义理论，该理论体现了共成分和类型强制的概念，以及语义描述的几个层次，其中语义负载在整个词汇中分布更均匀。我认为，如果词法分解是生成性的，那么它是可能的。与其假设一组固定的原语，我将假设一组固定数量的生成设备，这些设备可以被视为构造语义表达式。我发展了一种质量结构理论，这是一种词汇项目的表征语言，它使得词汇中的许多词汇歧义变得不必要，同时仍然解释了词汇所携带的系统多义现象。最后，我讨论了如何通过词汇继承理论将单个词汇结构整合到更大的词汇知识库中。这为我们提供了词汇全球组织的必要原则，使我们能够将自然语言词汇完全整合到一个概念整体中。我们提出了生成词典理论（GLT），该理论可以说是利用了语言学和概念学的方法，提供了一个从语言学研究和人工智能技术的整合中产生的框架。"}
{"pid": "P09-2012", "zh_sum": "树替换语法的贝叶斯学习树替换语法（TSG）比上下文无关语法（CFG）有许多优点，但很难学习。过去的方法都求助于启发式。在本文中，我们学习一个TSG使用吉布斯抽样与非参数之前控制子树大小。学习的语法在解析精度上明显优于启发式提取的语法。"}
{"pid": "P02-1017", "zh_sum": "一种用于改进语法归纳的生成成分上下文模型我们提出了一种用于自然语言语法无监督归纳的生成分布模型，该模型显式地模拟了成分产出和上下文。与以前的无监督系统相比，使用EM进行参数搜索可以产生更高质量的分析，从而在ATIS语料库上提供最佳的无监督分析结果。对长度相当的Penn treebank句子进行的实验表明，在非平凡括号中，F1更高，为71%。我们比较了作为输入数据的分布式诱导和实际词性标记，并研究了对基本模型的扩展。我们讨论了系统产生的错误，将系统与以前的模型进行了比较，并讨论了此任务的上界、下界和稳定性。我们从完整的《华尔街日报》树库和其他《华尔街日报》新闻专线中归纳词类。"}
{"pid": "N04-1021", "zh_sum": "统计机器翻译的特征大杂烩我们描述了一种快速实验统计机器翻译的方法，我们使用这种方法向基线系统添加大量的特征，利用各种句法表示级别的特征。在对数线性模型中结合特征值，从n-最佳列表中选择得分最高的候选翻译。特征权重直接针对BLEU评估指标对保留数据进行优化。我们给出了在句法表示的每个层次上选择一小部分特征的结果。在2003年约翰·霍普金斯大学统计机器翻译暑期研讨会上，对大量功能进行了测试，以发现哪些功能可以改进最先进的翻译系统，唯一能带来“真正显著改进”的功能是模型1分数。研究了将句法结构集成到最先进的统计机器翻译系统中的效果。"}
{"pid": "W09-0441", "zh_sum": "流利程度或HTER？使用可调机器翻译度量探索不同的人类判断机器翻译（MT）评估度量传统上是通过机器翻译输出的分数与人类对翻译性能的判断之间的相关性来评估的。不同类型的人类判断，如流利性、充分性和HTER，衡量机器翻译性能的不同方面，这些方面可以通过自动机器翻译度量来捕获。我们通过使用一种新的可调机器翻译度量：TER Plus来探索这些差异，该度量使用可调参数扩展了翻译编辑率评估度量，并结合了词法、同义词和释义。TER Plus被证明是NIST的metrics MATR 2008挑战赛中的顶级指标之一，在Pearson和Spearman相关性方面具有最高的平均排名。将TER Plus优化到不同类型的人类判断中，可以显著改善不同类型编辑的相关性和权重的有意义变化，显示出人类判断类型之间的显著差异。我们以类似的方式扩展了TER算法，生成了一个新的评估指标TER plus（TERp），它允许调整编辑成本，以最大限度地提高与人类判断的相关性。"}
{"pid": "W00-1427", "zh_sum": "健壮的应用形态学生成在实际的自然语言生成系统中，拥有一个单独的组件来处理形态学处理通常是有利的。我们提供了这样一个组件：一个基于有限状态技术的快速而健壮的英语形态学生成器，该生成器根据引理、词性和所需屈折类型的规范生成单词形式。我们描述了如何在英文报纸文本自动简化原型系统中使用该形态学生成器，并讨论了在该应用程序中生成无限制文本时遇到的实际形态学和正交问题。"}
{"pid": "W97-0313", "zh_sum": "基于语料库的语义词典构建方法语义知识对于自然语言处理系统来说是一个巨大的资产，但通常为每个应用程序手工编码。虽然一些语义信息在通用知识库中可用，如WordNet和Cyc，但许多应用程序需要特定领域的词典，以表示特定主题的单词和类别。本文提出了一种基于语料库的方法，可用于构建特定类别的语义词典。系统的输入是一组小的类别种子词和一个具有代表性的文本语料库。输出是与类别关联的单词的排序列表。然后，用户查看排名靠前的单词，并决定哪些单词应该输入语义词典。在五个类别的实验中，用户通常会在10-15分钟内发现每个类别大约60个单词，以构建核心语义词典。我们发现连词或同位语中的名词往往是语义相关的。我们建议使用连词和同位语数据对名词进行聚类；我们通过查看特定NP每侧最近的NP来近似此数据。我们还对与某一特定类别相关但不属于该类别的词语给予奖励。"}
{"pid": "E06-1043", "zh_sum": "我们研究了一类习语表达式的词汇和句法灵活性。我们开发了利用这些语言特性的度量，并证明这些基于语料库的统计度量可以成功地用于区分惯用组合和非惯用组合。我们还提出了一种自动确定特定习语可以出现在哪些句法形式中的方法，因此应该包括在其词汇表示中。为了衡量固定性，我们使用词汇、句法和整体固定性的统计指标。我们提出了十几种可能的动词-宾语对句法形式（基于被动化、限定词和宾语复数化），并使用基于语料库的统计方法来确定规范形式。"}
{"pid": "W97-0119", "zh_sum": "从非平行语料库中查找术语翻译我们提供了一个统计单词特征，即单词关系矩阵，它可以用于跨语言组从非平行语料库中查找翻译的单词和术语对。在线词典条目被用作种子词，根据相关度量为未知词生成词关系矩阵。然后将单词关系矩阵映射到语料库中，以找到翻译对。当只统计最优秀的候选人时，翻译准确率约为30%。尽管如此，前20名候选输出在人工翻译的准确性上平均提高了50.9%。在我们的工作中，将一个翻译模型应用于一对不相关的语言（英语/日语），随机选择测试词，其中许多是多词术语，当只提出最热门的候选词时，精度大约为30%。"}
{"pid": "P06-1015", "zh_sum": "Espresso：利用泛型模式自动获取语义关系在本文中，我们提出了Espresso，这是一种弱监督、通用且精确的语义关系获取算法。主要贡献有：i）通过使用Web过滤不正确的实例来利用泛型模式的方法；以及ii）实现过滤算法的模式和实例可靠性的原则性度量。我们将意式浓缩咖啡与各种最先进的系统、不同规模和体裁的语料库进行实证比较，以提取各种一般和具体的关系。实验结果表明，我们对泛型模式的利用大大提高了系统的查全率，但对整体查准率的影响很小。在模式归纳步骤中，我们的系统根据到目前为止提取的所有实例的模式的加权逐点互信息PMI计算每个候选模式的可靠性得分。我们以bootstrapping的方式归纳特定的可靠模式，用于实体关系提取。我们的最小监督浓缩咖啡算法是用一个单一集合初始化的，该集合混合了异质类型的种子，如leader panel和oxygen water，它们分别对应于Keet和Artale（2008）分类学中关系的成员和子数量。"}
{"pid": "J07-2003", "zh_sum": "基于层次短语的翻译我们提出了一个统计机器翻译模型，该模型使用包含子短语的层次短语短语。该模型形式上是一种同步上下文无关语法，但从没有任何语法注释的平行文本中学习。因此，它可以被视为结合了基于句法的翻译和基于短语的翻译的基本思想。我们详细描述了系统的训练和解码方法，并对其翻译速度和翻译精度进行了评估。使用BLEU作为翻译准确性的衡量标准，我们发现我们的系统的性能明显优于对齐模板系统，这是一种最先进的基于短语的系统。基于层次短语的模型利用层次短语对统计机器翻译进行了改进，它不仅使用短语来学习本地翻译，而且还使用层次短语来捕获词和子短语的重新排序，可以覆盖很大的范围。"}
{"pid": "N04-1019", "zh_sum": "评估摘要中的内容选择：金字塔方法我们提出了一种基于经验的方法来评估摘要中的内容选择。它融合了这样一种思想，即对于一组文档，不存在单一的最佳模型摘要。我们的方法量化了要传达的事实的相对重要性。我们认为，它是可靠的、预测性的和诊断性的，因此大大改善了目前在文件理解会议上使用的人类评估方法的缺点。我们提出了一种手动评估方法，该方法基于这样一种想法，即对于一组文档，没有单一的最佳模型摘要。"}
{"pid": "C04-1024", "zh_sum": "基于位向量的高歧义上下文无关语法的高效解析提出了一种基于位向量的CKY风格上下文无关语法分析器。解析器计算大型树库语法和长输入句子的完整可能分析集的紧凑解析林表示。解析器使用位向量操作来并行基本解析操作。当需要所有分析而不仅仅是最可能的分析时，解析器特别有用。我们应用维特比算法，利用其处理高度歧义语法的能力。"}
{"pid": "N03-1033", "zh_sum": "具有循环依赖网络的特征丰富的词性标记我们提出了一种新的词性标记器，它展示了以下思想：（i）通过依赖网络表示明确使用前后标记上下文，（ii）广泛使用词汇特征，包括对多个连续词的联合条件作用，（iii）在条件对数线性模型中有效使用先验知识，以及（iv）对未知单词特征进行细粒度建模。综合运用这些思想，生成的标记器在Penn Treebank WSJ上的准确率为97.24%，在之前最好的单一自动学习标记结果上的错误率降低了4.4%。我们提出了一种有监督的条件马尔可夫模型词性标记器（CMM），它利用了来自左右上下文的信息。"}
{"pid": "W99-0625", "zh_sum": "检测短文中的文本相似度：通过机器学习探索语言特征组合我们提出了一种新的复合相似度度量，该度量结合了来自多个语言指标的信息来度量小文本单元对之间的语义距离。研究了几种潜在特征，并通过机器学习选择了最佳组合。我们讨论了比传统的、面向文档级和信息检索的相似度定义更严格的相似度概念，并通过展示其与多文档文本摘要问题的相关性来激励相似度。根据标准信息检索技术对我们系统的结果进行了评估，证明新方法在识别密切相关的文本单元方面更有效。在短文或句子层面，我们超越了N-gram，利用WordNet同义词，以及共享单词之间的顺序和距离。"}
{"pid": "P95-1037", "zh_sum": "分析句法自然语言解析器的统计决策树模型已经证明自己不足以处理高度模糊的大词汇量文本，这可以从它们在《华尔街日报》等领域的糟糕表现以及从基于解析的方法转向一般的文本处理来证明。在本文中，我描述了SPATTER，这是一种基于决策树学习技术的统计解析器，它为每个句子构造了一个完整的语法分析，其准确率远远高于任何已发布的结果。这项工作基于以下前提：（1）语法过于复杂和详细，无法为最感兴趣的领域手动开发；（2） 句法分析模型必须严重依赖词汇和上下文信息来准确分析句子；现有的n-gram建模技术不适合解析模型。在将SPATTER与IBM的计算机手册解析器进行比较的实验中，SPATTER的性能明显优于基于语法的解析器。使用PARSEVAL方法对《宾夕法尼亚州Treebank华尔街日报》语料库中的SPATTER进行评估，对于40个单词或更少的句子，SPATTER的准确率达到86%，召回率达到86%，每个句子有1.3个交叉括号，对于长度在10到20个单词之间的句子，SPATTER的准确率达到91%，召回率达到90%，交叉括号达到0.5。我们使用头传播规则技术创建了FTB-UC-DEP，这是一个从FTB-UC派生的依赖树库。我们发现，与概率上下文无关语法等非词法化基线模型相比，词汇化显著提高了性能。"}
{"pid": "W03-1008", "zh_sum": "使用组合范畴语法识别语义角色我们提出了一个基于组合范畴语法统计解析器的输出自动识别PropBank风格语义角色的系统。该系统的性能至少与基于传统树库解析器的系统一样好，并且在核心参数角色方面优于它。我们发现，使用从组合范畴语法表示中提取的特征可以提高核心参数的语义标记性能。"}
{"pid": "E06-1027", "zh_sum": "挖掘WordNet中的模糊情感：从WordNet中提取情感标记可以掩盖短语语义标记所需的许多任务，而文本依赖于带有某些语义特征注释的单词列表。我们提出了一种使用情感标记提取程序（STEP）从WordNet中提取情感形容词的方法。我们在从手动注释的正负形容词列表中提取的唯一非相交种子列表上运行了58步，并对照其他手动注释的列表对结果进行了评估。然后，58个单词被压缩成一组7813个独特的单词。对于每个单词，我们计算了一个净重叠分数，方法是从认为该单词为正的总跑步次数中减去指定该单词为负情绪的总跑步次数。我们证明，净重叠分数可以用来衡量情感模糊类别中的词语隶属度：净重叠分数最高的核心形容词，无论是通过分步还是通过人工注释，都能最准确地识别出来，而类别外围的单词得分最低，且注释者之间的一致性较低。我们发现，训练集中主观性歧义词的存在会影响主观性自动标注在词汇层面的性能。从WordNet中提取非中性形容词，并分配模糊情感类别隶属度/中心度得分和标记。WordNet同义词、反义词和gloss用于迭代扩展种子列表。"}
{"pid": "P05-1066", "zh_sum": "统计机器翻译的子句重构我们描述了一种在统计机器翻译系统中合并句法信息的方法。该方法的第一步是解析正在翻译的源语言字符串。第二步是对解析树应用一系列转换，有效地重新排序翻译系统源语言端的表面字符串。此步骤的目标是恢复比原始字符串更接近目标语言词序的基础词序。在基于短语的统计机器翻译系统的训练和解码阶段，将重排序方法作为预处理步骤。我们描述了从德语到英语的翻译实验，结果表明，基线系统的Bleu分数从25.2%提高到了重新排序系统的Bleu分数26.8%，这在统计学上是显著的改进。我们提出了符号测试来衡量蓝色分数改善的显著性。我们注意到，对于蓝色的情况，是否满足引导重采样所需的条件尚不清楚，建议使用符号测试。我们使用六个手工制作的重新排序规则，针对动词、主语、助词和否定的位置。"}
{"pid": "W97-0713", "zh_sum": "从语篇结构到语篇摘要，我们描述的实验表明，修辞分析者和核心性的概念可以有效地用于确定语篇中最重要的单位。我们展示了如何实现这些概念，并讨论了使用基于语篇的摘要程序获得的结果。"}
{"pid": "J90-1003", "zh_sum": "词汇联想规范了互信息和词典编纂。词汇联想一词在心理语言学文献中有着非常特殊的意义。（一般来说，如果护士一词跟在医生等高度相关的词后面，受试者对护士一词的反应会比正常人更快。）我们将扩展该术语，为统计描述各种有趣的语言现象提供基础，从医生/护士类型的语义关系（内容词/内容词）到动词和介词（内容词/虚词）之间的词汇-句法共现约束。本文将提出一种基于信息论互信息概念的客观度量方法，用于从计算机可读语料库中估计词语联想规范。（获取单词关联规范的标准方法是测试几千个单词：几百个单词的M主题，这种方法既昂贵又不可靠。）所提出的关联比率（association ratio）直接从计算机可读语料库中估计单词关联规范，从而可以估计数万个单词的规范。在我们的工作中，关联（x，y）的重要性是通过互信息I（x，y）来衡量的，即共同观察x和y的概率，与单独观察x和y的概率相比。"}
{"pid": "P96-1027", "zh_sum": "如果字符串位置被更适合逻辑形式的概念所取代，并且采取措施缩减包含语义不完整短语的生成路径，则图表生成图表构成了一个自然统一的解析和生成架构。我们建议通过只考虑语义不重叠和索引兼容的组合成分来减少实现过程中构建的成分数量。我们提出了一个基于图表的生成过程，该过程以压缩表示为输入，生成所有的释义，而无需首先扩展为析取范式。"}
{"pid": "P05-1059", "zh_sum": "用于对齐的随机词汇化倒装转换语法我们提出了一种倒装转换语法，其中规则概率在整个同步解析树中被词汇化，以及用于有效训练的修剪技术。对于完全EM是可行的短句，对齐结果比未本地化的ITG有所改善，但修剪似乎对长句有负面影响。我们提出了一个模型，在该模型中，非终结词由英语和外语单词对进行词汇化，从而使倒装词依赖于同步规则左侧的词汇信息。我们提出了Tic-tac-toe剪枝，它基于模型1中一对跨距内外的词对概率。Tic-tac-toe修剪算法使用动态规划来计算O（n4）中跨距对的内外得分。"}
{"pid": "P10-1040", "zh_sum": "单词表示：半监督学习的一种简单而通用的方法如果我们采用现有的有监督NLP系统，提高准确性的一种简单而通用的方法是使用无监督单词表示作为额外的单词特征。我们评估了Brown clusters、Collobert和Weston（2008）的嵌入，以及HLBL（Mnih和Hinton，2009）在NER和组块上的单词嵌入。我们使用接近最先进水平的监督基线，发现三个单词表示中的每一个都提高了这些基线的准确性。通过组合不同的单词表示，我们发现了进一步的改进。您可以在此处下载我们的word功能，以便在现有NLP系统中现成使用，以及我们的代码：http://metaoptimize.com/projects/wordreprs/我们表明，单语词簇作为单语模型中的特征广泛适用于语言结构预测。"}
{"pid": "W07-2012", "zh_sum": "SemEval-2007 WePS评估：建立网民搜索任务的基准本文介绍了网民搜索任务的任务定义、资源、参与情况和比较结果，这是SemEval-2007评估活动的一部分。此任务包括根据使用人名引用的实际实体，对一组提到模棱两可人名的文档进行聚类。我们考虑在Web搜索场景中消除人名歧义的问题。Web人员搜索任务的目标是将网页分配给组，其中每个组包含引用一个唯一实体的所有（且仅包含那些）网页。我们的Web Persona Search（WePS）任务创建了一个基准数据集。"}
{"pid": "N04-1042", "zh_sum": "使用条件随机场从研究论文中准确提取信息随着越来越多的研究论文搜索引擎（如Citeser）用于文献搜索和招聘决策，此类系统的准确性至关重要。本文采用条件随机场（CRF）从研究论文的标题和引文中提取各种公共字段。CRF的基本理论正在得到很好的理解，但将其应用于实际数据的最佳实践需要更多的探索。本文对几个因素进行了实证研究，包括改进正则化的高斯、指数和双曲L1先验的变化，以及几类特征和马尔可夫阶。在标准的基准数据集上，我们实现了最新的性能，与以前最好的SVM结果相比，平均F1错误减少了36%，字错误率减少了78%。与HMMs相比，精确度更高。CORA由两个集合组成：一组为标题、作者和机构等实体注释的研究论文标题；以及使用BibTeX字段（如journal、year和publisher）注释的引用集合。"}
{"pid": "J90-1004", "zh_sum": "语义头驱动生成我们提出了一种从逻辑形式编码生成字符串的算法，该算法改进了以前的算法，因为它对适用的语法类限制较少。特别是，与以前的自底向上生成器不同，它允许使用符号非单调语法，但与自顶向下方法不同，它还允许左递归。该算法的启用设计功能是隐式遍历以语义头驱动方式生成的字符串的分析树。我们介绍了一种头部驱动的逻辑表单生成算法。"}
{"pid": "P06-1085", "zh_sum": "无监督分词中的上下文依赖性开发更好的连续文本分词方法对于改进亚洲语言的处理非常重要，并可能为人类如何学习分词提供帮助。我们提出了两种新的贝叶斯分词方法，分别假设单词依赖的单图和双图模型。bigram模型的性能大大优于unigram模型（和以前的概率模型），这表明了这种依赖关系对于分词的重要性。我们还表明，以前的概率模型主要依赖于次优搜索过程。我们从语料库的随机派生开始，在每次迭代中，通过在节点级别进行局部更改来修改当前派生，从而对派生重新采样。我们使用层次Dirichlet过程（HDP）来归纳上下文词模型。"}
{"pid": "P01-1064", "zh_sum": "一种与领域无关的文本分割统计模型我们提出了一种统计方法，可以找到给定文本的最大概率分割。这种方法不需要训练数据，因为它根据给定的文本估计概率。因此，它可以应用于任何域中的任何文本。实验表明，该方法比最先进的文本分割系统更精确，或者至少与最先进的文本分割系统一样精确。我们将TS问题建模为在图中寻找最小代价路径的问题，因此采用了动态规划算法。我们介绍了最早使用动态规划（DP）的概率方法之一，称为U00。"}
{"pid": "H05-1004", "zh_sum": "在共指消解性能指标方面，本文提出了一种用于评价共指消解的约束实体对齐F-度量（CEAF）。通过将引用和系统实体（或共引用链）与系统（引用）实体最多与一个引用（系统）实体对齐的约束对齐来计算度量。我们证明了最佳对齐是一个最大二部匹配问题，这个问题可以用Kuhn-Munkres算法来解决。对比实验表明，广为人知的MUC F测度在评价共指系统时存在严重缺陷。该度量还与自动内容提取（ACE）任务中的官方评估度量ACE值进行了比较，我们得出结论，该度量具有ACE值中缺少的对称性和更好的可解释性等特性。我们使用钟形树对搜索路径进行评分和存储。"}
{"pid": "W98-1411", "zh_sum": "使用随机搜索进行文本规划的实验Marcu描述了文本规划中的一个重要而困难的问题：给定一组要传达的事实和一组可以用来将它们联系在一起的修辞关系，如何安排这些材料以产生尽可能好的文本？我们描述了针对这项任务的一些启发式搜索方法的实验。我们研究了一组基本言语行为的话语树确定问题，这些基本言语行为部分受到修辞关系的约束。我们提倡将遗传算法作为彻底搜索博物馆文物描述的最佳顺序的替代方法。"}
{"pid": "J93-2005", "zh_sum": "语料库分析中的词汇语义技术在本文中，我们概述了一个计算语言学的研究计划，广泛使用文本语料库。我们展示了词汇知识的语义框架如何能够暗示文本中单词之间的关系比简单的共现关系更丰富。这项工作提出了如何利用转喻和多义等语言现象对词汇进行语义标注。与纯粹的统计搭配分析不同，语义理论的框架允许自动构建关于搭配系统中出现的单词之间更深层语义关系的预测。我们举例说明了几类名词的词汇信息获取方法，以及这些技术如何微调从机器可读词典的初始种子中获取的词汇结构。除了传统的词汇语义关系外，我们还展示了当使用适当的语义工具进行分析时，如何从语料库中获取有关词汇预设和偏好关系的信息。最后，我们讨论了语料库研究对于丰富理论语言学研究的数据集，以及帮助确认或推翻语言学假设的潜力。我们提出了一个有趣的从语料库中获取语义关系的框架，该框架不仅依赖于统计数据，而且以理论词汇原则为指导。我们展示了诸如互信息度量等统计技术如何有助于自动获取有关名词和谓词之间联系的词汇信息。我们使用广义句法模式从部分解析的语料库中提取质量结构。"}
{"pid": "P05-1057", "zh_sum": "基于对数线性模型的词语对齐我们提出了一个基于对数线性模型的词语对齐框架。所有知识源都被视为特征函数，它依赖于源语言句子、目标语言句子和可能的附加变量。对数线性模型允许通过合并语法信息轻松扩展统计对齐模型。在本文中，我们使用IBM Model 3对齐概率、POS对应和双语词典覆盖率作为特征。我们的实验表明，对数线性模型的性能明显优于IBM翻译模型。我们提出了一个对数线性模型，该模型将在两个方向上训练的IBM模型3与启发式特征相结合，从而实现了1对1的对齐。"}
{"pid": "P02-1042", "zh_sum": "使用宽覆盖CCG解析器构建深度依赖结构本文描述了一种使用组合范畴语法（CCG）派生依赖结构的宽覆盖统计解析器。该解析器不同于大多数现有的宽覆盖树库解析器，它捕获了结构（如协调、提取、提升和控制）中固有的长期依赖关系，以及标准的局部谓词参数依赖关系。用于训练和测试解析器的一组依赖结构是从CCG范式派生树库中获得的，CCG范式派生树库是从Penn树库（半）自动派生的。解析器正确地恢复了80%以上的标记依赖项和90%左右的未标记依赖项。我们提供的示例显示了头如何在派生过程中填充依赖项槽，以及如何通过统一联合索引头变量来恢复长期依赖项。我们根据具有词汇函子类别的单词和它们的参数之间的依赖关系来定义CCG的谓词参数结构。"}
{"pid": "W07-2216", "zh_sum": "关于非投影数据驱动依赖解析的复杂性，本文研究了几种用于依赖解析的非投影解析算法，在假设每个依赖决策独立于其他所有依赖决策的情况下，提供了新的多项式时间解，这里称之为边因子模型。我们还研究了考虑非局部信息的非投影解析算法，并给出了几个硬度结果。这表明，对于任何比边缘因子模型更丰富的模型，精确的非投影依赖解析都不太可能是可处理的。我们认为，主要的障碍是非投影分析是超越弧因子模型的NP难问题。"}
{"pid": "W98-0705", "zh_sum": "使用WordNet语法集进行索引可以改进文本检索。如果选择WordNet语法集作为索引空间，而不是单词形式，则用于文本检索的经典向量空间模型可以提供更好的结果（在我们的实验中高达29%）。此结果是针对从SEMCOR语义一致性派生的手动消歧测试集合（查询和文档）获得的。还测量了索引文档时检索性能对（自动）消歧错误的敏感性。最后，可以观察到，如果查询没有消除歧义，那么通过语法集进行索引的性能（最多）只能与标准单词索引一样好。我们指出了WordNet在信息检索方面的一些弱点，特别是缺少领域信息，以及感觉区分对于任务来说过于细粒度。"}
{"pid": "D09-1005", "zh_sum": "一阶和二阶期望半环及其在翻译林最小风险训练中的应用许多统计翻译模型可以看作是加权逻辑推导。在这种范式下，我们使用期望半环（Eisner，2002）中的权重来计算压缩翻译林（晶格或超图）上的一阶统计量（例如，期望假设长度或特征计数）。然后，我们引入一种新的二阶期望半环，它计算二阶统计量（例如，假设长度的方差或熵的梯度）。这种二阶半环对于许多有趣的训练范式是必不可少的，例如最小风险、确定性退火、主动学习和半监督学习，其中梯度下降优化需要计算熵或风险的梯度。我们在开源机器翻译工具包Joshua中使用这些半环，使风险最低的培训达到1.0 BLEU点。我们使用BLEU的线性可分解近似来考虑最小风险培训。利用期望半环可以计算出图期望BLEU的充分统计量。我们扩展了Smith和Eisner的工作，并通过使用压缩图而不是n-best列表来获得更好的特性期望估计。我们在Hiero（Chiang，2007）生成的翻译林上使用确定性退火进行预期的BLEU训练。"}
{"pid": "P09-1094", "zh_sum": "应用程序驱动的统计释义生成释义生成（PG）在许多NLP应用程序中都很重要。然而，对PG的研究还远远不够。在本文中，我们提出了一种新的统计释义生成方法（SPG），该方法可以（1）基于统一的统计模型实现各种应用，（2）自然地组合多个资源以提高PG性能。在我们的实验中，我们使用所提出的方法为三种不同的应用生成意译。结果表明，该方法可以很容易地从一个应用程序转换到另一个应用程序，并生成有价值和有趣的解释。我们提出了一种可以针对不同任务配置的句子释义方法，包括一种句子压缩形式。"}
{"pid": "P06-2094", "zh_sum": "按需信息抽取目前，使信息抽取系统适应新主题是一个代价高昂且缓慢的过程，需要对每个新主题进行一些知识工程。我们提出了一种新的信息提取范式，它根据用户的查询“按需”操作。按需信息提取（ODIE）旨在完全消除定制工作。给定用户的查询，系统将自动创建模式以提取主题文本中的显著关系，并使用复述发现技术从提取的信息中构建表。它依赖于模式发现、释义发现和扩展命名实体标记方面的最新进展。我们报告了实验结果，其中系统为许多主题创建了有用的表格，证明了这种方法的可行性。"}
{"pid": "C94-1027", "zh_sum": "利用神经网络对词性信息进行标注的文本语料库在语言学研究的许多领域都很有用。本文提出了一种新的基于神经网络的词性标注方法（Net-Tagger），并将其性能与HMM-Tagger（Cutting et al.，1992）和基于三角图的Tagger（Kempe，1993）进行了比较。结果表明，网络标记器的性能与基于三角图的标记器相当，优于HMM标记器。通过使用大量的训练数据，标签的正确率达到了95%。"}
{"pid": "W04-1013", "zh_sum": "ROUGE：自动评估摘要的软件包ROUGE代表注册评估的面向回忆的替补。它包括通过将摘要与人类创建的其他（理想）摘要进行比较来自动确定摘要质量的措施。这些度量值统计计算机生成的待评估摘要与人类创建的理想摘要之间的重叠单元数，如n-gram、单词序列和单词对。本文介绍了四种不同的胭脂衡量标准：胭脂总结评估包中的胭脂N、胭脂L、胭脂W和胭脂S及其评估。其中三个已在2004年文件理解会议（DUC）中使用，该会议是由NIST赞助的大规模总结评估。"}
{"pid": "W04-1221", "zh_sum": "基于条件随机场和丰富特征集的生物医学命名实体识别"}
{"pid": "W03-1006", "zh_sum": "使用深层语言特征识别和标记语义参数我们使用深层语言特征预测句法参数的语义角色，并表明这些特征比面向表面的特征表现得更好。我们还表明，从生成深层语法特征的轻型解析器预测标签的性能与使用仅生成表层语法特征的完整解析器的性能相当。我们认为，从框架网中获得的深层语言特征有助于成功地将PropBank角色分配给成分。我们将基于LTAG的解析树分解用于SRL。我们没有使用SRL模型中使用的典型解析树特性，而是使用基本树中从谓词到组成参数的路径。"}
{"pid": "P88-1012", "zh_sum": "作为诱因的解释塔西佗项目开发的诱因推理方法极大地简化了文本解释问题的概念化。本文描述并举例说明了它在解决指称、复合名词、句法歧义和转喻等局部语用问题中的应用。它还建议将语法、语义和语用学完美地结合起来。"}
{"pid": "P09-1058", "zh_sum": "一种错误驱动的汉语分词与词性标注联合模型本文提出了一种区分性的汉语分词与词性标注联合模型。我们的单词-字符混合模型提供了高性能，因为它可以处理已知和未知的单词。我们描述了我们的策略，这些策略能够在学习已知词和未知词的特征时产生良好的平衡，并提出了一种错误驱动策略，该策略通过从训练语料库中的特定错误中获取未知词的示例来实现这种平衡。我们描述了一个基于边缘注入松弛算法（MIRA）训练模型的有效框架，并在宾夕法尼亚大学中国树库上对我们的方法进行了评估，结果表明，与文献中报道的最新方法相比，该方法取得了优异的性能。我们分离了已知词和未知词的处理，并使用一组分割标记来表示字符的分割。"}
{"pid": "E06-1011", "zh_sum": "近似依赖解析算法的在线学习在本文中，我们扩展了McDonald等人（2005c）的最大生成树（MST）依赖解析框架，以合并高阶特征表示，并允许每个单词具有多个父级的依赖结构。我们表明，这些扩展可以使MST框架在计算上变得棘手，但可以通过新的近似解析算法来避免这种棘手。实验结果表明，使用这些近似算法的区分性在线学习在捷克语和丹麦语中获得了最好的解析精度。我们提出了一种基于二阶图的依赖分析模型，该模型融合了两种子树的特征。我们使用维特比解码算法来实现O（n3）解析时间。我们证明了具有水平标记的非投影依赖分析是FNP困难的。我们定义了一个二阶依赖解析模型，其中允许相邻同级之间的交互。"}
{"pid": "J93-2002", "zh_sum": "从语法到词汇：词汇句法的无监督学习想象一种完全陌生的语言；学习它的唯一方法是一本普通的语法书和一个非常大的文本语料库。没有可用的词典。如何利用容易识别的表面语法事实从语料库中提取尽可能多的关于单个单词的句法信息？本文描述了一种基于两个原则的方法。首先，依赖局部形态句法线索来构建结构，而不是试图解析整个句子。其次，将这些线索视为句法结构的概率指标，而不是绝对指标。对使用线索收集的数据应用推断统计，而不是从单一线索中得出分类结论。使用Lerner程序对英语语料库进行的实验证明了这种方法在推断动词句法框架方面的有效性。勒纳一开始对内容词一无所知——它从限定词、助词、情态词、介词、代词、补语、并列连词和标点符号中衍生出来。我们的研究重点是大规模自动获取子类别化帧（SCF）。"}
{"pid": "W02-0603", "zh_sum": "无监督的语素发现我们提出了两种无监督的词素单元切分方法。所使用的模型特别适用于词法丰富的语言，如芬兰语。第一种方法基于最小描述长度（MDL）原理，可以在线工作。在第二种方法中，使用最大似然（ML）优化。使用一种评估方法来衡量分割的质量，该方法将生成的分割与现有的形态学分析进行比较。在芬兰语和英语语料库上的实验表明，与当前最先进的系统相比，本文提出的方法表现良好。我们的方法基于使用最小描述长度MDL代价函数联合最小化变形码本的大小和所有单词形式的编码大小。"}
{"pid": "P96-1011", "zh_sum": "组合范畴语法的高效范式分析在具有组合等强大规则的范畴语法下，一个简单的n字句子可以进行指数级的语法分析。生成所有语法分析效率很低，并且会掩盖输入中真正的语义歧义。本文通过一种高效、正确且易于实现的范式解析技术，解决了组合范畴语法的一种相当普遍的形式的问题。证明了解析器在允许的语法分析的每个语义等价类中只找到一个语法分析；也就是说，虚假歧义（如仔细定义的）被证明是安全且完全消除的。我们提供了一种安全完整的解析算法，如果组合有界或语法以其他方式受到限制，则在必要时可以返回非NF派生以保留解释。"}
{"pid": "P88-1015", "zh_sum": "专家-客户对话中的线索与控制我们对控制与话语结构之间的关系进行了实证分析。我们将控制标准应用于四个对话，确定了三个层次的话语结构。我们研究了这些结构之间控制变化的机制，发现话语类型和非线索词可以预测控制的变化。当话语目标成功进行时，参与者使用了某些类型的信号，但当话语目标不成功时，他们会求助于打断。我们将主动性定义为在对话中的任何时候推动对话的演讲者所持有的主动性。我们根据话语类型提出了跟踪主动性的规则：例如，陈述、提议和问题显示主动性，而回答和确认则不显示主动性。"}
{"pid": "P08-1115", "zh_sum": "广义词格翻译词格解码在口语翻译中被证明是有用的；我们认为，它也为文本体裁的翻译提供了一个引人注目的模式。我们表明，先前使用有限状态技术翻译格的工作可以自然地扩展到更具表现力的同步上下文无关语法模型。此外，我们还解决了非线性字格输入在重排序模型中引入的一个重要复杂性。我们的实验表明，该方法对于汉英和阿拉伯文的翻译有很大的帮助。在我们的模型中，将多个不同的汉语切分器组合起来创建晶格。我们提供的所有系统都使用Moses的晶格输入格式（Dyer等人，2008），包括不需要它们的基线。"}
{"pid": "J08-1002", "zh_sum": "用于概率HPSG分析的特征林模型词典化语法的概率建模很困难，因为这些语法利用复杂的数据结构，例如类型化特征结构。这使得我们无法应用概率建模的常用方法，即在子结构之间统计独立的假设下，将完整结构划分为子结构。例如，将句子的词性标注分解为每个单词的标注，将CFG分析分解为CFG规则的应用。这些方法依赖于目标问题的结构，即格或树，不能应用于包括类型化特征结构在内的图结构。本文提出了特征林模型，以解决包括类型化特征结构在内的复杂数据结构的概率建模问题。当概率事件用特征林表示时，特征林模型提供了一种无需独立性假设的概率建模方法。特征林是在压缩林结构中表示不明确树的通用数据结构。特征林模型是在特征林上定义的最大熵模型。提出了一种无需解包特征林的最大熵估计动态规划算法。因此，当任何数据结构由特征林表示时，都可以对其进行概率建模。本文还描述了用特征林表示HPSG语法结构和谓词-参数结构的方法。因此，我们描述了为HPSG解析开发概率模型的完整策略。通过在Penn Treebank上的句法分析实验，对所提方法的有效性进行了实证评估，并讨论了其在实际句子句法分析中的适用性。"}
{"pid": "D07-1103", "zh_sum": "通过丢弃大部分短语表来提高翻译质量使用基于平行语料库中短语对共现显著性测试的技术，可以减少统计机器翻译中短语表的数量。节省的费用可能相当可观（高达90%），并且不会导致BLEU分数降低。在某些情况下，BLEU同时得到改善，但如果采用最先进的短语表平滑，效果不太明显。我们使用Fisher精确检验。我们过滤掉统计上不可靠的翻译对。"}
{"pid": "P06-1014", "zh_sum": "有意义的词义聚类有助于提高词义消歧性能细粒度的词义差异是词义消歧成功的主要障碍之一。在本文中，我们提出了一种基于映射到手工编制的词典编码语义层次结构（即牛津英语词典）的方法来降低WordNet语义库的粒度。我们评估了映射和诱导聚类的质量，并在Senseval-3英语全词任务中评估了粗略WSD系统的性能。在我们的粗粒度任务中，首先半自动地对感官清单进行聚类，每个聚类表示感官上的一个等价类。我们提出了一种感知库存之间映射的自动方法；在这里，我们利用了两个词义目录在光泽定义和结构关系方面的相似性，以便在WordNet词义和粗粒度牛津英语词典中的区别之间进行映射。我们认为，自动创建新的对齐方式很困难，因为单词的歧义、不同的感官粒度或特定于语言的概念化。"}
{"pid": "W09-1105", "zh_sum": "处理否定范围的元学习方法在文本中查找否定信号及其范围是信息提取中的一个重要子任务。在这篇文章中，我们提出了一个机器学习系统，可以发现生物医学文本中否定的范围。该系统结合了多个分类器，分两个阶段工作。为了研究该方法的稳健性，该系统在代表不同文本类型的BioScope语料库的三个子语料库上进行了测试。与当前最先进的结果相比，它实现了迄今为止该任务的最佳结果，误差减少了32.07%。我们描述了一种通过组合IGTREE、CRF和支持向量机（SVM）来提高否定范围分辨率的方法（Morante和Daelemans，2009）。我们开创了否定范围发现的研究，将其表述为组块问题，将句子中的单词分类为否定信号范围之内或之外。"}
{"pid": "D07-1104", "zh_sum": "基于词缀数组的分级短语翻译统计机器翻译系统中的一个主要工程挑战是超大翻译规则集的有效表示。在基于短语的模型中，可以通过将训练数据存储在内存中并使用后缀数组作为有效索引来快速查找和提取规则来解决此问题。基于层次短语的翻译引入了带有间隙的源短语的附加褶皱。用于连续短语的查找算法不再适用，最好的近似模式匹配算法速度太慢，每个句子需要几分钟的时间。我们描述了基于层次短语翻译的新查找算法，该算法将经验计算时间减少了近两个数量级，使得对有间隙的源短语进行动态查找成为可能。该方法的基础是使用后缀数组查找连续子串的出现，然后将它们相交以查找不连续子串的出现。"}
{"pid": "P99-1065", "zh_sum": "捷克语的统计句法分析本文考虑捷克语的统计句法分析，捷克语与英语至少在两个方面有根本不同：（1）它是一种高度屈折的语言，（2）它有相对自由的语序。这些差异很可能会给英语中的技巧带来新的问题。我们描述了在（Collins 97）的解析模型上构建的经验。我们的最终结果——80%的依赖性准确率——代表了语法分析器在英语（华尔街日报）文本上91%的准确率方面取得的良好进展。我们使用布拉格依赖树库的转换树库对捷克语进行成分分析。"}
{"pid": "N07-1030", "zh_sum": "使用整数规划标准的成对共指消解系统联合确定回指性和共指消解时，由于将回指识别作为共指消解的一个隐式部分而导致错误。在本文中，我们提出了一种用于共指消解的整数线性规划（ILP）公式，该公式将回指和共指建模为一个联合任务，使得每个局部模型为最终赋值通知另一个局部模型。与ACE数据集上的基本共指分类器相比，这种联合ILP公式的f分数提高了3.7-5.3%。通过对回指和共指的联合推理，我们避免了级联错误，无需单独优化阈值。"}
{"pid": "C02-1145", "zh_sum": "构建大规模带注释的中文语料库本文讨论了构建大规模中文语料库的相关问题。我们试图回答四个问题：（i）如何加快注释速度，（ii）如何保持较高的注释质量，（iii）语料库适用于什么目的，以及最后（iv）我们预期的未来工作。"}
{"pid": "W07-0718", "zh_sum": "机器翻译的元评估本文评估了8种语言对机器翻译系统的翻译质量：将法语、德语、西班牙语和捷克语翻译成英语和英语。我们进行了广泛的人工评估，不仅可以对不同的机器翻译系统进行排名，还可以对评估过程进行更高层次的分析。我们测量了三种主观评价的时间安排和注释者内部和内部一致性。我们测量了自动评估指标与人类判断的相关性。这个元评估揭示了关于最常用方法的令人惊讶的事实。我们发现，排名句子比得分充分性和流利性更能体现注释者之间的一致性。"}
{"pid": "P96-1042", "zh_sum": "基于语料库的自然语言处理方法通常使用监督训练，需要对训练语料库进行昂贵的手动标注，从而最大限度地降低人工标注成本。本文研究了通过样本选择降低标注成本的方法。在这种方法中，在培训期间，学习程序检查许多未标记的示例，并仅选择在每个阶段信息量最大的示例进行标记（注释）。这避免了对几乎没有新信息的示例进行冗余注释。本文扩展了我们以前在基于委员会的概率分类器样本选择方面的工作。我们描述了一系列基于委员会的样本选择方法，并报告了随机词性标注任务的实验结果。我们发现，所有变体都显著降低了注释成本，尽管它们的计算效率不同。特别是，最简单的方法（无需调整参数）可以提供极好的结果。我们还表明，样本选择可以显著减少标记者使用的模型的大小。我们使用HMMs进行词性标注，并发现有选择的句子取样可以显著减少实现理想标记精度所需的样本数量。我们使用投票熵度量，即分类器集合分配给示例的标签分布的熵，来估计集合内的不一致性。"}
{"pid": "E06-1042", "zh_sum": "一种几乎无监督的非文字语言识别聚类方法本文介绍了一种通过几乎无监督的词义消歧和聚类技术对动词的文字和非文字用法进行自动分类的系统TroFi（Trope Finder）。TroFi使用句子上下文，而不是语义层次中的选择约束冲突或路径。它还使用在没有人监督的情况下获取和清理的文本和非文本种子集来引导学习。我们将词义消歧算法应用到我们的任务中，并使用多个种子集学习器、投票模式和附加功能（如SuperTags和句外上下文）对其进行扩充。在手工标注数据上的详细实验表明，我们的增强算法比基线算法的性能高24.4%。使用TroFi算法，我们还构建了TroFi示例库，这是一个可扩展的注释文字/非文字示例资源，可供NLP研究社区免费使用。对于评分，文字召回定义为（文字集群中的正确文字/总正确文字）；文字精度定义为（文字簇中的正确文字/文字簇的大小）。我们将文字分类与非文字分类建模为词义消歧任务，并使用聚类算法将测试实例与两个自动构建的种子集（一个具有文字表达式，一个具有非文字表达式）进行比较，指定最接近集的标签。"}
{"pid": "W99-0623", "zh_sum": "利用自然语言处理中的多样性：结合解析器将三种最先进的统计解析器结合起来，以产生更精确的解析器，以及可实现的树库解析精度的新界限。提出了两种通用方法，并针对每种方法描述了两种组合技术。探讨了参数模型和非参数模型。由此产生的解析器超过了之前发布的Penn Treebank的最佳性能结果。我们对Penn TreeBank的组成结构使用朴素贝叶斯投票，将最佳解析器的F-度量从89.7提高到91.3（错误减少16%）。关于系统组合研究，我们提出了两种解析器组合方案，一种是从其中一个解析器中选择整个树，另一种是通过选择初始树建议的成分来构建新树。我们通过最大化所选解析相对于组合的解析集的预期精度来执行解析选择。"}
{"pid": "W04-2609", "zh_sum": "名词短语语义分类模型本文提出了一种检测名词短语语义关系的方法。一种称为语义分散的学习算法用于自动标记具有相应语义关系的复杂名词、属格和形容词名词短语。我们提出了一个35类的方案来对不同短语中的关系进行分类。我们提出了一种称为语义散射的方法来解释NCs。"}
{"pid": "P84-1085", "zh_sum": "语篇语义的句法分析正确的语篇结构分析是理解语篇的前提。本文概述了一种话语语法，它承认几个不同层次的结构。这种语法称为“动态语篇模型”，它使用一种扩展的转换网络解析机制，在构成语篇的各个从句的语义表示的基础上，从左到右逐步构建语篇语义的表示。解析器的中间状态模拟了生成话语的社会情境的中间状态。本文试图证明，话语确实可以被看作是通过话语成分的顺序和递归嵌套来构建的。它给出了不同层次的语篇结构的比较详细的例子，并展示了这些结构是如何在本文提出的框架中被描述的。"}
{"pid": "H05-1059", "zh_sum": "基于最简单优先的序列数据标注策略的双向推理本文针对词性标注、命名实体识别和文本组块等序列标注问题，提出了一种双向推理算法。该算法可以枚举所有可能的分解结构，并在多项式时间内找到概率最大的序列和相应的分解结构。我们还提出了一种基于最简单优先策略的高效解码算法，该算法在计算量显著降低的情况下，对全双向推理具有较好的性能。词性标注和文本组块的实验结果表明，所提出的双向推理方法始终优于单向推理方法，双向MEMM的性能与包括核支持向量机在内的最新学习算法相当。我们提出了最简单的第一确定性解码。"}
{"pid": "P01-1030", "zh_sum": "机器翻译的快速解码和最佳解码良好的解码算法对于任何统计机器翻译系统的成功都至关重要。解码器的工作是根据之前学习的一组参数（以及组合参数的公式）找到最有可能的翻译。由于可能的翻译空间非常大，典型的解码算法只能检查其中的一部分，因此可能会错过好的解决方案。在本文中，我们比较了基于堆栈的传统解码算法与两种新解码器的速度和输出质量：快速贪婪解码器和将解码视为整数规划优化问题的缓慢但最优的解码器。我们将多栈解码器和贪婪爬山算法获得的翻译与将解码视为旅行商问题变体的最优整数规划解码器产生的翻译进行比较。"}
{"pid": "D09-1058", "zh_sum": "半监督结构化条件模型依赖分析的实证研究本文描述了基于半监督学习方法的高性能依赖分析的实证研究。我们描述了半监督结构化条件模型（SS-SCMs）对依赖解析问题的扩展，其框架最初是在（Suzuki和Isozaki，2008）中提出的。此外，我们还介绍了与依赖关系解析相关的两个扩展：第一个扩展是将SS SCMs与另一种半监督方法相结合，如（Koo et al.，2008）所述。第二个扩展是使用两阶段半监督学习方法将该方法应用于二阶解析模型，如（Carreras，2007）中所述。我们使用两个广泛使用的测试集（英语的Penn树库和捷克的Prague dependency树库）证明了我们提出的方法在依赖项解析实验中的有效性。我们对上述数据集中的测试数据的最佳结果表明，英语和捷克语的预测准确率分别为93.79%和88.05%。我们提出了一种非常有效的半监督方法，将基于未标记数据估计的多个生成模型的特征组合到一个用于结构化预测的判别系统中。"}
{"pid": "N03-1003", "zh_sum": "学习释义：一种使用多序列比对的无监督方法我们解决了句子层面释义的文本到文本生成问题，这一现象不同于单词或短语层面的释义，而且比单词或短语层面的释义更困难。我们的方法将多序列比对应用于从未注可比语料库收集的句子：它学习一组由单词格对表示的释义模式，并自动确定如何应用这些模式重写新句子。我们的评估实验结果表明，该系统获得了准确的释义，优于基线系统。我们建议将多重序列比对（MSA）应用于传统的句子级PR。我们使用迭代成对多重序列比对（MSA）算法在释义上构建格。我们提出了一种多序列比对算法，该算法采用结构相似的句子，并构建一个紧凑的晶格表示，对局部变化进行编码。我们提出了一种生成句子级释义的方法，从数据中学习结构相似的表达模式，并使用可比较的语料库识别其中的释义对。"}
{"pid": "C90-2067", "zh_sum": "从机器可读词典中提取超大神经网络的词义消歧本文描述了一种从机器可读词典的定义文本中自动构建超大神经网络（VLNN）的方法，并演示了这些网络在词义消歧中的应用。我们的方法将早期两种独立的词义消歧方法结合在一起：使用机器可读词典和传播和激活模型。VLNN的自动构建可以实现用于自然语言处理的神经网络的实际大小实验，从而提供对其行为和设计的洞察，并可能导致改进。我们将传统的扩散激活方法应用于词义消歧。"}
{"pid": "H05-1066", "zh_sum": "使用生成树算法的非投影依赖分析我们将加权依赖分析形式化为在有向图中搜索最大生成树（MST）。使用这种表示，Eisner（1996）的解析算法足以在O（n3）时间内搜索所有投影树。更令人惊讶的是，使用Chu-Liu-Edmonds（Chu-Liu-Edmonds，1965；Edmonds，1967）MST算法自然地将表示扩展到非投影解析，从而产生了一个O（n2）解析算法。我们使用在线大幅度学习技术（Crammer et al.，2003；McDonald et al.，2005）在Prague依赖树库上评估了这些方法，并表明MST解析可以提高具有非投影依赖的语言的效率和准确性。关键思想是构建一个由句子标记组成的完整图，其中每条边通过学习的评分函数进行加权。"}
{"pid": "N04-1016", "zh_sum": "网络作为基线：评估基于网络的无监督模型在一系列NLP任务中的性能以前的工作表明，网络计数可以用来近似二元图频率，因此应该对各种NLP任务有用。到目前为止，只有两个生成任务（机器翻译候选选择和混淆集消歧）使用web规模的数据集进行了测试。本文研究了这些结果是否可以推广到涵盖语法和语义、生成和分析以及更大范围的n-gram的任务。对于大多数任务，我们发现，当n-gram频率是从web而不是从大型语料库中获得时，简单的、无监督的模型表现更好。然而，在大多数情况下，基于网络的模型无法胜过在小型语料库上训练的更复杂的最先进模型。因此，我们认为基于web的模型应该用作标准模型的基线，而不是标准模型的替代品。我们基于web的无监督模型基于Lauer的8个介词列表对名词-名词实例进行分类，并使用web作为训练语料库。"}
{"pid": "P02-1018", "zh_sum": "一种用于恢复空节点及其先行项的简单模式匹配算法本文描述了一种用于恢复空节点并在不包含此信息的短语结构树中识别其联合索引先行项的简单模式匹配算法。这些模式是最小的连接树片段，其中包含一个空节点和与之联合索引的所有其他节点。本文还提出了一种空节点恢复过程的评估过程，该过程独立于短语结构的大部分细节，从而可以比较解析器输出的空节点恢复性能与金标准语料库中的空节点注释。根据Charniak解析器（Charniak，2000）和Penn treebank（Marcus et al.，1993）的输出对算法进行评估，结果表明，由于模式匹配算法的简单性，它在最常见的空节点类型上表现得出奇地好。我们提出了一种能够在解析后作为后处理步骤找到长距离依赖关系的算法。虽然Charniak的解析器不会生成空类别信息，但我们开发了一种算法，可以从树库中提取模式，用于将空类别插入解析器的输出中。这是第一种非局部依赖恢复的后处理方法，在上下文无关树上使用简单的模式匹配算法。"}
{"pid": "P06-2005", "zh_sum": "一种基于短语的SMS文本规范化统计模型SMS文本的行为与正常书面文本截然不同，并且有一些非常特殊的现象。为了翻译短信文本，传统的方法直接在机器翻译（MT）中对这种不规则性进行建模。然而，这些方法面临着定制问题，因为需要付出巨大的努力来调整现有翻译系统的语言模型以处理SMS文本样式。我们提供了另一种方法，通过在机器翻译之前对SMS文本进行规范化来解决此类不规则现象。在本文中，我们将SMS规范化任务视为从SMS语言到英语的翻译问题，并建议采用基于短语的统计机器翻译模型。通过对5000个句子的平行短信规范化语料库进行5倍交叉验证，我们的方法BLEU得分可以达到0.80702，而基线BLEU得分为0.6958。另一个在单独的短信文本语料库上进行的短信英译汉实验表明，使用短信规范化作为机器翻译预处理，可以在BLEU分数上大幅提高短信翻译性能，从0.1926提高到0.3770。我们还在字符级别上使用基于短语的SMT技术。我们使用一种基于短语的统计机器翻译模型，将句子拆分为k个最可能的短语。"}
{"pid": "C94-1042", "zh_sum": "Comlex语法：构建计算词典我们描述了复杂语法的设计，这是一个计算词典，为大约38000个英语标题词提供详细的语法信息。我们考虑在创建这样一个词典时出现的错误类型，以及如何测量和控制这些错误。我们的COMLEX语法词典提供动词子范畴化信息和语法释义，但它们是由单词索引的，因此不适合直接用于生成。"}
{"pid": "P01-1008", "zh_sum": "从平行语料库中提取释义虽然释义对于自然语言的解释和生成都至关重要，但当前的系统使用手动或半自动的方法来收集释义。我们提出了一种无监督学习算法，用于从同一源文本的多个英语翻译语料库中识别意译。我们的方法产生短语和单字词汇释义以及句法释义。我们将词性信息和其他形态句法线索纳入我们的联合训练算法中。"}
{"pid": "W99-0612", "zh_sum": "与语言无关的命名实体识别结合形态学和上下文证据，对文本中的个人、地理、机构或其他名称进行识别和分类，对于许多应用来说是一项重要任务。本文描述并评估了一种与语言无关的自举算法，该算法基于迭代学习和重新估计分层平滑trie模型中捕获的上下文和形态模式。该算法从没有注释的文本中学习，在没有其他特定语言信息、标记器或工具的情况下，在非常短的标记名称列表上进行训练，从而获得有竞争力的性能。我们认为每个话语现象都有一种意义，即术语在单个文档中具有固定意义的趋势。我们采用一种字符级的方法，使用前缀和后缀尝试进行命名实体识别（NER）。引导阶段使用初始或当前实体分配来估计实体和上下文沿其trie路径的类条件分布，然后递归地重新估计它们链接到的上下文/实体候选的分布，直到到达所有可访问的节点。"}
{"pid": "W04-3208", "zh_sum": "挖掘非常非平行语料库：通过Bootstrapping和EM提取平行句子和词汇我们提出了一种方法，通过在IBM Model 4 EM上利用Bootstrapping，我们可以从比以前的“可比语料库”方法更为不同的“非常非平行语料库”中提取平行句子。与以前的方法一样，我们的方法的步骤1，首先使用相似性度量在语料库中找到匹配的文档，然后从这些文档中提取平行句子和新词翻译。但与以前的方法不同，我们使用基于“找到一个得到更多”原则的迭代引导框架对此进行了扩展，该框架声称，如果发现文档包含一对平行句子，则即使文档的相似度很低，也必须包含其他句子。我们根据提取的句子对重新匹配文档，并迭代优化挖掘过程直至收敛。这种新颖的“找到一个得到更多”原则允许我们从不同的文档中向基线集添加更多的平行句子。实验结果表明，我们提出的方法比没有迭代的基线方法的效率提高了近50%。我们还表明，我们的方法在提高IBM Model 4 EM词汇学习器的性能方面是有效的，因为后者虽然比之前工作中使用的Model 1更强，但在来自非常非平行语料库的数据上表现不佳。"}
{"pid": "A97-1030", "zh_sum": "文本中专有名称的消歧识别文本中专有名称的出现及其引用的实体可能是一项困难的任务，因为名称及其引用之间存在多对多映射。我们分析了文本中难以发现专有名称的歧义类型（结构和语义），并描述了用于在命名器中消除名称歧义的启发式方法，命名器是IBM T.J.Watson研究中心开发的一个完整实现的专有名称识别模块。我们使用手写规则和知识库将专有名称分为大类。"}
{"pid": "J05-1003", "zh_sum": "针对自然语言分析的区分性重排序本文考虑了对现有概率解析器的输出进行重排序的方法。基本解析器为每个输入句子生成一组候选语法分析，相关概率定义这些语法分析的初始排名。然后，第二个模型尝试利用树的其他特征作为证据，改进初始排名。我们的方法的优势在于，它允许将树表示为一组任意的特征，而不需要考虑这些特征如何相互作用或重叠，也不需要定义将这些特征考虑在内的派生或生成模型。基于Freund等人（1998）描述的排名问题的boosting方法，我们介绍了一种新的重新排名任务方法。我们将boosting方法应用于分析《华尔街日报》树库。该方法将基线模型（Collins[1999]的模型）下的对数可能性与原始模型中未包含的解析树上额外500000个特征的证据相结合。新模型实现了89.75%的F-measure，与基线模型88.2%的得分相比，F-measure误差相对减少了13%。本文还介绍了一种新的boosting算法，该算法利用了特征空间在解析数据中的稀疏性。实验表明，与boosting方法的明显实现相比，新算法的效率显著提高。我们认为，在对数线性（最大熵）模型中，该方法是一种简单高效的特征选择方法。虽然本文中的实验是关于自然语言解析（NLP）的，但该方法应该适用于许多其他自然地被定义为排序任务的NLP问题，例如，语音识别、机器翻译或自然语言生成。我们表明，对基本解析器的n-best输出应用重排序技术可以提高解析性能。我们提出了一种方法，在每次迭代中只更新与规则特征同时出现的特征值。"}
{"pid": "W08-1301", "zh_sum": "斯坦福类型依赖关系表示本文研究斯坦福类型依赖关系表示，旨在为任何可以从自动文本理解中受益的用户提供语法关系的直接描述。为此，我们认为依赖模式必须遵循简单的设计，提供语义上有内容的信息，并提供自动提取关系的过程。我们从这个角度考虑斯坦福方案的基本设计原则，并将其与GR和PARC表示进行比较。最后，我们讨论了斯坦福方案对解析器评估的适用性问题。斯坦福依存关系提供了一个句子中单词对之间关系的简单描述。"}
{"pid": "P00-1037", "zh_sum": "一种改进的噪声信道拼写校正误差模型噪声信道模型已被广泛应用于各种问题，包括拼写校正。这些模型由两部分组成：源模型和通道模型。很少有研究对拼写纠正的通道模型进行改进。本文描述了一种新的基于通用字符串到字符串编辑的拼写更正通道模型。与以前提出的模型相比，使用此模型可以显著提高性能。我们提出了一种改进的噪声信道拼写纠正错误模型，它超越了单次插入、删除、替换和换位。我们表明，添加源语言模型可以显著提高准确性。我们通过计算逐片字符串编辑的操作概率乘积来描述错误模型。我们引入了一个模型，该模型不仅在字符级别上，而且在单词中的位置上，处理字符序列。"}
{"pid": "P11-2033", "zh_sum": "具有丰富非局部特征的基于转换的依赖关系解析基于转换的依赖关系解析器通常使用启发式解码算法，但可以容纳任意丰富的特征表示。在本文中，我们表明，通过考虑比以前系统中使用的更丰富的特征集，我们可以提高此类解析器的准确性。在标准的Penn Treebank设置中，我们的新功能将附件得分从91.4%提高到92.9%，提供了迄今为止基于转换的解析的最佳结果，总体上与最佳结果相媲美。对于中国树库来说，它们极大地提高了技术水平。我们的解析器的开源版本是免费提供的。我们为arc eager模型开发了特征模板。"}
{"pid": "E03-1076", "zh_sum": "复合词拆分的经验方法对于机器翻译（MT）等NLP应用来说是一个挑战。我们介绍了从单语语料库和平行语料库中学习分裂规则的方法。我们根据金标准对其进行评估，并衡量其对统计机器翻译系统性能的影响。结果表明，在德语-英语名词短语翻译任务中，机器翻译的准确率为99.1%，性能提高了0.039 BLEU。我们提出了一种不需要语言动机的形态分析来拆分化合物的方法。我们根据潜在分解中单词的频率来划分德语复合词。"}
{"pid": "W04-3237", "zh_sum": "最大熵资本化器的自适应：小数据可以帮助很多。提出了一种最大熵（MaxEnt）和最大熵马尔可夫模型（MEMM）的最大后验概率（MAP）自适应新技术。该技术应用于自动大写大小写一致的文本的问题。自动大写是一个实际相关的问题：语音识别输出需要大写；此外，现代文字处理器在其他文本校对算法（如拼写更正和语法检查）中执行大写。大写也可以用作命名实体提取或机器翻译中的预处理步骤。从1987年开始，一名接受《华尔街日报》（WSJ）2000万字文本培训的“背景”大写字母师被改编成两个广播新闻（BN）测试集，一个包含美国广播公司黄金时段直播文本，另一个包含美国国家公共广播电台（NPR）晨报/美国有线电视新闻网（CNN）晨报版文本，从1996年开始。在《华尔街日报》1994年发布的测试集上进行评估时，与1克基线相比，《华尔街日报》Capitaler的“域内”性能提高了45%。在对不匹配的“域外”测试数据进行评估时，1-gram基线的相对性能优于60%；使用非常少量的匹配BN数据（2.5-7万个单词）的自适应技术带来的改进大约是相对的20-25%。总体而言，BN数据的自动资本化错误率达到1.4%。使用我们的自适应技术，在背景数据的基础上使用少量的域外训练数据，所获得的性能提升是惊人的：域内数据的最小值为0.14 M单词，比使用10倍多的背景训练数据（从2 M单词到20 M单词）带来更大的改善。我们提出了一种在最大熵模型中进行转移学习的方法，该方法包括修改高斯先验的mu。在目标数据上训练新模型时，我们使用源域最大熵分类器的参数作为高斯先验的手段。"}
{"pid": "W95-0101", "zh_sum": "词性标注消歧规则的无监督学习本文描述了一种无监督学习算法，用于在不使用手动标注语料库的情况下自动训练基于规则的词性标注者。我们将该算法与Baum-Welch算法进行了比较，Baum-Welch算法用于随机标记器的无监督训练。接下来，我们展示了一种将无监督和有监督的基于规则的训练算法相结合的方法，以创建一个仅使用少量手动标记文本的高精度标记器。我们提出了一种基于规则的无监督训练语料库词性标记器。我们提出了一种获取上下文相关的词性消歧规则的方法，并通过结合监督和非监督学习，创建了一个准确的标记器，即使是从一个很小的带注释文本中。"}
{"pid": "C92-2070", "zh_sum": "基于大型语料库训练的罗吉特词类统计模型的词义消歧本文描述了一个程序，该程序使用罗吉特主要词库类别的统计模型，在无限制的文本中消除英语词义的歧义。罗吉特的类别是概念类的近似值。罗杰索引中一个词所列的类别往往与词义的区别相对应；因此，选择最可能的类别可以有效地消除歧义。类别的选择是通过使用贝叶斯理论框架，在上下文中识别和加权表示每个类别的单词来完成的。其他统计方法要求对大部分词汇使用特殊语料库或手工标注的训练示例。我们对类模型的使用克服了这一知识获取瓶颈，实现了无限制单语文本的培训，而无需人工干预。将该系统应用于1000万字的格罗利尔百科全书，该系统正确地消除了先前文献中研究过的12个多义词中92%的歧义。我们依靠的是一种直觉，即词义是由上下文信息暗示出来的。从生成过程的角度来看，目标语的相邻词是由目标语的潜在意义生成的。"}
{"pid": "P04-1035", "zh_sum": "情感教育：情感分析使用基于最小割集的主观性摘要情感分析试图确定文本跨度背后的观点；一个示例应用程序将电影评论分类为“竖起大拇指”或“竖起大拇指”。为了确定这种情感极性，我们提出了一种新的机器学习方法，将文本分类技术仅应用于文档的主观部分。提取这些部分可以使用在图中寻找最小割的有效技术来实现；这极大地促进了跨句语境约束的结合。我们创建了一个用于意见检测的电影评论数据集。我们认为，在情感分析之前进行的主观性检测可以在情感分析中获得更好的结果。我们表明，句子级分类可以改进文档级分析。在我们的主观性检测方法中，在文档中的每个句子之间创建软局部一致性约束，并使用最小割算法解决推理问题。"}
{"pid": "C96-1055", "zh_sum": "词义消歧在词汇习得中的作用：从句法线索预测语义本文讨论了从机器可读资源中提取词义歧义以构建大规模知识源的问题。我们描述了两个实验：一个实验忽略了词义差异，导致基于的动词语义分类准确率为6.3%（Levin，1993）；还有一种利用词义差异，准确率达97.9%。这些实验有双重目的：（1）验证（Levin，1993）工作的中心论点，即动词语义和句法行为是可预测的相关；（2） 为了证明，如果我们首先将句法线索划分为与不同词义相关的不同分组，那么从句法线索中提取语义信息的能力可以提高15倍。最后，我们表明，我们可以通过结合在线来源为新的词义提供有效的获取技术。我们表明，如果我们对可能的句法框架有充分的了解，那么动词几乎可以完美地划分为正确的类别。"}
{"pid": "J05-1004", "zh_sum": "命题库：带注释的语义角色语料库命题库项目采用了一种实用的语义表示方法，为Penn Treebank的语法结构添加了一层谓词参数信息或语义角色标签。由此产生的资源可以被认为是肤浅的，因为它不代表共指、量化和许多其他高阶现象，但也很广泛，因为它涵盖了语料库中每个动词的每个实例，并允许计算代表性统计数据。我们讨论了用于定义注释过程中使用的语义角色集以及分析语料库中句法/语义交替频率的标准。我们描述了一个在语料库上训练的语义角色自动标注系统，并讨论了各种类型信息对其性能的影响，包括使用平面表示的完整句法分析的比较以及树库的空“跟踪”类别的贡献。由于命题库是宾州风格树库的语义注释版本，它们跨同一动词的不同语法实现提供一致的语义角色标签。"}
{"pid": "N06-1039", "zh_sum": "使用无限制关系发现的抢占式信息提取我们试图扩展信息提取（IE）系统的边界。现有的IE系统需要大量的时间和人力来调整以适应新的场景。先发制人的信息提取是一种尝试，可以在不需要人工干预的情况下，自动提前创建所有可行的IE系统。我们提出了一种称为无限制关系发现的技术，该技术可以从文本中发现所有可能的关系，并将它们表示为表。我们提出了一个初步的系统，取得了相当好的结果。我们将NER、共指消解和句法分析应用于一个报纸文章语料库，以提取NEs之间的两地关系。我们进一步依赖有监督的方法，在完整的语法分析中定义特征，并利用newswire中对同一事件的多个描述来识别有用的关系。抢占式IE是一种范例，它首先基于成对向量聚类对文档进行分组，然后基于文档聚类对实体进行分组。"}
{"pid": "W04-3230", "zh_sum": "将条件随机场应用于日语形态分析本文提出了基于条件随机场的日语形态分析。之前在CRF中的工作假设观察序列（word）边界是固定的。然而，日语中的单词边界不明确，因此不可能直接应用CRF。我们展示了CRF如何应用于存在单词边界歧义的情况。CRF为基于语料库或统计日语形态学分析中长期存在的问题提供了解决方案。首先，分层标记集的灵活特征设计成为可能。其次，标签和长度偏差的影响被最小化。我们在用于日语形态学分析的标准测试床语料库上进行了CRF实验，并使用与本任务之前报告的HMMs和MEMMs相同的实验数据集对我们的结果进行了评估。我们的结果证实，与HMMs和MEMMs相比，CRF不仅解决了长期存在的问题，而且提高了性能。我们研究了使用条件随机场（CRF）和基于规则的未知词处理的日语分词和词性标注。"}
{"pid": "E03-1071", "zh_sum": "研究GIS和最大熵标记平滑。本文研究了最大熵标记的两个要素：在广义迭代缩放（GIS）估计算法中使用校正功能和模型平滑技术。我们通过分析和经验证明，假设GIS的正确性需要校正功能，但校正功能是不必要的。我们还探讨了高斯先验和简单截止的平滑使用。实验用两个标记集进行：标准的Penn Treebank词性标记集和组合范畴语法中较大的词性类型集。我们的supertagger找到给定sentenc的单个最可能的类别序列，并使用根据先前指定的类别定义的其他特征。"}
{"pid": "N06-1006", "zh_sum": "学习识别有效语篇蕴涵的特征本文提出了一种新的语篇推理体系结构，在该体系结构中，发现良好的对齐与评价蕴涵是分离的。当前问答和文本蕴涵中的语义推理方法将蕴涵问题近似为使用局部可分解匹配分数计算假设与文本的最佳对齐。我们认为这种方法存在明显的弱点，包括对单调性和局部性的错误假设。相反，我们提出了一种流水线方法，其中对齐之后是分类步骤，在此步骤中，我们提取表示蕴涵问题高级特征的特征，并将得到的特征向量传递给根据开发数据训练的统计分类器。我们报告了2005年Pascal RTE挑战赛的数据结果，超过了之前报告的基于校准系统的结果。我们强调，推理有效性比紧密的词汇或结构对应关系更重要：否定、模型、非事实动词和隐含动词以及其他语言结构都会以难以捕捉的方式影响有效性。"}
{"pid": "N09-1028", "zh_sum": "使用依赖解析器改进主宾动词语言的SMT，我们在统计机器翻译系统中引入了一种新的基于依赖解析器的优先级重新排序方法。与其他预处理重排序方法类似，我们的方法可以在不增加解码复杂度的情况下将语言知识有效地融入到SMT系统中。对于一组五种主宾动词（SOV）顺序语言，与其他重新排序方法相比，在最先进的基于短语的SMT系统中，我们显示出从英语翻译时BLEU分数的显著提高。我们表明，通过对平行数据的源端重新排序，可以改进主语-动词-宾语（英语）和主语-宾语-动词（普什图）语言之间的翻译。在网络文本中，我们报告了在将英语翻译成五种SOV语言（朝鲜语、日语、印地语、乌尔都语和土耳其语）时应用一套手工规则的重大改进。"}
{"pid": "D12-1050", "zh_sum": "基于向量的语义合成表示方法的比较本文讨论了使用分布方法对短语和句子的合成意义进行建模的问题。我们尝试了几种可能的表现和构图组合，表现出不同程度的复杂性。有些是浅薄的，而另一些则是对句法结构进行操作，依赖于参数学习，或者需要访问非常大的语料库。我们发现，对于两个特定的测试，浅层方法与计算密集型方法一样好：（1）短语相似性和（2）释义检测。所涉及的训练语料库和生成的向量的大小不如意义表示和合成方法之间的匹配重要。我们计算出现在待分类文档中的单词嵌入的加权线性组合。我们比较计数和预测表示作为合成函数的输入。对于释义检测，我们使用句子对之间的余弦相似度以及两个浅层相似度提示：两个句子之间的单词重叠和句子长度的差异。Add和mult凭借两个功绩指标的简单模型获得了最佳性能。"}
{"pid": "W04-3239", "zh_sum": "半结构化文本分类的Boosting算法文本分类的研究重点已从简单的主题识别扩展到更具挑战性的任务，如观点/模态识别。不幸的是，后一个目标超出了传统的词袋表示方法的能力，需要更丰富、更结构化的表示。因此，必须创建能够处理文本中观察到的结构的学习算法。在本文中，我们提出了一种Boosting算法来捕获嵌入文本中的子结构。该方案包括i）使用子树作为特征的决策树桩和ii）使用基于子树的决策树桩作为弱学习者的Boosting算法。我们还讨论了该算法与树核支持向量机的关系。两次意见/情态分类实验证实了子树特征的重要性。我们采用BACT学习算法有效地学习对先行词识别和零代词检测都有用的子树。"}
{"pid": "I05-3017", "zh_sum": "第二届国际汉语分词大赛第二届国际汉语分词大赛于2005年夏天举行，旨在评估分词技术的现状。23个小组提交了130个结果集，分两个轨道和四个不同的语料库。我们发现，在这两年中，这项技术有所改进，尽管词汇表外的问题仍然至关重要。在第二届国际汉语分词比赛中，封闭式比赛中得分最高的两个系统都基于CRF模型。"}
{"pid": "N01-1025", "zh_sum": "支持向量机组块我们应用支持向量机（SVM）来识别英语基本短语（组块）。众所周知，支持向量机即使在输入高维特征空间的数据时也能获得很高的泛化性能。此外，根据核原理，支持向量机能够以较小的计算开销进行训练，而与维数无关。我们对8个基于支持向量机的系统进行加权投票，这些系统使用不同的组块表示进行训练。实验结果表明，我们的方法比以前的方法获得了更高的精度。在本文中，我们开发了一个基于支持向量机的分块工具YamCha。"}
{"pid": "D07-1076", "zh_sum": "基于树核的上下文敏感结构化解析树关系提取本文提出了一种基于上下文敏感结构化解析树信息的树核用于关系提取。它通过两种方式解决了以往关系抽取树核中的两个关键问题。首先，通过将广泛使用的最短路径封闭树（SPT）扩展为包含SPT之外的必要上下文信息，自动确定用于关系提取的动态上下文敏感树跨度。其次，提出了一种上下文敏感卷积树核，该核通过将其祖先节点路径作为上下文来枚举上下文无关和上下文敏感的子树。此外，本文还评估了我们的树核和最先进的线性核之间的互补性。对ACE RDC语料库的评估表明，我们的动态上下文敏感树跨度比SPT更适合关系提取，我们的树核优于最先进的Collins和Duffy卷积树核。它还表明，我们的树核实现了比最先进的线性核更好的性能。结果表明，基于特征的方法和基于树核的方法是相辅相成的，复合核可以很好地集成平面特征和结构化特征。我们的复合内核部分依赖于完整解析，部分依赖于浅层语法特征的集合。"}
{"pid": "W04-3206", "zh_sum": "扩展基于网络的蕴涵关系习得-释义识别是自然语言口译的关键步骤。因此，许多NLP应用程序将受益于释义的高覆盖率知识库。然而，最先进的释义获取方法的可扩展性仍然有限。我们提出了一种基于Web的蕴涵关系提取的完全无监督学习算法，这是一种扩展的释义模型。我们的重点是相对于以前的工作提高可扩展性和通用性，最终目标是建立一个全面的知识库。我们当前的算法实现将动词词典作为输入，并针对每个动词在Web上搜索相关的语法蕴涵模板。实验表明，在最终目标方面取得了令人满意的结果，实现了比以前基于Web的方法更好的可扩展性。我们描述了从Web中提取隐含关系模板的梳理方法。"}
{"pid": "P03-1029", "zh_sum": "一种改进的用于IE模式自动获取的提取模式表示模型描述了用于信息提取的模式自动无监督获取的几种方法。每种方法都基于要获取的模式的特定模型，例如谓词参数结构或依赖链。这些替代模型的影响之前尚未研究过。在本文中，我们比较了以前的模型，并介绍了一种新的模型，子树模型，基于依赖树的任意子树。我们描述了该模型的发现过程，并通过实验证明了使用子树模式在回忆方面的改进。我们的方法包括三个阶段，用于从用户指定的场景的源文档中学习提取模式。我们使用TF*IDF度量的频繁依赖子树来识别对给定域很重要的命名实体和IE模式。我们还提出了IE模式的表示，扩展了SVO表示。"}
{"pid": "P03-1011", "zh_sum": "基于松散树的机器翻译对齐我们扩展了一个基于句法树中节点重新排序的翻译模型，以允许对齐不符合原始树结构，同时保持句子长度的计算复杂性多项式。这是通过向树到字符串或树到树对齐算法添加新的子树克隆操作来实现的。我们发现平行树过度约束了对齐问题，使用树到字符串模型比使用两棵树的树到树模型取得了更好的结果。我们在韩语-英语树库的平行成分树上训练一个系统，用手工标注的单词对齐来评估一致性。“克隆”操作允许单词对齐，即使是在完全不匹配的树的情况下，也要以对齐概率为代价。"}
{"pid": "W06-1606", "zh_sum": "SPMT：目标语短语句法化的统计机器翻译我们介绍了SPMT，这是一种新的使用目标语短语句法化的统计翻译模型。SPMT模型在NIST 2003中文-英语测试语料库中的表现优于最先进的基于短语的基线模型，在基于人的质量指标上的表现优于2.64个Bleu分，而在基于人的质量指标上的表现则优于0.28个分，该质量指标对翻译进行了从1到5的分级。"}
{"pid": "A94-1016", "zh_sum": "三个头比一个头好机器翻译（MT）系统目前无法在自由文本上实现最佳质量的翻译，无论它们采用何种翻译方法。我们的假设是，如果机器翻译环境使用不同机器翻译系统对同一文本的输出，机器翻译的质量将会提高。在最新版本的泛光泽机器翻译项目中，我们在图表数据结构中收集了三个翻译引擎的结果——通常是亚句子块。由于各个MT系统完全独立运行，其结果可能不完整、冲突或冗余。我们使用简单的评分启发式方法来估计每个块的质量，并找到得分最高的块序列（“最佳覆盖”）。本文详细描述了组合方法，给出了算法，并举例说明了它在许多实际翻译中的进展。它使用动态规划有效地比较相邻评分组件翻译集的加权平均值。当前系统主要在人工辅助机器翻译模式下运行。简要介绍了翻译交付系统及其相关的后期编辑助手，以及对该方法有用性的初步评估。单独的MT发动机将单独报告，因此，此处不作详细描述。我们根据三种不同MT发动机对发动机内部工作原理的了解，将其输出组合在一起，从而生产出第一个MEMT系统。我们开发了一个多引擎机器翻译系统，该系统使用每个输入系统中的翻译单元构建一个图表，然后使用图表遍历算法来找到源句子的最佳覆盖。"}
{"pid": "W04-3201", "zh_sum": "最大裕度分析受支持向量机大裕度准则的启发，我们提出了一种新的区分性分析方法。我们的公式使用类似于标准动态程序的因式分解进行解析。特别是，它允许人们有效地学习在整个解析树空间中进行区分的模型，而不是重新排列前几个候选树。我们的模型可以以输入句子的任意特征为条件，从而合并了一种重要的词汇信息，而不会增加建模头部的算法复杂性。我们提供了一种有效的算法来学习这些模型，并通过实验证明了该模型比自然基线模型和词汇化概率上下文无关语法有更好的性能。我们提出了一种最大裕度解析方法，该方法将动态规划方法用于解码和参数估计问题。"}
{"pid": "A97-1039", "zh_sum": "一个用于文本生成系统的快速便携式Realizer我们发布了一个surface Realizer，RealPro，它是一个现成的插件Realizer。我们的RealPro surface realizer可生成表面语言话语。"}
{"pid": "D07-1071", "zh_sum": "在线学习用于解析为逻辑形式的松弛CCG语法我们考虑了学习将句子解析为其底层语义的lambda演算表示的问题，并提出了一种学习加权组合范畴语法（CCG）的算法。一个关键的想法是引入非标准CCG组合词，以学习成本放松语法的某些部分，例如允许灵活的词序或插入词汇项。我们还提出了一种新的在线算法来推导加权CCG。ATIS数据上的方法结果显示，86%的F-测量值可恢复完全正确的语义分析，95.9%的F-测量值符合部分匹配标准，比He和Young（2006）报告的90.3%部分匹配值提高了5%以上。我们开发了用于语义分析的ATIS数据集。我们开发了一个集合，其中包括对词汇选择和所构建的逻辑形式的结构敏感的特征。我们介绍了标准应用程序、组合和协调组合符，以及类型转换规则来模拟自发的、未编辑的文本。"}
{"pid": "W06-1639", "zh_sum": "退出投票：从国会辩论记录中确定支持或反对我们调查是否可以从美国国会辩论记录中确定这些演讲是否代表对拟议立法的支持或反对。为了解决这个问题，我们利用了这样一个事实，即这些演讲是作为讨论的一部分进行的；这使我们能够使用有关话语片段之间关系的信息来源，例如某一特定话语是否表示与另一个表达的观点一致。我们发现，与孤立地对演讲进行分类相比，纳入这些信息会产生实质性的改进。我们提出了一种基于支持向量机的方法，利用美国国会辩论记录来确定参与者的发言是否代表对拟议立法的支持或反对。"}
{"pid": "P08-1067", "zh_sum": "森林重排：具有非局部特征的区分性解析传统的n-best重排技术通常受到nbest列表范围的限制，这排除了许多潜在的好选择。相反，我们提出了林重排序，这是一种对包含指数级多个解析的压缩林进行重排序的方法。由于精确推理难以处理非局部特征，我们提出了一种受森林重分类启发的近似算法，使区分性训练在整个树库中变得实用。我们的最终结果，F分数为91.7，优于50个最佳和100个最佳重新排序基线，并且优于之前报道的在树库上训练的任何系统。我们表明，非局部特征的使用实际上对解析器性能有很大的贡献。为了修剪密林，我们使用内部和外部概率来计算最佳派生的距离，该距离穿过超边远离全局最佳派生。"}
{"pid": "J88-2003", "zh_sum": "时间本体论和时间引用语言中时间类别的语义及其用于定义事件之间时间关系的理论都需要比通常假设的更复杂的语义表示域结构。本文提出了一种基于因果关系和结果等概念的本体论，而不是基于纯粹的时间原语。本体论中的一个核心概念是称为“核心”的基本事件复合体核心可以被认为是一个目标事件或“高潮”与完成它的“准备过程”以及随后的“结果状态”的关联。自然语言类别，如方面、将来词、状语和when子句，在这种核知识表示结构的控制下，可以改变命题的时间/方面类别。同一个核心概念在时间指称理论和时态语义学理论中起着核心作用，我们遵循McCawley、Partee和Isard将其视为回指范畴。我们声称，任何可管理的自然语言时态描述形式都必须体现这样一种本体论，任何可用的时态数据库都必须体现这样一种本体论，以获取有关使用自然语言查询的事件的知识。我们描述了与状态变化相关的时间表达式。"}
{"pid": "W97-0802", "zh_sum": "GermaNet-德语词汇语义网我们为德语“GermaNet”提供了词汇语义网，它将概念本体信息与词汇语义集成在词类内和词类间。它与普林斯顿WordNet兼容，但在结构和组织层面以及词汇和概念关系层面上集成了基于原则的修改。日耳曼网对规则多义词、人工概念和助词动词进行了新的处理。它还对交叉分类和基本语法信息进行编码，构成了探索语法和语义交互的有趣工具。开发如此大规模的资源尤为重要，因为到目前为止，德语缺乏对大型语料库进行语义探索的基本在线工具。GermaNet是一个大型词汇数据库，其中单词与词性信息和语义分类相关联，这些信息和语义分类以细粒度层次结构进行组织。"}
{"pid": "W02-1001", "zh_sum": "隐马尔可夫模型的区分性训练方法：感知器算法的理论与实验我们描述了用于训练标记模型的新算法，作为最大熵模型或条件随机场（CRF）的替代方法。该算法依赖于训练示例的维特比解码，并结合简单的加法更新。我们描述了通过修改分类问题感知器算法的收敛性证明来证明算法合理性的理论。我们给出了词性标注和基本名词短语组块的实验结果，在这两种情况下，与最大熵标注器的结果相比都有所改进。我们描述了如何使用投票感知器来训练最大熵风格的标记器，并讨论了感知器算法应用于排序任务背后的理论。投票感知器训练尝试最小化训练实例的全局特征向量与相同特征向量之间的差异，以根据当前模型对该实例进行最佳评分标记。"}
{"pid": "P07-1040", "zh_sum": "最近，混淆网络解码被应用于机器翻译系统组合中。由于假设对齐中的错误，解码可能会导致不符合语法的组合输出。本文描述了一种改进的基于混淆网络的多机器翻译系统输出合并方法。在这种方法中，可以将任意特征对数线性地添加到目标函数中，从而允许语言模型扩展和重新评分。此外，还提出了一种自动选择其他假设所对应的假设的新方法。通用权重调整算法可用于优化各种自动评估指标，包括TER、BLEU和METEOR。使用2005年阿拉伯文到英文和中文到英文的NIST机器翻译评估任务进行的实验表明，与早期基于混淆网络解码的方法相比，BLEU分数有显著提高。我们使用tercom脚本（Snover et al，2006），该脚本使用许多启发式方法（以及动态编程）来查找将输入字符串转换为另一个字符串的编辑序列（插入、删除、替换和块移位）。我们提出了一个多重混淆网络（CN）或超级网络框架，在该框架中，我们使用所有单个系统结果作为主干，以基于对齐度量TER构建CNs。混淆网络中的每个单词都与单词后验概率相关联。"}
{"pid": "W02-2026", "zh_sum": "本文提出了一种不需要平行双语语料库或直接双语种子词典就可以在两种遥远的语言之间建立翻译词典的方法。该算法成功地结合了新闻语料库中跨日期的时间发生相似度、广域和局部跨语言上下文相似度、加权Levenshtein距离、相对频率和突发性相似度。这些相似性度量在斯拉夫语和北印度语系的稳健分类器组合方法下与桥梁语言概念相结合。我们使用与目标语言相关的桥梁语言，归纳出没有共同平行语料库的语言的翻译词汇。我们在源语言和目标语言单词周围创建单词包上下文向量，然后通过当前的小翻译词典将源向量投影到目标空间。"}
{"pid": "A97-1052", "zh_sum": "从语料库中自动提取子类别我们描述了一种从文本语料库中构建子类别词典的新技术和实现系统。每个词典条目都对英语的一整套子类别的相对出现频率进行编码。在14个呈现多种补语模式的动词样本上进行的初步实验表明，该方法的准确性与之前的方法相当，而之前的方法都局限于一组高度受限的子类别。我们还证明了使用该系统构建的子类别化词典可以显著提高解析器的准确性。我们使用语法和复杂的分析工具来区分参数附加项。"}
{"pid": "P12-1092", "zh_sum": "通过全局上下文和多个单词原型改进单词表达无监督单词表达在NLP任务中非常有用，既可以作为学习算法的输入，也可以作为NLP系统中的额外单词特征。然而，这些模型中的大多数都是建立在只有本地上下文和每个单词一个表示的基础上的。这是有问题的，因为单词通常是多义的，全局上下文也可以为学习单词的含义提供有用的信息。我们提出了一种新的神经网络结构，它1）学习单词嵌入，通过结合本地和全局文档上下文更好地捕获单词的语义；2）通过学习每个单词的多个嵌入来解释同音异义和多义异义。我们引入了一个新的数据集，该数据集包含人类对句子语境中单词对的判断，并对我们的模型进行了评估，结果表明我们的模型优于竞争基线和其他神经语言模型。我们的表示旨在捕获词义消歧。"}
{"pid": "W06-3105", "zh_sum": "为什么生成短语模型不如表面启发式我们研究了为什么在基于短语的机器翻译中，生成模型的权重不如启发式估计。我们首先提出了一个简单的生成、基于短语的模型，并验证了其估计值低于表面统计给出的估计值。性能差距主要源于添加了一个隐藏的切分变量，这增加了使用EM进行最大似然训练时的过度拟合能力。特别是，虽然单词级模型从重新估计中受益匪浅，但短语级模型却没有：关键的区别在于，不同的单词对齐不能都是正确的，而不同的切分可以。交替分段而非交替对齐相互竞争，导致短语表的确定性增加，泛化程度降低，最终BLEU分数降低。我们还表明，这两种方法的插值可以导致BLEU分数的适度增加。我们尝试了另一种类似于IBM word translation model 3（Brown et al.，1993）的生成短语翻译模型，再次发现标准模型优于其生成模型。我们探讨了在基于原始信源信道公式的条件翻译模型下，使用EM估计短语对概率。我们得出结论，生成翻译模型中的分段变量会导致过度拟合，同时获得比启发式估计器更高的训练数据可能性。"}
{"pid": "A00-2031", "zh_sum": "将函数标记分配给已解析的文本通常认为，句法成分（NP、VP等）的常见非末端标记不会耗尽人们想要的有关句法树部分的句法和语义信息。例如，Penn Treebank为每个成分提供了零个或多个“功能标签”，表示语义角色和其他不容易封装在简单成分标签中的相关信息。我们提出了一种用于分配这些函数标记的统计算法，在已解析到简单标签级别的文本上，该算法实现了87%的F-度量，当将“无标记”视为有效选择时，F-度量上升到99%。作为硬编码启发式的替代方法，我们建议自动恢复Penn函数标记。"}
{"pid": "A00-2009", "zh_sum": "构建用于词义消歧的朴素贝叶斯分类器集合的一种简单方法本文提出了一种基于语料库的词义消歧方法，该方法构建了一个朴素贝叶斯分类器集合，每个朴素贝叶斯分类器都基于在不同大小的上下文窗口中表示共现词的词汇特征。尽管这种方法很简单，但对广泛研究的名词“行”和“兴趣”进行消歧的实证结果表明，这样的集合可以达到与之前公布的最佳结果相媲美的精度。我们提出了一个由81个朴素贝叶斯分类器组成的集合，这些分类器基于目标词左右两侧不同大小的上下文窗口来定义共现特征。"}
{"pid": "J87-1004", "zh_sum": "一种高效的增广上下文无关语法分析算法介绍了一种高效的增广上下文无关语法分析算法，并讨论了其在在线自然语言接口中的应用。该算法是一种广义的LR解析算法，它从给定的扩充上下文无关语法中预计算LR shift-reduce解析表（可能有多个条目）。与标准LR解析算法不同，它可以处理任意上下文无关语法，包括歧义语法，而通过引入“图结构堆栈”的概念，大部分LR效率得以保持。图结构堆栈允许LR shift-reduce解析器维护多个解析，而无需以相同的方式对输入的任何部分进行两次解析。我们还可以将我们的解析算法视为由LR解析表有效引导的扩展图表解析算法。由于LR表的预计算，该算法速度很快。在几个不同英语语法和句子的实验中，计时显示出比Earley的上下文无关解析算法快五到十倍的速度优势。该算法严格地从左到右在线解析一个句子，也就是说，只要用户键入句子的第一个单词，它就开始解析，而不必等待句子完成。基于该算法的实用在线解析器已在通用Lisp中实现，并在Symbolics和HP AI工作站上运行。该解析器用于CMU的多语言机器翻译项目。此外，智能技术公司（Intelligent Technology Incorporation）正在基于CMU开发的技术构建一个商业化的日语在线解析器。"}
{"pid": "D11-1014", "zh_sum": "用于预测情感分布的半监督递归自动编码器我们介绍了一种基于递归自动编码器的机器学习框架，用于情感标签分布的句子级预测。我们的方法学习多词短语的向量空间表示。在情感预测任务中，这些表示在常用数据集（如电影评论）上优于其他最先进的方法，而不使用任何预定义的情感词典或极性转移规则。我们还评估了该模型在基于经验项目自白的新数据集上预测情绪分布的能力。该数据集由带有多个标签的个人用户故事组成，这些标签在聚合时形成一个多项式分布，捕捉情感反应。与多个竞争基线相比，我们的算法可以更准确地预测此类标签上的分布。我们介绍了一种半监督方法，该方法使用递归自动编码器来学习句子的层次结构和情感分布。"}
{"pid": "W05-0909", "zh_sum": "METEOR：机器翻译评估的自动度量，与人类判断的相关性得到改善。我们描述了METEOR，一种机器翻译评估的自动度量，它基于机器翻译和人类参考翻译之间的单图匹配的广义概念。单字可以根据其表面形式、词干形式和含义进行匹配；此外，METEOR可以很容易地扩展到包含更高级的匹配策略。一旦找到两个字符串之间的所有广义单字匹配，METEOR将使用单字精度、单字召回率和碎片度量的组合来计算该匹配的分数，该碎片度量旨在直接捕获机器翻译中匹配的单词相对于引用的有序程度。我们通过测量翻译质量的度量分数和人类判断之间的相关性来评估METEOR。我们计算了LDC TIDES 2003阿拉伯语到英语和汉语到英语数据集的得分与人类素质评估之间的Pearson R相关值。我们进行了逐段相关，结果表明METEOR在阿拉伯数据上的R相关值为0.347，在中国数据上的R相关值为0.331。这被证明是对简单使用单图精度、单图召回及其谐波F1组合的改进。我们还进行了实验，以显示各种映射模块的相对贡献。"}
{"pid": "W02-1006", "zh_sum": "对词义消歧的知识源和学习算法的实证评估本文在SENSEVAL-2和SENSEVAL-1数据上评估了各种用于词义消歧的知识源和监督学习算法。我们的知识来源包括相邻单词的词性、周围语境中的单个单词、局部搭配和句法关系。评估的学习算法包括支持向量机（SVM）、朴素贝叶斯、AdaBoost和决策树算法。我们给出的实证结果显示了组件知识源和不同学习算法的相对贡献。特别是，使用所有这些知识源和SVM（即单一学习算法）可以获得比SENSEVAL-2和SENSEVAL-1测试数据的最佳官方分数更高的准确度。我们的特征集由以下四种类型组成：邻近单词的局部上下文n-gram、给定上下文中所有单词的全局上下文、邻近单词的词性n-gram和从解析器输出获得的语法信息。"}
{"pid": "W04-3252", "zh_sum": "TextRank：将顺序引入文本本文介绍了一种基于图形的文本处理排序模型TextRank，并展示了该模型如何成功地应用于自然语言应用程序。特别是，我们提出了两种创新的无监督关键词和句子提取方法，并表明所获得的结果与之前在已建立的基准上发布的结果相比是令人满意的。我们提出了TextRank，这是最著名的基于图的关键短语提取方法之一。我们提出了基于词间共现联系的TextRank模型对关键词进行排序。"}
{"pid": "J04-4002", "zh_sum": "介绍了一种基于短语的统计机器翻译方法——对齐模板方法。这种翻译方法考虑到词之间的一般多对多关系。因此，在翻译模型中考虑了单词的上下文，可以明确地了解从源语言到目标语言的词序的局部变化。该模型使用对数线性建模方法进行描述，该方法是常用源信道方法的推广。因此，该模型比经典的统计机器翻译系统更易于扩展。我们详细描述了短语翻译的学习过程、使用的特征函数和搜索算法。该方法的评估是在三个不同的任务上进行的。对于德语-英语语音Verbmobil任务，我们分析了各种系统组件的影响。在法语-英语-加拿大汉莎语任务中，对齐模板系统比基于单个单词的翻译模型获得了更好的结果。在2002年的中英文国家标准与技术研究所（NIST）机器翻译评估中，统计上NIST的得分明显优于所有竞争研究和商业翻译系统。我们描述了一种短语提取算法，用于从带有1-最佳对齐注释的句子对中提取短语对。"}
{"pid": "W02-1039", "zh_sum": "短语衔接与统计机器翻译利用短语移动来提高统计机器翻译一直备受关注。我们将探讨两种语言（特别是英语和法语）中短语的衔接情况，并研究它们在哪些特定条件下不衔接。我们证明，虽然在某些情况下连贯性较差，但统计机器翻译系统可以利用许多规律。我们还比较了三种不同的句法表征，以确定哪一种在衔接方面具有最好的特性。我们通过计算交叉点来衡量金本位路线中的短语内聚力。我们比较了树库解析器风格分析，这是一种具有扁平VPs和依赖结构的变体。"}
{"pid": "J08-4003", "zh_sum": "确定性增量依赖解析算法从左到右处理输入并构造单一派生的解析算法通常被认为不适合自然语言解析，因为自然语言语法中通常存在大量的歧义。然而，已经证明，这种算法与树库诱导的分类器相结合，可以用来构建高度精确的消歧解析器，尤其是基于依赖关系的语法表示。在本文中，我们首先提出了一个描述和分析确定性增量依赖解析算法的通用框架，形式化为转换系统。然后，我们描述并分析了这类算法的两个家族：基于堆栈的算法和基于列表的算法。在前一个仅限于投射依赖结构的族中，我们描述了一个arc-eager和一个arc-standard变体；在后一个家族中，我们提出了一个投射变体和一个非投射变体。对于这四种算法中的每一种，我们都给出了正确性和复杂性的证明。此外，我们使用13种语言的数据，结合支持向量机分类器对所有算法进行了实验评估，以预测下一个解析动作。我们证明，这四种算法都具有竞争性的准确性，尽管对于非投影结构比例不可忽略的语言，基于非投影列表的算法通常优于投影算法。然而，投影算法与伪投影分析技术相结合时，通常会产生类似的结果。基于堆栈的算法的线性时间复杂度使其在学习和解析效率方面都具有优势，但基于投影列表的算法在实践中同样有效。此外，当使用投影算法实现伪投影解析时，它们有时在解析（但不是学习）方面的效率低于基于非投影列表的算法。虽然之前的文献中已经部分描述了大多数算法，但这是第一次在统一的框架内对算法进行全面的分析和评估。我们系统地描述了arc标准和arc渴望算法，这是目前两种流行的基于转换的词级依赖分析方法。"}
{"pid": "P06-3002", "zh_sum": "采用高效图聚类的无监督词性标注描述了一种基于图聚类方法的无监督词性标注系统。与当前最先进的方法不同，不同标记的种类和数量是由方法本身生成的。我们计算并合并了两个词图分区：一个基于高频词的上下文相似度，另一个基于低频词的对数似然统计。将得到的词簇作为词典，训练维特比词性标记器，并通过形态学成分对其进行细化。通过测量与现有标记者的一致性，对三种不同语言的方法进行了评估。我们通过信息论方法直接比较了英语、德语和芬兰语的标记输出与监督标记输出。我们将词语网络概念化，以捕获词语共现模式。我们使用由最频繁的150-200个单词组成的上下文对最频繁的10000个单词进行聚类。"}
{"pid": "P06-1103", "zh_sum": "从多语可比语料库中进行弱监督命名实体音译和发现命名实体识别（NER）是许多自然语言处理任务的重要组成部分。当前的方法通常采用机器学习技术，并且需要有监督的数据。然而，许多语言缺乏这种资源。本文提出了一种（几乎）无监督的学习算法，用于自动发现无资源语言中的命名实体，给出了一个双语语料库，该语料库在时间上与资源丰富的语言弱对齐。NE在这些语料库中具有相似的时间分布，多词NE中的一些标记通常是音译的。我们开发了一种迭代利用这两个观测值的算法。该算法利用了一种新的基于频率的时间分布度量和一种无资源的识别性音译方法。该算法以少量音译对为种子，发现多个单词NEs，并利用字典（如果存在）来解释翻译或部分翻译的NEs。我们在一个英俄语料库上对该算法进行了评估，结果表明该算法在俄语中具有很高的NEs发现水平。我们探索了基于感知器的排名模型的使用，目的是在可比较的语料库中发现名称的音译。我们的特征空间由两种语言的n-gram对组成。我们发现，与投影其他类型的注释（如POS标记和BPC）相比，NER标记的投影更容易。我们介绍了一个由时间对齐的新闻文章组成的俄罗斯数据集。"}
{"pid": "P04-1061", "zh_sum": "基于语料库的句法结构归纳：依赖和选区模型我们提出了一个用于依赖结构无监督学习的生成模型。我们还描述了这种依赖模型与线性选区模型的乘法组合。产品模型在各自的评估指标上优于这两个组件，为无监督依赖关系分析和无监督选区分析提供了最佳的发布数据。我们还证明，该组合模型在跨语言方面有效且稳健，能够利用数据中显著的依附或分布规律。我们的贡献包括带价的生成依赖模型（DMV）。我们认为，在评估无监督语法分析器时，一致的语法表示是可取的。"}
{"pid": "S10-1011", "zh_sum": "SemEval-2010任务14：词义归纳&#x26；本文介绍了SemEval-2010词义归纳与消歧任务的描述和评估框架，以及26个参与系统的评估结果。在这项任务中，参与者被要求使用一个训练集归纳出100个目标单词的词义，然后使用归纳出的词义消除同一单词中看不见的实例的歧义。系统答案的评估方式为：（1）使用两种聚类评估方法，以无监督的方式进行，（2）在WSD任务中以有监督的方式进行。在构建数据集时，我们使用WordNet首先随机选择单词的一个词义，然后根据第一个单词所选的语法集构建一组单词。"}
{"pid": "N04-4026", "zh_sum": "统计机器翻译的单图方向模型本文提出了一种统计机器翻译的单图切分模型，其中切分单元是块：没有内部结构的短语对。分割模型使用一种新的方向组件来处理相邻块的交换。在训练期间，我们收集具有方向的块单图计数：我们计算一个块出现在某个前置块的左侧或右侧的频率。与两种模型相比，定向模型可以提高翻译性能：1）不使用块重新排序，2）块交换仅由语言模型控制。我们展示了标准阿拉伯语-英语翻译任务的实验结果。这项工作介绍了失真建模的词汇特征。"}
{"pid": "W99-0501", "zh_sum": "WordNet 2-一个形态学和语义增强的资源本文介绍了一个正在进行的项目，旨在增强WordNet的形态学和语义。这项工作的动机源于WordNet当前用作语言知识库时的局限性。我们设想一个软件工具，可以自动解析概念定义的修饰语，对词性标记和短语括号进行归属。然后，每个定义中的名词、动词、形容词和副词都会消除歧义，并与相应的语法集联系起来。这增加了语法集之间的连接，允许检索与主题相关的概念。此外，该工具还将光泽先转换为逻辑形式，然后再转换为语义形式。使用派生形态学在语法集之间添加新的链接。在将WordNet gloss转换为逻辑形式的框架内，我们提出了一种将语义标记附加到谓词的方案。扩展WordNet是WordNet的一个公开版本，其中WordNet gloss中出现的每个术语（示例短语中的术语除外）都被引理化并映射到它所属的语法集。"}
{"pid": "P06-1104", "zh_sum": "一种用于提取具有平面特征和结构化特征的实体之间关系的复合核本文提出了一种用于关系提取的复合核。复合内核由两个单独的内核组成：一个允许实体相关特性的实体内核和一个对关系示例的语法信息建模的卷积解析树内核。我们的方法的动机是充分利用核方法的优良特性来探索关系抽取的各种知识。我们的研究表明，复合内核可以有效地捕获平面和结构化特征，而无需进行大量的特征工程，并且可以轻松地扩展以包含更多的特征。对ACE语料库的评估表明，我们的方法在关系提取方面优于以前的最佳报告方法，并且显著优于以前的两个依赖树核。我们使用卷积树核（CTK、Collins和Duffy，2001）研究了用于关系提取的各种结构化信息，发现最短路径封闭树（SPT）在ACE RDC 2004语料库的7种关系类型上达到了67.7的F-度量。"}
{"pid": "J91-1003", "zh_sum": "met*：一种通过计算机区分转喻和隐喻的方法met*方法将选定的转喻示例与隐喻、短句中的文字性和异常区分开来。在met*方法中，文字性是有区别的，因为它满足上下文约束，而非文字性的其他所有约束都违反了上下文约束。转喻与隐喻和异常的区别【1】支持Lakoff和Johnson（1980）的观点，即在转喻中，一个实体代表另一个实体，而在隐喻中，一个实体被视为另一个实体，【2】允许转喻链（Reddy 1979），并且【3】允许转喻与文学性、隐喻或异常的实例同时发生。隐喻与异常不同，因为前者包含一个相关的类比，而非后者。met*方法是排序语义（一种用于自然语言处理的语义）的一部分，并已在名为meta5的计算机程序中实现。文中列举了meta5对隐喻和转喻的分析实例。met*方法与人工智能、语言学、哲学和心理学的方法进行了比较。我们使用选择性偏好违背技术来检测隐喻。我们开发了一个名为met*的系统，能够区分文字性、转喻、隐喻和异常。我们构建了一个系统met*，该系统旨在区分隐喻和转喻与字面文本，并提供处理这些比喻语言实例的特殊技术。我们开发了一个名为met*的系统，能够区分文字性、转喻、隐喻和异常。"}
{"pid": "N10-1063", "zh_sum": "利用文档级对齐从可比语料库中提取平行句统计机器翻译（SMT）系统的质量在很大程度上取决于训练中使用的平行句数量。近年来，已经开发了几种从非平行或可比数据中获取平行句子的方法，例如在同一时间段内发表的新闻文章（Munteanu和Marcu，2005），或具有类似结构的网页（Resnik和Smith，2003）。维基百科（Wikipedia）是一种在线百科全书，包含多种语言的链接文章，目前尚未被彻底发掘。我们通过对文档级对齐进行建模，推动了平行句抽取的最新进展，这是因为我们发现平行句对往往非常接近。我们还包括使用维基百科提供的附加注释的功能，以及使用自动归纳词典模型的功能。文中给出了在SMT系统中句子提取的准确性和后续改进的结果。我们报告了使用更复杂的句子平行性指标挖掘平行维基百科文章的重大改进，在条件随机场（CRF）模型中加入了更丰富的特征集和跨句子依赖性。"}
{"pid": "W03-0407", "zh_sum": "基于未标记数据的Bootstrapping词性标记器本文研究了使用联合训练的boosttrapping词性标记器，其中两个标记器对彼此的输出进行迭代重新训练。由于标记器的输出是有噪声的，因此有一个问题，即要将哪些新标记的示例添加到训练集中。我们研究了通过直接最大化未标记数据上的tagger一致性来选择示例，这是一种在联合培训文献中从理论和经验上都受到激励的方法。我们的结果表明，基于协议的联合训练可以显著提高小型种子数据集的标记性能。进一步的结果表明，这种形式的共同培训远远超过了自我培训。然而，我们发现，在某些情况下，对所有新标记的数据进行简单的重新训练可以产生与基于协议的联合训练相当的结果，只需计算成本的一小部分。我们报告了少量标记训练数据的阳性结果，但当标记训练数据量增加时，则为阴性结果。我们将自我训练定义为一个过程，在该过程中，标记者在每一轮中都在其自己的标记缓存上重新训练。"}
{"pid": "J08-4004", "zh_sum": "这篇文章是对语料库注释者之间一致性度量方法的综述。它揭示了一致性系数的数学和基本假设，包括Krippendorff的alpha以及Scott的pi和Cohen的kappa；讨论了系数在几个注释任务中的使用；并认为，在计算语言学中，加权的类α系数通常比类kappa系数使用较少，可能更适合于许多语料库注释任务，但它们的使用使得对系数值的解释更加困难。本文全面概述了计算语言学各个领域中用于衡量注释者之间一致性的方法。"}
{"pid": "P86-1004", "zh_sum": "恢复隐含信息本文描述了SDC专家（Prolog理解集成文本）处理自然语言消息的系统。PUNDIT用Prolog编写，是一个高度模块化的系统，由不同的语法、语义和语用组成。每个组件都利用一组或多组数据，包括词典、英语的广泛语法、语义动词分解、句法和语义成分之间的规则映射以及域模型。本文讨论了使隐性语言信息显性化所必需的句法、语义和语用模块之间的交流。关键是让语法和语义将缺失的语言实体识别为隐式实体，这样就可以将它们标记为隐式实体，并且可以指导被引用者解析以找到实体的特定引用。这样，使隐性语言信息显性化的任务就成为了参照消解任务的一个子集。这种方法的成功取决于将缺失的句法成分标记为省略，将缺失的语义角色标记为必要，以便参照解析可以知道何时查找参照物。我们首次尝试了隐式语义角色的自动标注。我们最早的尝试之一是自动恢复句子外的论点。"}
{"pid": "P90-1032", "zh_sum": "自动提取和表示搭配用于语言生成搭配知识是语言生成所必需的。问题是搭配有多种形式。它们可以包含两个、三个或更多的单词，这些单词可以是不同的句法类别，它们可以以或多或少僵化的方式参与。这导致了两个主要困难：搭配知识必须获得，并且必须灵活地表示，以便用于语言生成。我们在本文中解决了这两个问题，重点是收购问题。我们描述了一个程序Xtract，该程序自动从大型文本语料库中获取一系列搭配，并描述了如何使用基于统一的形式主义在灵活的词典中表示这些搭配。"}
{"pid": "P93-1022", "zh_sum": "近几年来，人们对词语的共现关系，如n-gram、动宾组合或有限语境下的共现关系，产生了极大的兴趣。本文讨论了如何估计在训练数据中不发生共现的概率。我们提出了一种方法，通过适当的词语相似度度量，在每个特定的未观察到的共现和包含相似词语的其他共现之间进行局部类比。我们的评估表明，该方法的性能优于现有的平滑方法，并可能为基于类的模型提供一种替代方法。我们认为，使用相对较少的类来模拟单词之间的相似性可能会导致大量信息丢失。相似单词的聚类是根据它们能够一次一个地恢复从输入语料库中删除的数据项的程度来评估的。"}
{"pid": "P07-1049", "zh_sum": "快速无监督增量语法分析本文描述了一种增量语法分析器和一种从纯文本中归纳出该语法分析器的无监督学习算法。解析器使用类似于依赖关系链接的语法结构表示，非常适合增量解析。与以前的无监督解析器相比，解析器不使用词性标记，而且学习和解析都是局部的、快速的，不需要显式聚类或全局优化。解析器通过将其输出转换为等效括号来进行评估，并改进了先前发布的纯文本无监督解析结果。我们的增量解析方法使用了一种称为公共覆盖链接的新表示，可以将其转换为组成方括号。虽然在无监督的句法分析研究中，标点符号通常被完全忽略，但我们使用短语标点符号——通常在句子中标记短语边界的标点符号。"}
{"pid": "P04-1066", "zh_sum": "改进IBM Word Alignment Model 1我们研究了许多提高IBM Model 1的Word Alignment准确性的简单方法。我们证明，由于（1）对空词的对齐概率给予额外的权重，（2）对稀有词的平滑概率估计，以及（3）使用简单的启发式估计方法初始化或替换模型参数的EM训练，对齐错误率降低了约30%。IBM Model 1的一个限制是，目标句子中的每个单词最多只能由源句子中的一个单词生成。我们还建议在IBM Model 1的目标句子中添加多个空词。我们的方法还通过支持查询和文档中相邻单词之间的翻译，减轻了另一个相关限制。"}
{"pid": "D08-1068", "zh_sum": "联合无监督共指消解和马尔可夫逻辑机器学习方法共指消解通常是有监督的，并且需要昂贵的标记数据。已经提出了一些无监督的方法（如Haghighi和Klein（2007）），但它们的准确性较低。在本文中，我们提出了第一种无监督的方法，它与有监督的方法相竞争。与监督方法中通常使用的成对分类不同，这是通过在提及之间执行联合推理实现的，并且通过使用马尔可夫逻辑作为表示语言，这使我们能够轻松地表达同位语和谓词名词等关系。在MUC和ACE数据集上，我们的模型仅使用一小部分训练数据就优于Haghigi和Klein的模型，并且通常与最先进的监督模型的精度相匹配或超过。我们的经验表明，全局方法的性能优于基于增量处理文本的方法。我们的方法基于实体提及模型。在谓语主格结构中，连接动词的宾语（动词be的形式）被限定为与主语的中心词。"}
{"pid": "W04-2319", "zh_sum": "ICSI会议记录者对话行为（MRDA）语料库我们描述了一个新的语料库，包含180000多个手动注释的对话行为标签和伴随的邻接对注释，用于75次自然发生的会议大约72小时的演讲。我们简要介绍了注释系统和标记过程、注释者之间的可靠性统计、总体分布统计、随语料库分布的辅助文件的描述以及如何获取数据的信息。"}
{"pid": "P04-1005", "zh_sum": "一种基于标签的噪声信道语音修复模型本文描述了一种噪声信道语音修复模型，该模型可以识别和纠正语音记录中的修复。源模型采用语法分析器，信道模型采用一种新型的基于标签的转换器。TAG的使用源于这样一种直觉，即重新命名是修复的“粗略复制”。该模型在交换机不流利标注语料库上进行了训练和测试。噪声信道模型在不流畅检测任务中表现良好。虽然标准的噪声信道模型性能良好，但可以使用对数线性重新分级来提高性能。我们的标签系统获得了很高的编辑F分数，这主要是因为它明确跟踪了重新命名和更改之间的重叠单词。"}
{"pid": "P93-1041", "zh_sum": "基于词间相似度的文本分割本文提出了一种新的文本结构指标，即词汇衔接模式（LCP），它可以定位文本中的片段边界。文本片段是连贯的场景；片段中的单词通过词汇衔接关系连接在一起。LCP记录了一系列文本中单词的相互相似性。单词的相似度表示其内聚性，使用语义网络计算。与许多受试者标记的文本片段的比较表明，LCP和人类的判断密切相关。LCP可以为解决回指和省略提供有价值的信息。我们发现，使用与领域无关的知识源进行文本分割并不一定会比仅基于文本中单词分布的工作产生更好的结果。"}
{"pid": "W06-1607", "zh_sum": "统计机器翻译中的短语表平滑我们讨论了统计机器翻译中用于平滑短语表的不同策略，并给出了一系列翻译设置的结果。我们证明，任何类型的平滑都比通常使用的相对频率估计更好。根据BLEU度量，最佳平滑技术产生约1%（绝对）的一致增益。"}
{"pid": "J93-3003", "zh_sum": "线索短语消歧的实证研究线索短语是一种语言表达，例如now和well，可以作为话语结构的明确指标。例如，现在可能标志着一个副标题的开始或回到以前的主题，而well可能将后续材料标记为对以前材料的回应，或作为解释性评论。然而，虽然提示短语可以传达语篇结构，但每个提示短语也有一个或多个替代用法。例如，虽然在句子中可以顺便用作状语，但语篇的使用会引发离题。尽管区分话语和线索短语的句子使用对话语的解释和生成至关重要，但很少涉及说话人和听话人如何完成这种消歧的问题。本文报道了对线索短语的语篇和句子使用的实证研究结果，其中对基于文本和韵律的特征进行了检验，以消除歧义。基于这些研究，有人提出，语篇与句子的使用可以通过语调特征来区分，特别是音高重音和韵律短语。确定了一个描述这些区别的韵律模型。该模型与文本分析中可识别的特征相关联，包括正字法和词性，以允许将韵律分析的结果应用于生成适当的语调特征，用于合成语音中提示短语的话语和句子使用。在文献中，对话语标记语还没有统一的定义。我们发现，民族短语和音高重音在消除线索短语的歧义，从而帮助确定语篇结构方面发挥了作用。"}
{"pid": "J94-4001", "zh_sum": "基于连词结构检测的日语长句句法分析方法本文提出了一种句法分析方法，该方法首先通过检查两组词的平行性来检测句子中的连词结构，然后利用连词结构的相关信息来分析句子的依存结构。长句分析是自然语言处理中最困难的问题之一。造成这种困难的主要原因是在长句中出现的连词结构中常见的结构歧义。人类可以识别连词结构，因为连词之间存在某种相似性，但有时很细微。因此，我们开发了一种算法，用于计算连接词左右两个任意单词序列之间的相似度，并选择可以合理地认为构成连接结构的两个最相似的单词序列。这是使用动态规划技术实现的。通过识别连词结构，长句可以简化为短句。因此，可以通过相对简单的头部相关规则来获得句子的总体依赖结构。连词结构的一个严重问题是，除了连词范围的模糊性外，连词的一些成分也被省略了。通过依赖性分析过程，我们可以找到省略号并恢复省略的组件。我们报告了150个日语句子的分析结果，以说明该方法的有效性。我们提出了一种通过计算两个连词序列之间的相似度来检测连词结构的方法。我们提出了一种基于相似度的方法来解决日语的这两个任务。我们提出了一种包含坐标结构检测的日语句法分析方法。"}
{"pid": "W08-2123", "zh_sum": "基于依赖关系的语法&#x2013；PropBank和NomBank的语义分析本文介绍了我们在2008年CoNLL共享任务的封闭轨道上的贡献（Surdeanu等人，2008）。为了解决联合句法-语义分析的问题，该系统依赖于句法和语义子组件。语法模型是使用伪投影变换的自底向上投影解析器，语义模型在分类器管道上使用全局推理机制。从子系统生成的候选池中选择完整的语法语义输出。该系统在封闭式挑战赛中获得最高分：标记句法准确率为89.32%，标记语义F1为81.65，标记宏F1为85.49。我们的系统使用两个30个不同的子系统分别处理动词谓词和名词谓词。我们提出了在谓词变元结构分析中捕获核心变元的非局部依赖性的重要性。在我们的工作中，研究了不同语法表示对基于框架的浅层语义分析任务的影响，并概述了词汇泛化能力差的问题。"}
{"pid": "A00-1043", "zh_sum": "自动文本摘要的句子约简我们提出了一种新的句子约简系统，用于从摘要文档中提取的句子中自动删除无关短语。该系统使用多个知识源来决定可以删除提取句子中的哪些短语，包括句法知识、上下文信息和从由专业人员编写的示例组成的语料库中计算出的统计信息。缩减可以显著提高自动摘要的简洁性。我们研究了一种新的方法，通过使用多个知识源来决定句子中哪些短语可以删除，从而从句子中删除无关短语。在我们的方法中，关于在句子摘要中包括/删除哪些材料的决定并不依赖于单词的相对频率信息，而是依赖于从句子及其摘要的语法分析语料库中学习到的子树删除的概率模型。"}
{"pid": "P03-1010", "zh_sum": "日文和英文新闻文章和句子对齐的可靠方法我们对齐了日文和英文新闻文章和句子，形成了一个大型平行语料库。我们首先使用基于跨语言信息检索（CLIR）的方法对齐日文和英文文章，然后使用基于动态规划（DP）匹配的方法对齐日文和英文文章中的句子。然而，结果包括许多不正确的对齐。为了消除这些问题，我们提出了两种评估对齐有效性的方法（分数）。文章对齐度量使用DP匹配对齐的句子中的相似度，而句子对齐度量使用CLIR对齐的文章中的相似度。它们相互增强，以提高对齐精度。利用这些方法，我们成功地构建了一个面向公众的大规模文章和句子对齐语料库。我们构建了一个由180K个句子对组成的自动句子对齐的日英读卖新闻语料库。我们使用BM25相似性度量。"}
{"pid": "N01-1023", "zh_sum": "将协同训练方法应用于统计句法分析，提出了一种新的统计句法分析协同训练方法。该算法以一个带有解析树注释的小语料库（9695个句子）、一个包含训练集中每个单词可能的词汇化结构的词典和一个大的未标记文本池作为输入。该算法使用解析树迭代地标记整个数据集。使用基于分析《华尔街日报》语料库的实证结果，我们表明，在组合标记和未标记数据上训练统计解析器的效果明显优于仅在标记数据上训练。我们的联合训练主要是无监督的算法，它通过两个（或更多）解析器为彼此标记训练示例来代替人类。"}
{"pid": "W01-0501", "zh_sum": "大数据集自然语言学习的协同训练局限性协同训练是一种弱监督学习范式，通过使用相同数据的不同视图训练两个分类器来捕获学习任务的冗余。这允许通过一组大的未标记数据从一组小的标记训练数据中进行引导。本研究考察了自然语言处理任务的共同训练的学习行为，这些任务通常需要大量的训练实例才能达到可用的性能水平。以基本名词短语括号为例，我们发现联合训练可以减少36%分类器之间的误差差异，并在标记版本上监督训练所有可用数据。然而，自举数据的质量下降成为进一步改进的障碍。为了解决这个问题，我们提出了一种适度监督的协同训练变体，在这种变体中，人类可以纠正自动标记过程中所犯的错误。我们的分析表明，正确的共同训练和类似的适度监督方法可能有助于共同训练扩展到大型自然语言学习任务。我们表明，自动标记的训练数据的质量对于协同训练的良好性能至关重要，因为太多的标记错误会阻止高性能模型的学习。"}
{"pid": "W04-0811", "zh_sum": "英语全词任务我们描述了我们在准备英语全词任务中使用的语义标记语料库方面的经验，并将分数制成表格。"}
{"pid": "W00-1303", "zh_sum": "基于支持向量机的日语依赖结构分析本文提出了一种基于支持向量机的日语依赖结构分析方法。传统的基于机器学习框架的句法分析技术，如决策树和最大熵模型，难以选择有用的特征，也难以找到所选特征的适当组合。另一方面，众所周知，支持向量机即使在输入高维特征空间的数据时也能获得很高的泛化性能。此外，通过引入核原理，支持向量机可以在高维空间中进行训练，并且计算量较小，与维数无关。我们将支持向量机应用于日语依赖结构识别问题。在京都大学语料库上的实验结果表明，即使使用少量的训练数据（7958句话），我们的系统也能达到89.09%的准确率。我们引入了一种称为动态特性的新特性，它是在解析过程中动态创建的。"}
{"pid": "D07-1077", "zh_sum": "统计机器翻译中的汉语句法重排句法重排方法是统计机器翻译（SMT）系统中处理源语言和目标语言词序差异的有效方法。本文介绍了一种汉英翻译的重新排序方法。我们描述了一套利用汉英语序系统差异的句法重排规则。生成的系统被用作训练句和测试句的预处理器，将汉语句子转换为在词序方面更接近英语的句子。我们评估了基于MOSES短语的SMT系统中的重新排序方法（Koehn等人，2007年）。根据NIST 2006年评估数据，重新排序方法将摩西系统的BLEU分数从28.52提高到30.86。我们还进行了一系列实验，以分析不同类型的重新排序规则的准确性和影响。与基于成分的方法相比，我们的规则集大大减少了大约60%的规则应用总次数。汉语的顺序与英语的不同主要在于从句的顺序。"}
{"pid": "W04-3207", "zh_sum": "带系数估计的双语解析：使用英语解析朝鲜语我们描述了如何将简单、普遍理解的统计模型（如统计依赖性解析器、概率上下文无关语法和词到词翻译模型）有效地组合成一个统一的双语解析器，共同搜索最佳的英语解析、朝鲜语解析和词对齐，这些隐藏结构都相互约束。用于解析的模型完全分解为两个解析器和TM，允许单独的参数估计。我们在宾夕法尼亚大学韩国语树库上和几个基线系统上评估了我们的双语解析器，并展示了在标记数据非常有限的情况下解析韩国语的改进。我们建议在统一的对数线性模型下，合并一个英语语法分析器、一个单词对齐模型和一个由少量韩语语法树训练的韩语PCFG语法分析器。"}
{"pid": "P08-2012", "zh_sum": "在共指消解中加强及物性共指消解系统的一个理想品质是处理及物性约束的能力，这样即使它很可能将某一特定提及与其他两个提及中的每一个提及相互关联，在进行最终赋值时也会考虑这两个提及相互关联的可能性。这正是整数线性规划（ILP）理想的约束类型，但令人惊讶的是，以前将ILP应用于共指解析的工作并没有编码这种类型的约束。我们训练了一个成对提及的共指分类器，并展示了如何在成对分类器输出的概率之上对这种类型的约束进行编码，以提取最可能的法律实体分配。我们在两个常用的数据集上展示了结果，结果表明，传递闭包的实施持续地提高了性能，包括使用b3评分器提高了3.6%，使用聚类f度量提高了16.5%。我们提出了一个监督系统，该系统使用ILP推理来协调成对分类器的预测。"}
{"pid": "N01-1024", "zh_sum": "屈折形态的无知识归纳我们提出了一种只使用文本语料库而不需要人工输入的算法来自动归纳屈折语言的形态。我们的算法结合了来自正字法、语义和句法分布的线索来归纳德语、荷兰语和英语中的形态关系。使用CELEX作为评估的金标准，我们证明了我们的算法是对任何尚未提出的无知识算法的改进。我们使用潜在语义分析来发现德语、荷兰语和英语中的前缀、后缀和环绕词。"}
{"pid": "P03-1021", "zh_sum": "统计机器翻译中的最小错误率训练通常，统计机器翻译模型的训练过程基于最大似然或相关准则。这种方法的一个普遍问题是，对于看不见的文本，最终的翻译质量只有松散的关系。本文分析了直接优化翻译质量的各种培训标准。这些培训标准利用了最近提出的自动评估指标。我们描述了一种有效训练非光滑错误计数的新算法。我们表明，如果将最终评估标准作为培训程序的一部分直接考虑，通常可以获得显著更好的结果。在我们的模型中，通过最小错误率训练（MERT）调整特征权重以最大化BLEU。"}
{"pid": "D08-1059", "zh_sum": "两个解析器的故事：研究并结合基于图和基于转换的依赖关系解析基于图和基于转换的依赖关系解析方法采用了非常不同的问题视图，每个视图都有自己的优势和局限性。我们在波束搜索的框架下研究了这两种方法。通过开发基于图和基于转换的依赖关系解析器，我们表明波束搜索解码器对于这两种方法都是一种有竞争力的选择。更重要的是，我们提出了一种基于波束搜索的解析器，它将基于图和基于转换的解析结合到一个单独的系统中进行训练和解码，表明它优于基于纯图和基于纯转换的解析器。通过对宾夕法尼亚州树状银行（Penn Treebank）中英文数据的测试，该组合系统的最新准确率分别为92.1%和86.2%。我们定义head规则来将短语结构转换为依赖结构。我们使用结构感知器学习和Collins和Roark（2004）的早期更新策略，将波束搜索与全局归一化判别模型相结合，并探索在基于转换的解析器中添加基于图的特征。"}
{"pid": "W11-2103", "zh_sum": "2011年统计机器翻译研讨会的结果本文介绍了WMT11共享任务的结果，其中包括翻译任务、系统组合任务和机器翻译评估指标任务。我们对148个机器翻译系统和41个系统组合条目进行了大规模手动评估。我们使用这些系统的排名来衡量21个评估指标的自动指标与人类对翻译质量的判断之间的关联程度。今年，一项海地克里奥尔语到英语的任务是翻译海地地震后发送给应急服务机构的短信。我们还进行了一项试点“可调指标”任务，以测试根据不同指标优化固定系统是否会导致明显不同的翻译质量。尽管避免使用特定语文资源，只使用讲习班提供的培训数据，但一项广泛的手动评估确定，所产生的产出的质量远远高于使用特定语文资源的统计和基于规则的系统。"}
{"pid": "D08-1092", "zh_sum": "两种语言优于一种语言（对于语法分析），我们表明，联合分析一个位文本可以显著提高双方的分析质量。在最大熵位文本解析模型中，我们定义了源树、目标树以及它们之间的节点到节点对齐的分布。特征包括单语分析分数和各种句法差异度量。使用中国树库的翻译部分，我们的模型进行迭代训练，以最大化训练树对的边际可能性，并将对齐作为潜在变量。由此产生的位文本解析器在预测英语边树方面的性能比最先进的单语解析器基线高2.5 F，在预测中文边树方面的性能比基线高1.8 F（这些语料库上公布的数据最高）。此外，当用于下游MT评估时，这些改良树的BLEU增加2.4。在双文本解析中，我们使用三元组（语言1中的解析树、语言2中的解析树、单词对齐）上定义的特征函数，并将其组合到对数线性模型中，以最大限度地提高解析精度。我们使用单词对齐密度特征来衡量对齐实体对与独立单词对齐器中的对齐匹配程度。"}
{"pid": "C92-2082", "zh_sum": "从大型文本语料库中自动获取上下义词我们描述了一种从非限制文本中自动获取上下义词关系的方法。有两个目标推动了这种方法：（i）避免对预先编码的知识的需要，以及（ii）在广泛的文本中的适用性。我们确定了一组易于识别的词汇句法模式，这些模式频繁出现并跨越文本体裁边界，无可争议地表明了感兴趣的词汇关系。我们描述了一种发现这些模式的方法，并建议通过这种方式也可以获得其他词汇关系。实现了采集算法的一个子集，并将结果用于对大型手工构建的同义词库的结构进行整理和评论。建议在信息检索等领域进行扩展和应用。我们使用模式匹配技术从文本中找到单独的上下位词对。"}
{"pid": "C92-3150", "zh_sum": "术语名词短语提取的表面语法分析LEXTER是一个术语提取软件包。输入任何主题领域的法语文本语料库，LEXTER生成可能的术语单位列表，提交给专家进行验证。为了确定术语单位，LEXTER将其形式考虑在内，并分两个主要阶段进行：分析、语法分析。在第一阶段，LEXTER使用一系列规则来识别边界标记，以分析文本并提取最大长度的名词短语。在第二阶段，LEXTER分析这些最长名词短语以提取子群，这些子群根据其语法结构和在最长名词短语中的位置很可能是术语单位。在本文中，重点介绍了所使用的分析类型（表面语法分析），以及为适应规则而采用的方法论方法（实验方法）。我们提出了一种表面句法分析器，它从法语文本中提取最大长度的名词短语，主要是限定词、前置修饰词、名词词头序列，以及某些类型的后修饰介词短语和形容词序列，用于术语应用。我们的方法完全依赖于语言信息，即候选词的形态句法特征。"}
{"pid": "W95-0107", "zh_sum": "使用基于转换的学习进行文本分块Eric Brill介绍了基于转换的学习，并表明它可以以相当高的精度进行词性标注。同样的方法可以应用于更高级别的文本解释，以定位标记文本中的块，包括非递归的“baseNP”块。为此，通过将组块结构编码为附加到每个单词的新标记，可以方便地将组块视为标记问题。在使用树库衍生数据的自动测试中，这种技术对基本NP组块的召回率和准确率约为92%，对划分句子的更复杂组块的召回率和准确率约为88%。该应用程序还建议对基于转换的学习方法进行一些有趣的调整。我们将组块形式化为一项分类任务，其中每个单词被分类为组块外的（B）egingning、（I）nside或（O）。我们开创了机器学习技术来解决分块问题。"}
{"pid": "I05-3027", "zh_sum": "Sighan-bakeoff2005的条件随机场分词器我们提出了一个符合Sighan-bakeoff2005闭轨的中文分词系统。我们的切分器是使用条件随机场序列模型构建的，该模型提供了一个框架来使用大量的语言特征，例如字符身份、形态和字符重叠特征。因为我们的形态特征是从训练语料库中自动提取的，所以我们的系统不会偏向于任何特定的普通话品种。因此，我们的系统不会过度适应系统设计者最熟悉的各种普通话。我们的最终系统的F分数为0.947（AS）、0.943（HK）、0.950（PK）和0.964（MSR）。我们开发了斯坦福中文分词器。"}
{"pid": "W04-3236", "zh_sum": "中文词性标注：一次性还是一次性？基于单词还是基于字符？汉语词性标注为汉语句子中的每个单词指定一个词性标注。然而，由于汉语句子中没有分词，因此汉语词性标注需要以分词为前提。我们可以在分词后严格执行中文词性标注（一次一个方法），或者同时执行分词和词性标注（一次完成所有步骤）。此外，我们还可以选择在逐字的基础上分配POS标记，利用周围上下文中的单词特征（基于单词），或在字符特征的基础上逐字符分配POS标记（基于字符）。本文在最大熵框架下对中文词性标注的处理结构和特征表示等问题进行了深入研究。我们发现，虽然基于角色的一次性方法是最好的，但基于角色的一次性方法是一种值得折衷的方法，在准确性方面表现稍差，但训练和运行时间更短。作为我们调查的一部分，我们还构建了一个最先进的中文分词器，在四分之三的测试语料库中，该分词器的性能优于最好的SIGHAN 2003分词器。"}
{"pid": "P03-1019", "zh_sum": "统计机器翻译中重排序约束的比较研究在统计机器翻译中，生成翻译假设的计算代价很高。如果允许任意单词重新排序，则搜索问题是NP困难的。另一方面，如果我们以适当的方式限制可能的单词重排序，我们可以得到一个多项式时间搜索算法。在本文中，我们比较了两种不同的重新排序约束，即ITG约束和IBM约束。该比较包括对每个约束条件下允许的重新排序次数的理论讨论。我们展示了ITG约束与自1870年以来已知的Schroder数之间的联系。我们在两个任务上评估这些约束：Verbmobil任务和Canadian Hansards任务。评估包括两部分：首先，我们检查训练语料库中有多少维特比对齐满足这些约束。其次，我们将搜索限制在这些约束中的每一个，并比较得出的翻译假设。实验将表明，基线ITG约束在加拿大Hansards任务中是不够的。因此，我们对ITG约束进行了扩展。这些扩展的ITG约束将路线覆盖率从87%提高到96%。我们表明，ITG约束在德语英语（Verbmobil语料库）和法语英语（Canadian Hansards语料库）上的对齐覆盖率明显优于IBM统计机器翻译模型中使用的约束。我们引入了一个避免过度计数的范式ITG。"}
{"pid": "P08-1004", "zh_sum": "开放式和传统关系提取之间的权衡传统信息提取（IE）以关系名称和手动标记的关系示例作为输入。开放式IE是一种独立于关系的抽取范式，它是为大规模异构语料库（如Web）量身定制的。开放式IE系统从文本中提取一组不同的关系元组，而无需任何特定于关系的输入。如何实现开放式IE？我们分析了一个英语句子样本，以证明许多关系都是使用一组紧凑的关系无关的词汇句法模式来表达的，这些模式可以通过开放的IE系统来学习。开放式IE和传统IE之间的权衡是什么？我们在两项任务的背景下审议这个问题。首先，当关系的数量很大，并且关系本身没有预先指定时，我们认为开放IE是必要的。然后，我们提出了一种新的开放式IE模型，称为O-CRF，并表明该模型比TEXTRUNNER（先前最先进的开放式IE系统）所采用的模型精度更高，召回率几乎翻了一番。其次，当目标关系的数量很小时，并且它们的名称是预先知道的，我们表明O-CRF能够与传统提取系统的精度相匹配，尽管召回率要低得多。最后，我们展示了如何将这两种类型的系统组合成一个混合系统，以达到比传统提取器更高的精度，并具有可比的召回率。我们使用条件随机场（CRF）分类器进行开放关系提取，将TextRunner系统中朴素贝叶斯模型的F分数提高了60%以上。我们的系统使用CRF分类器对来自解析语料库的S-V-O元组进行训练，将其作为正例，将违反短语结构的元组作为负例。"}
{"pid": "D08-1035", "zh_sum": "贝叶斯无监督主题分割本文描述了一种新的贝叶斯无监督主题分割方法。这项任务的无监督系统是由词汇衔接驱动的：结构良好的片段倾向于诱导紧凑一致的词汇分布。我们表明，通过将每个主题片段中的单词建模为与该片段相关的多项式语言模型，可以将词汇衔接置于贝叶斯语境中；在这样的模型中，最大化观察可能性会产生词汇衔接切分。这与以前的方法形成了对比，以前的方法依赖于手工编制的内聚度量。贝叶斯框架提供了一种原则性的方法来整合额外的特征，例如提示短语，这是一种话语结构的强大指示器，以前在无监督的分词系统中没有使用过。我们的模型在文本和语音数据集上都比一系列最先进的系统有一致的改进。我们还表明，基于熵的分析和以前的著名技术都可以作为贝叶斯框架的特例导出。我们提出了一个线性分割的动态程序。如果片段的实际数量已知，并且只能接受线性话语结构，那么只需移动片段边界即可。我们发现，更丰富的模型有利于会议语料库，但不利于教科书。"}
{"pid": "J91-1002", "zh_sum": "词汇衔接是由语音关系计算出来的，作为语篇结构的一个指标，词汇衔接是一系列相关词的结果，这些词有助于词汇意义的连续性。这些词汇链是“关于同一事物”的文本单位的直接结果，而查找文本结构需要查找关于同一事物的文本单位。因此，计算链是有用的，因为它们将与文本的结构相对应。确定文本的结构是确定文本深层含义的关键步骤。本文使用同义词库作为计算词汇链的主要知识库。词汇链和结构元素之间存在对应关系。由于词汇链是可计算的，并且存在于非特定领域的文本中，因此它们为文本结构提供了一个有价值的指标。词汇链还为解释单词、概念和句子提供了语义上下文。我们建议使用词汇链作为词汇衔接的指标。我们提出词汇链的概念来探索语篇的语篇结构。"}
{"pid": "W07-0403", "zh_sum": "联合短语翻译模型的倒装转换语法我们提出了一种短语倒装转换语法，作为联合短语翻译模型的替代。这种句法模型类似于其扁平字符串短语的前身，但允许使用多项式时间算法进行维特比对齐和EM训练。我们证明，允许扁平短语模型进行缩放的一致性约束也有助于ITG算法，生成的内外算法速度快80倍。我们还表明，ITG生成的短语翻译表优于平关节短语模型，BLEU分数提高了2.5分。最后，我们首次探讨了联合短语翻译模型作为词对齐方法的实用性。我们使用同步ITG（Wu，1997）和约束来寻找非成分短语等价。"}
{"pid": "P93-1023", "zh_sum": "自动识别形容词尺度：根据意义对形容词进行聚类本文提出了一种根据意义对形容词进行分组的方法，作为自动识别形容词尺度的第一步。我们讨论了形容词量表和语义相关形容词组的性质，以及它们如何在文本语料库中暗示语言知识的来源。我们描述了我们的系统如何利用这一语言知识，使用统计技术计算两个形容词之间的相似度，而不需要访问有关形容词的任何语义信息。我们还展示了聚类算法如何利用这些相似性来生成形容词组，并给出了我们的系统生成的一组样本形容词的结果。最后，我们提出了手头任务的评估方法，并分析了所获得结果的重要性。我们通过对表示相同属性值的形容词进行聚类来学习属性。"}
{"pid": "P00-1065", "zh_sum": "语义角色的自动标注我们提供了一个系统，用于识别语义关系或语义角色，由语义框架内的句子成分填充。各种词汇和句法特征都是从解析树中派生出来的，并用于从手工标注的训练数据中派生统计分类器。我们将任务分为两个连续的子任务：首先，参数识别决定每个实例是否具有语义角色；然后，参数标签为被识别为角色承担者的实例分配一个标签。我们提出了一个系统，该系统使用完整的语法特征对FrameNet数据库中句子中的框架元素进行分类。"}
{"pid": "C02-1054", "zh_sum": "用于命名实体识别的高效支持向量分类器命名实体（NE）识别是一项从文档中提取专有名词和数字信息并将其分类为人员、组织和日期等类别的任务。它是信息抽取和开放领域问答的关键技术。首先，我们证明了基于支持向量机（SVM）的NE识别器比传统系统给出了更好的分数。然而，现成的SVM分类器对于这项任务来说效率太低。因此，我们提出了一种方法，使系统大大加快。这种方法也可以应用于其他类似的任务，如组块和词性标记。我们还提出了一种基于支持向量机的特征选择方法和一种有效的训练方法。我们提出了一种核扩展方法，通过修改决策函数将基于d次多项式核的分类器转换为线性分类器。我们提出了一种XQK（扩展二次核），它可以大大提高命名实体识别器的速度。"}
{"pid": "W02-2016", "zh_sum": "基于级联组块的日语依赖分析本文提出了一种基于级联组块模型的统计日语依赖解析器。传统的日本统计依赖解析器主要基于概率模型，这并不总是有效的或可扩展的。我们提出了一种简单而有效的新方法，因为它只能确定当前片段是否修改其右侧的片段来解析句子。使用京都大学语料库进行的实验表明，该方法在提高句法分析和训练效率的同时，优于以前的系统。我们的级联组块模型不需要依赖性的概率，并且可以确定地解析句子。"}
{"pid": "P90-1034", "zh_sum": "从谓词论元结构对名词进行分类描述了一种基于大型文本语料库中主语、动词和宾语分布的度量来确定名词相似性的方法。由此产生的名词准语义分类证明了分布假设的合理性，并有可能应用于各种任务，包括自动索引、解析名词复合词和确定修饰范围。我们使用基于互信息的度量来确定名词相似度。我们使用分布相似性的概念，即两个含义相似的单词将在相似的上下文中使用。"}
{"pid": "P08-1068", "zh_sum": "简单半监督依赖解析我们提出了一种简单有效的半监督依赖解析器训练方法。我们关注词汇表示的问题，引入了从大型未注语料库中提取的包含词簇的特征。我们在Penn树库和Prague dependency树库上的一系列依赖关系解析实验中证明了该方法的有效性，并且我们还表明，基于聚类的特征在各种条件下都能显著提高性能。例如，在英语无标记二阶句法分析的情况下，我们将基线准确率从92.02%提高到93.16%；在捷克语无标记二阶句法分析的情况下，我们将基线准确率从86.13%提高到87.13%。此外，我们还证明，当有少量的训练数据可用时，我们的方法也可以提高性能，并且可以将达到所需性能水平所需的监督数据量大致减半。我们发现，对于句法依赖分析，将棕色聚类特征与词形或词性标记相结合，即使只需很少的训练数据，也能获得较高的精度。我们建议使用词簇作为特征来改进英语和捷克语基于图的统计依赖分析。"}
{"pid": "P93-1024", "zh_sum": "英语单词的分布聚类我们描述并实验评估了一种根据单词在特定句法环境中的分布进行聚类的方法。单词由其出现的上下文的相对频率分布表示，这些分布之间的相对熵被用作聚类的相似性度量。聚类由给定单词的平均上下文分布表示，上下文分布是根据它们的聚类成员概率得出的。在许多情况下，这些簇可以被认为是对粗略意义上的区别进行编码。确定性退火用于寻找失真度最低的聚类集：随着退火参数的增加，现有聚类变得不稳定并细分，从而产生数据的分层“软”聚类。聚类被用作单词共现的类模型的基础，并根据保留的测试数据对模型进行评估。我们利用确定性退火将动词-论元对聚类为动词和名词类。"}
{"pid": "S10-1010", "zh_sum": "SemEval-2010任务13:TempEval-2 TempEval-2包括时间表达式、事件和时间关系的评估任务，后者分为四个子任务，其动机是更小的子任务将使数据准备和时间关系提取更容易。提供了六种语言的手动注释数据：中文、英文、法文、意大利文、韩文和西班牙文。本研讨会的任务之一是确定同一句子中事件和时间表达式之间的时间关系。"}
{"pid": "P06-1134", "zh_sum": "词义和主观性、主观性和意义都是语言的重要属性。本文探讨了它们之间的相互作用，并提供了实证证据来支持以下假设：（1）主观性是一种与词义相关的属性，（2）词义消歧可以直接受益于主观性注释。我们研究了一个词在不同意义上的客观性和主观性之间的区别，以及它们在情感分析中的经验效应。我们提供的证据表明，词义标签和上下文主观性分析可以用来提高词义消歧的性能。我们证明，即使是可靠的主观性线索也有客观意义。我们发现，当一个词具有明显的主观性和客观性时，主观性注释有助于词义消歧。我们对354个极性词义的人类注释进行了研究，发现注释者之间的一致性很高。我们将主观表达定义为用来表达心理和情感状态的词语和短语，如推测、评估、情感和信仰。"}
{"pid": "J87-1005", "zh_sum": "生成量词范围的算法句子的句法结构通常非常清楚地显示谓词论元结构和语法从属关系。但范围依赖关系并不是那么透明。因此，许多表示句子语义的系统都忽略了范围界定或产生了范围界定，其机制往往无法解释它们所选择的范围界定，或在它们允许的范围界定中肆意挥霍。本文提出了一种从编码谓词参数结构的非作用域表达式生成作用域语义形式的算法，并证明了它的一些重要性质。该算法不像基于量词排列的算法那样挥霍无度，它可以为计算解决方案提供坚实的基础，在计算解决方案中，为了效率和启发式效率而牺牲完整性。我们扩展了这种形式来支持操作符（比如not），并提出了一种比朴素包装方法更有效的枚举算法。我们提出了一种从谓词-论元关系和语法从属关系的表示中生成量词范围的算法。我们介绍了一种生成所有可能的量词范围的算法。"}
{"pid": "W04-3253", "zh_sum": "使用具有不同信息源的支持向量机进行情感分析本文介绍了一种情感分析方法，该方法使用支持向量机（SVM）将潜在相关信息的不同来源汇集在一起，包括短语和形容词的一些偏好度量，以及文本主题的知识（如果可用）。使用引入的特征的模型进一步与过去证明有效的unigram模型（Pang et al.，2002）和unigram模型的线性化版本相结合。对来自互联网电影数据库的电影评论数据的实验表明，混合支持向量机将基于单图风格特征的支持向量机与基于实值偏好度量的支持向量机相结合，可获得优异的性能，产生迄今为止使用该数据发布的最佳结果。此外，还报告了在为主题手工标注的较小音乐评论数据集上使用丰富主题信息的特征集进行的进一步实验，结果表明，将主题信息纳入此类模型也可能产生改进。"}
{"pid": "P97-1017", "zh_sum": "机器音译在不同字母和声音目录的语言中翻译姓名和技术术语是一项挑战。这些项目通常是音译的，即用近似的语音等价物代替。例如，英语中的计算机在日语中显示为（konpyuutaa）。将这些项目从日语翻译回英语更具挑战性，也更具实际意义，因为音译项目构成了双语词典中未发现的大部分文本短语。我们描述并评估了一种通过机器执行向后音译的方法。这种方法使用生成模型，在音译过程中包含几个不同的阶段。我们建议组成一组加权有限状态转换器来解决日语片假名到英语的回译问题。"}
{"pid": "P09-1113", "zh_sum": "无标记数据关系抽取的远程监控ACE等任务的关系抽取现代模型基于对小型手标记语料库中关系的有监督学习。我们研究了一种不需要标记语料库的替代范式，避免了ACE风格算法的领域依赖性，并允许使用任何大小的语料库。我们的实验使用Freebase（一个包含数千个关系的大型语义数据库）来提供远程监控。对于出现在某个Freebase关系中的每一对实体，我们在一个大型的未标记语料库中找到包含这些实体的所有句子，并提取文本特征来训练关系分类器。我们的算法结合了有监督IE（在概率分类器中结合400000个噪声模式特征）和无监督IE（从任何领域的大型语料库中提取大量关系）的优点。我们的模型能够以67.6%的精度提取102个关系的10000个实例。我们还分析了特征性能，表明句法分析特征对于表达中的歧义或词汇疏远关系特别有用。此外，研究人员还试图从维基百科（Weld et al，2008）和数据库（Mintz et al，2009）等资源中自动提取监督学习的示例，或尝试开放信息提取（IE）（Banko et al，2007），以提取所有可能的关系。通过启发式地将文本中的实体与知识库中的实体对齐，远程监控（DS）可以自动收集标记数据。"}
{"pid": "J07-3004", "zh_sum": "CCGbank：从Penn Treebank中提取的CCG派生和依赖结构语料库本文提出了一种将Penn Treebank翻译成组合范畴语法（CCG）派生语料库的算法，该语料库增加了局部和长期单词-单词依赖。由此产生的语料库CCGbank包含了宾州树库中99.4%的句子。它可从语言数据协会获得，并已用于培训覆盖范围广泛的统计解析器，以获得最先进的依赖恢复率。为了获得语言学上充分的CCG分析，并消除原始注释中的噪音和不一致，需要对宾夕法尼亚树库中的结构和注释进行广泛的分析，并且需要对树库进行大量的更改。我们讨论了我们的发现对从树库中提取其他语言表达语法以及设计未来树库的意义。CCGbank风格的依赖关系是头-子关系的有向图，标有头的词法类别和子级填充的参数槽。CCGbank是一个CCG派生词的语料库，由宾夕法尼亚树状银行的《华尔街日报》栏目半自动转换而成。"}
{"pid": "J03-3001", "zh_sum": "作为语料库的网络特刊简介网络上充斥着语言数据，种类繁多，语言多样，数量巨大，免费提供，是语言学家们的乐园。这期《计算语言学》特刊探讨了人们探索这一梦想的方式。出于研究目的，人们自然会质疑web数据的适当性，因为web数据不可避免地会产生噪音，搜索引擎本身可能会引入某些特质，从而扭曲结果。"}
{"pid": "W98-1106", "zh_sum": "概念性自然语言处理系统通常依靠实例框架实例化来识别文本中的事件和角色对象。但是，为一个域生成一组好的案例框架非常耗时、乏味，而且容易出现遗漏错误。我们开发了一种基于语料库的算法，用于从未标注的文本中经验地获取概念案例框架。我们的算法建立在先前对基于语料库的抽取模式和语义词典获取方法的研究基础上。给定抽取模式和一个领域的语义词典，我们的算法学习每个抽取模式的语义偏好，并合并语法兼容的模式，以生成具有选择限制的多时隙case框架。与原始提取模式相比，case框架生成更内聚的输出，产生的错误命中更少。我们的系统只需要预先分类的培训文本和几个小时的手动检查来过滤词典，这表明概念性案例框架可以从未注释的文本中获得，而无需特殊的培训资源。我们的概念案例框架获取项目、抽取模式、一个领域语义词典以及一系列概念角色和相关语义类别用于生成具有选择限制的多个时隙案例框架。"}
{"pid": "P04-1056", "zh_sum": "利用关系马尔可夫网络进行集体信息提取大多数信息提取（IE）系统将单独的潜在提取视为独立的。然而，在许多情况下，考虑不同潜在提取之间的影响可以提高整体准确度。基于无向图形模型的统计方法，如条件随机场（CRF），已被证明是学习精确IE系统的有效方法。我们提出了一种新的IE方法，该方法采用关系马尔可夫网络（CRF的推广），可以表示抽取之间的任意依赖关系。这允许“集体信息提取”，利用可能提取之间的相互影响。从生物医学文本中学习提取蛋白质名称的实验证明了这种方法的优势。我们提出了AIM，一个用于评估PPI提取系统的语料库。"}
{"pid": "C94-2174", "zh_sum": "使用判别分析的简单度量识别文本类型使用判别分析的统计标准技术将文本分类为预先确定的文本类型类别的简单方法，并应用于Brown语料库。判别分析可以使用大量的参数，这些参数可能是特定于某一语料库或信息流的，并将它们组合成少量的函数，参数的权重取决于它们对区分文本体裁的有用程度。讨论了它在信息检索中的应用。我们将字长作为体裁分类等应用程序的正式性指标。"}
{"pid": "P90-1005", "zh_sum": "基于约束传播的结构消歧我们提出了一种新的语法形式，称为约束依赖语法（CDG），其中每个语法规则都是作为对词到词修改的约束给出的。CDG解析被形式化为有限域上的约束满足问题，因此可以使用有效的约束传播算法来减少结构歧义，而无需生成单独的解析树。还讨论了CDG分析的弱生成能力和计算复杂性。我们的约束依赖语法映射到构造满足技术的符号。"}
{"pid": "P08-1066", "zh_sum": "一种新的基于目标依赖语言模型的串-依赖机器翻译算法本文提出了一种新的用于统计机器翻译的串-依赖机器翻译算法。在这个新的框架下，我们在解码过程中使用了一个目标依赖语言模型，以利用传统的n-gram语言模型无法利用的长距离单词关系。我们的实验表明，在NIST 04中英文评估集上，与标准的分层字符串-字符串系统相比，字符串-依赖解码器在BLEU和TER方面分别提高了1.48个点和2.53个点。我们提出了一个字符串到依赖关系模型，该模型将每个层次规则的目标端限制为格式良好的依赖树片段，并使用依赖语言模型使输出更加语法化。"}
{"pid": "W03-0430", "zh_sum": "基于条件随机场特征归纳和Web增强词典的命名实体识别的早期结果"}
{"pid": "J04-4004", "zh_sum": "Collins解析模型的复杂性本文记录了Collins在其解析器中使用的大量迄今尚未公开的细节，因此，与Collins（1999）的论文一起，本文包含了复制Collins基准测试结果所需的所有信息。事实上，这些尚未公布的细节说明了从包含所有细节的实施到科林斯模型的洁净室实施，误差相对增加了11%。我们还展示了一种处理标点和连词的更干净、性能同样好的方法，并揭示了Collins解析器的某些其他概率奇怪之处。我们不仅分析了未发布的细节的影响，还重新分析了某些已知细节的影响，揭示了模型几乎没有使用双线性依赖，并且头部选择对整体解析性能的重要性远不如人们所认为的那样。最后，我们进行的实验表明，词汇化的真正辨别力似乎在于，非词汇化的句法结构是在中心词及其词性的条件下生成的。结果表明，柯林斯式句法分析模型的威力并不像人们曾经认为的那样主要在于使用双词性依赖，而是在于词汇-结构依赖，即预测句法结构对中心词的制约。我们表明，在Collins的Model-2解析器中，只有1.49%的决策使用了双自由度信息，删除这些信息会导致性能非常小的下降。"}
{"pid": "W02-2018", "zh_sum": "最大熵参数估计算法的比较条件最大熵（ME）模型提供了一种通用的机器学习技术，已成功应用于计算机视觉和计量经济学等领域，并用于自然语言处理中的各种分类问题。然而，ME模型的灵活性并非没有成本。虽然ME模型的参数估计在概念上很简单，但在实践中，典型自然语言任务的ME模型非常大，可能包含数千个自由参数。在本文中，我们考虑了许多估计ME模型参数的算法，包括迭代缩放、梯度上升、共轭梯度和变尺度方法。令人惊讶的是，与其他算法相比，标准使用的迭代缩放算法的性能非常差，对于所有测试问题，有限内存可变度量算法的性能优于其他选择。我们介绍了用于高级判别模型的开源工具包，该工具包使用有限内存变量度量。"}
{"pid": "A00-2030", "zh_sum": "自1995年以来，一些统计句法分析算法在句法分析准确性方面取得了突破，与UPenn树库作为黄金标准相比，这是统计句法分析从文本中提取信息的一种新方法。在本文中，我们介绍了一种词汇化的概率上下文无关解析器用于信息提取，并对MUC-7模板元素和模板关系进行了评估。我们基于规则的方法使用大量语言规则来捕获关系模式。我们认为，关系提取只是一种概率解析的形式，其中解析树被扩充以识别所有关系。我们在一个模型中集成了词性标注、命名实体识别、模板元素提取和关系提取等各种任务。我们将实体识别、解析和关系提取结合到一个经过联合训练的单一统计解析模型中，该模型可以提高所有子任务的性能。当前工作的一部分贡献在于表明，联合解码即使在联合训练不可能的情况下也可以有效，因为联合标记的数据不可用。"}
{"pid": "P03-1044", "zh_sum": "语义模式发现中的反训练本文提出了一种无监督的语义模式发现方法。语义模式对于各种文本理解任务都很有用，特别是对于在文本中定位事件以进行信息提取。该方法建立在前面描述的迭代无监督模式获取方法的基础上。先前方法的一个共同特征是，算法的输出是连续的模式流，精度逐渐降低。我们的方法不同于以前的模式获取算法，因为它同时引入了多个场景之间的竞争。这为无监督学习者提供了自然的停止标准，同时在终止时保持了良好的精度水平。我们讨论了几个场景的实验结果，并检查了新程序的不同方面。通过同时从多个域或类中学习，我们开发了用于检测特定域或特定类的负面规则的计数器训练。我们使用谓词参数（SVO）模型，该模型允许只包含动词及其直接主语和宾语的子树作为提取模式候选。"}
{"pid": "P03-2041", "zh_sum": "学习机器翻译中的非同构树映射通常人们希望学习树到树的映射，在未对齐的树对上或树与字符串的混合上对其进行训练。与以前的统计形式不同（仅限于同构树），同步TSG允许树拓扑的局部失真。我们将其重新格式化以允许依赖树，并绘制用于对齐、训练和解码的EM/Viterbi算法。我们认为，如果提供了源语句的解析树，解码（对于树到字符串和树到树模型）也可以转换为树解析问题。我们考虑同步树替换语法，这是一种可以解释结构不匹配的形式，并且经过区分训练。"}
{"pid": "E03-1008", "zh_sum": "从小型数据集引导统计解析器我们提出了一种实用的联合训练方法，用于使用少量手动解析的训练材料和更大的原始句子库引导统计解析器。实验结果表明，使用未标记语句可以提高统计解析器的性能。此外，当手动解析的训练材料与原始句子或测试材料位于不同的域时，我们考虑了引导解析器的问题。我们表明，即使没有使用来自目标域的手动生成的解析，引导仍然很有用。我们研究了在小种子情况下（<1k标记数据）PCFG解析的自我训练。我们报告了使用自我训练进行语法分析的轻微改进或严重损坏。我们发现使用词汇化树邻接语法解析器会导致性能下降，而使用Collins词汇化PCFG解析器则会有轻微的改进；然而，只有当解析器在一个小的标记集上训练时，才能获得这种增益。"}
{"pid": "P89-1009", "zh_sum": "烹饪指代表达式本文描述了EPICURE中使用的指代表达式生成机制。EPICURE是一个计算机程序，用于生成烹饪食谱的自然语言描述。该系统的主要特点包括：底层本体允许表示非奇异实体；歧视性权力的概念，用于确定描述中应使用哪些属性；以及一种类似PATR的统一语法，以生成表面语言字符串。我们以遭受NP难复杂性为代价，生成一个包含尽可能少的属性的描述。我们的算法试图通过总是选择最具歧视性的属性来构建最小的区分描述。我们将最小化定义为系统生成的描述中最简短的部分。我们提出了一个解决方案，以产生明确的描述，唤起话语实体已经介绍了上下文中的问题。"}
{"pid": "D08-1021", "zh_sum": "平行语料库释义的句法约束我们通过要求短语及其释义具有相同的句法类型来提高平行语料库释义的质量。这是通过分析平行语料库的英语侧并改变短语提取算法来实现的，以便在双语短语对旁边提取短语标签。为了保持非成分短语的广泛覆盖，引入了复杂的句法标签。手动评估表明，与基线方法相比，释义质量绝对提高了19%。我们展示了如何利用句子的单语语境来提高所获得的释义的质量。人类评估者被要求对每一对原始句子和复述句子进行以下两个5分制评分：语法性：复述句子是否符合语法，意义：复述句子是否正确保留了原始句子的意义。基于短语的释义或术语变体获取方法的一个问题是，系统提出的大部分术语变体或释义都是原始术语的高级子字符串。我们自动获取释义词典。"}
{"pid": "P11-1055", "zh_sum": "基于知识的重叠关系信息提取弱监督信息提取（IE）有望从Web的自然语言文本中生成大规模的知识库。基于知识的弱监督，使用结构化数据启发式地标记训练语料库，通过实现潜在无限多个关系提取器的自动学习，实现了这一目标。最近，研究人员开发了多实例学习算法，以对抗启发式标记可能产生的噪声训练数据，但他们的模型假设关系是不相交的-例如，他们无法提取（乔布斯，苹果）和（乔布斯，苹果）的CEO。本文提出了一种新的具有重叠关系的多实例学习方法，该方法将句子级抽取模型与简单的语料库级组件相结合，用于聚合单个事实。我们应用我们的模型学习纽约时报文本的提取器，使用Freebase的弱监督。实验表明，该方法运行速度快，在聚合和句子层面的准确性都有惊人的提高。我们采用贪婪推理算法。我们使用多重确定性或约束来训练句子关系提取器。"}
{"pid": "P93-1008", "zh_sum": "双子座：口语理解的自然语言系统我们报告DARPA航空公司预订语料库的语法和语义覆盖率为86%。双子座是一种表达形式主义，在其中编写形式语法。我们提出了Gemini自然语言解析器/生成器，它试图解析语音识别输出。"}
{"pid": "W06-3119", "zh_sum": "通过图表解析增加语法的机器翻译我们展示了共享任务“利用并行文本进行统计机器翻译”的翻译结果，该任务由图表解析解码器生成，该解码器操作目标语言语法类别增加和概括的短语表。我们使用目标语言解析器为双语训练语料库目标端的每个句子生成解析树，并将其与为相应源句子构建的短语表格进行匹配。考虑到与语法树中的语法类别相对应的短语，我们开发了一些技术来扩充（为短语对声明一个语法动机类别）并将短语表概括（形成混合终端和非终端短语）为同步双语语法。我们为本次研讨会展示了法语到英语任务的结果，代表了对研讨会基线系统的重大改进。我们的翻译系统是在GNU通用公共许可下开放源码的。在我们的工作中，语法成功地集成到分层SMT中。我们从传统PBMT启发式算法提取的一组完整短语开始，然后用包含跨度的目标端解析树中的组成节点标签注释每个短语条目的目标端。我们使用断开的语法片段来扩充它们的语法，以增加规则覆盖率；当我们通过生成框架学习从原始树转换而来的最优树片段时，它们会枚举原始树中可用的片段，而无需学习过程。"}
{"pid": "W02-1210", "zh_sum": "高效的日语深度处理我们提供了一个覆盖范围广泛的日语语法，它是用HPSG形式主义和MRS语义编写的。该语法是为在实际应用程序中使用而创建的，因此健壮性和性能问题起着重要作用。它连接到词性标记和分词工具。这种语法是在多语言环境下开发的，需要跨语言轻松比较的MRS结构。我们手工制作的日语HPSG语法JACY提供语义信息以及复杂结构的语言学分析。"}
{"pid": "J92-1001", "zh_sum": "本文利用多种知识源进行词义识别，解决了如何在不受限制的文本中识别单个单词的预期含义，而不必获得完整的句子表示的问题。为了区分意义，理解者可以考虑各种信息，包括句法标记、词频、搭配、语义上下文、角色相关期望和句法限制。然而，目前的方法仅利用这些信息的一小部分。在这里，我们将描述如何使用整个范围的信息。我们的讨论将包括偏好线索如何与一般词汇和概念知识以及搭配和语境的更专业知识相关联。我们将描述一种根据线索的个体特异性而不是线索类型之间的固定排名来组合线索的方法。我们还将讨论该方法在为任意文本计算语义标记的系统中的应用，即使它无法确定某些句子的单个句法或语义表示。我们是第一个在语义解释系统TRUMP中使用多种特征进行词义消歧的公司之一。我们描述了对词义消歧有用的不同来源的研究，包括形态学信息。"}
{"pid": "P08-1088", "zh_sum": "从单语语料库中学习双语词汇我们提出了一种从单语语料库中学习双语翻译词汇的方法。每种语言中的单词类型都具有纯粹的单语特征，例如上下文计数和正交子串。翻译使用基于典型相关分析的生成模型进行归纳，该模型从潜在匹配的角度解释单语词汇。我们表明，高精度词汇可以在各种语言对和各种语料库类型中学习。我们只使用一个包含100个单词对的小型双语词典作为种子词典。解析器的可用性是一个更严格的限制，但我们的结果表明，更基本的NLP方法可能足以用于双语词典提取。在这项工作中，我们使用了一组seed翻译（不像Haghhii等人（2008））。我们提出了一个基于典型相关分析的生成模型，该模型考虑了单语特征，如单词的上下文和正交子串。"}
{"pid": "P97-1035", "zh_sum": "PARADISE：口语对话主体的评价框架本文介绍了PARADISE（对话系统评价范式），这是一个评价口语对话主体的通用框架。该框架将任务需求与agent的对话行为解耦，支持对话策略之间的比较，支持计算子对话框和整个对话的性能，指定各种因素对性能的相对贡献，并通过任务复杂性的规范化，使比较执行不同任务的agent成为可能。我们确定了影响SDS性能的三个因素：代理因素（主要与对话和系统本身有关），任务因素（与SDS捕获任务的方式相关）和环境因素（如与声环境和传输通道相关的因素）。我们旨在通过将总体用户满意度与其他指标（如任务成功率、效率度量和定性度量）相关联来评估Dialogue agent策略。"}
{"pid": "P08-1043", "zh_sum": "闪族语言中用于联合形态切分和句法分析的单一生成模型形态过程提供了空格分隔的单词，这些单词将多个不同的句法单元引入到输入句子的结构中。这些词反过来又具有高度的歧义性，打破了大多数解析器的基本假设，即给定句子的树的产量是事先已知的。在这里，我们提出了一个单一的联合模型来执行形态分割和句法消歧，它绕过了相关的循环。使用树库语法、数据驱动词典和语言驱动的未知标记处理技术，我们的模型在希伯来语形态学和句法处理方面优于以前的流水线、集成或因子化系统，与迄今为止最好的已发布结果相比，误差减少了12%。Goldberg和Tsarfaty（2008）得出结论，希伯来语树库分析中的形态消歧和句法分析综合模型改进了流水线方法的结果。我们证明了格解析对于联合执行希伯来文文本的切分和解析的有效性。"}
{"pid": "P89-1031", "zh_sum": "评估话语处理算法为了建立一种评估自然语言系统的方法，我们进行了一个案例研究。我们试图通过比较两种已发表的算法在自然出现的文本和对话中寻找代词共同说明符的准确性和覆盖率，来评估两种不同的语篇回指处理方法。我们给出了手动模拟这些算法的定量结果，但这种分析自然会产生定性评估和执行此类评估的建议。我们说明了定量评估遇到的一般困难。这些问题涉及：（a）考虑到基本假设，（b）确定如何处理欠规范，以及（c）评估误报和错误链接的贡献。我们将错误链接称为这样一种情况：代词x正确地表示它与另一个代词y对应，而程序错误地识别了y的先行词。"}
{"pid": "P00-1071", "zh_sum": "开放领域问答系统的结构和性能本文介绍了在SMU自然语言处理实验室开发的LASSO问答系统的结构、操作和结果。为了找到答案，该系统依赖于句法和语义技术的结合。搜索答案是基于一种称为段落索引的新索引形式。在TREC-8比赛中，短答案得分为55.5%，长答案得分为64.5%。我们将自然语言问题转换为IR查询。我们选择所有被识别为专有名词的命名实体作为关键字。"}
{"pid": "J99-2004", "zh_sum": "超级标记：一种几乎解析的方法在本文中，我们提出了一种新的鲁棒性解析方法，该方法将语言动机词汇描述的灵活性与统计技术的鲁棒性相结合。我们的论点是，如果语言项与丰富的描述（SuperTag）相关联，并在局部上下文中施加复杂的约束，那么语言结构的计算可以本地化。supertag的设计使得只有词法项施加约束的元素才会出现在给定的supertag中。此外，每个词法项与词法项可以出现的不同语法上下文的数量一样多的超标记相关联。这使得每个词汇项的不同描述的数量比描述不太复杂时大得多，从而增加了解析器的局部歧义。但这种局部歧义可以通过使用从语法分析语料库收集的supertag共现的统计分布来解决。我们在词汇化树邻接语法（LTAG）框架的背景下探讨了这些想法。LTAG中的SuperTag将短语结构信息和依赖信息结合在一个表示中。Supertag消歧会产生一个有效的解析表示（几乎解析），而解析器只需要“仅”组合各个Supertag。这种解析方法还可以用于解析句子片段，例如在语音中，消除歧义的supertag序列可能无法组合成单个结构。我们指出，使用超级标记（supertagging）进行正确的消歧，即在解析之前分配词条，可以实现有效的LTAG（词汇化树邻接语法）解析。"}
{"pid": "P98-1012", "zh_sum": "当在多个文本源中讨论同一个人、地点、事件或概念时，就会发生使用向量空间模型的基于实体的跨文档核心引用。计算机识别这种现象很重要，因为它允许用户同时检查来自多个文本源的特定实体的信息，从而有助于打破“文档边界”。在本文中，我们描述了一种跨文档共指消解算法，该算法使用向量空间模型来解决同名人员之间的歧义。此外，我们还描述了一种用于评估系统生成的跨文档共指链的评分算法，并将我们的算法与MUC-6（文档内）共指任务中使用的评分算法进行了比较。我们提出了基于实体的跨文档共引用，即使用每个文档的共引用链生成其摘要，然后使用摘要而不是整篇文章来选择信息性词语作为文档的特征。"}
{"pid": "P94-1013", "zh_sum": "词汇歧义消解决策表：在西班牙语和法语重音恢复中的应用本文提出了一种词汇歧义消解的统计决策过程。该算法利用了局部句法模式和更遥远的搭配证据，生成了一个高效、有效且非常清晰的解决给定歧义的方法。通过在目标上下文中仅识别和利用单个最佳消歧证据，该算法避免了有问题的复杂统计依赖建模。虽然该算法直接适用于一大类歧义，但在一个现实的案例研究中对其进行了描述和评估，即恢复西班牙语和法语文本中缺失的重音的问题。当前的准确率在整个任务中超过99%，即使是最困难的歧义，通常也超过90%。我们注意到，通过将模棱两可的单词的所有屈折形式聚集在一起，柠檬化允许更紧凑和更具概括性的数据。我们采用的组合关系建模策略是提供并置词的二元图和三元图作为描述局部上下文的特征。"}
{"pid": "N04-3012", "zh_sum": "相似性-测量概念的相关性WordNet：：相似性是一个免费提供的软件包，它可以测量一对概念（或语法集）之间的语义相似性和相关性。它提供了六个相似度度量和三个相关性度量，所有这些度量都基于词汇数据库WordNet。这些度量值作为Perl模块实现，Perl模块将两个概念作为输入，并返回一个表示它们相似或相关程度的数值。WordNet：：Similarity package的出现促进了关于将语义知识集成到英语NLP中的广泛研究。这些度量从简单的边缘计数到试图通过考虑链路方向、相对路径和密度（如向量、lesk、hso、lch、wup、路径、res、lin和jcn）来考虑网络结构的特性，各不相同。"}
{"pid": "C90-3044", "zh_sum": "对于基于记忆的翻译，基于实例的翻译的一个基本问题是如何利用多个翻译实例来翻译一个源句子。本文提出了一种解决这一问题的方法。我们介绍了一种称为匹配表达式的表示法，它表示翻译示例片段的组合。翻译过程包括三个步骤：（1）从源句中做出源匹配表达式。（2） 将源匹配表达式转换为目标匹配表达式。（3） 从目标匹配表达式构造目标句子。这种机制会生成一些候选翻译。为了从中选择最好的翻译，我们定义了翻译的分数。我们将结构相似性度量与单词距离度量相结合，以获得用于匹配的总体距离度量。"}
{"pid": "C92-3126", "zh_sum": "语言性能的计算模型：面向数据的语法分析面向数据的语法分析（DOP）是一种没有抽象规则，但以分析语料库形式存在的语言经验构成语言处理基础的模型。分析新输入意味着系统试图找到最可能的方法，从语料库中已经存在的片段中重建输入。消除歧义是一种副作用。DOP可以通过使用传统的解析策略来实现。本文研究了其他随机文法之间的超强等价关系。我们表明，传统的上下文无关解析技术可以用于为DOP1中的句子创建解析林。"}
{"pid": "P97-1003", "zh_sum": "本文首先提出了一种新的统计句法分析模型，即词汇化上下文无关语法的生成模型。然后，我们将该模型扩展到包括子类别和wh移动的概率处理。《华尔街日报》文本上的结果显示，解析器的成分准确率/召回率为88.1/87.5%，平均比Collins 96提高了2.3%。我们提供了一个来自《华尔街日报》的2900万字的分析语料库。"}
{"pid": "W04-3247", "zh_sum": "LexPageRank：多文档文本摘要中的声望多文档抽取摘要依赖于句子中心的概念来识别文档中最重要的句子。中心性通常根据特定重要词的存在或与形心伪句的相似性来定义。我们现在正在考虑一种基于特征向量中心性（prestige）概念的句子重要性计算方法，我们称之为LexPageRank。该模型基于余弦相似度构造句子连通矩阵。如果两句话之间的余弦相似度超过特定的预定义阈值，则会向连通矩阵添加相应的边。我们根据DUC 2004年的数据对我们的方法进行了评估。结果表明，我们的方法优于基于质心的摘要，并且与其他摘要系统相比是非常成功的。我们提出Lex PageRank，这是一种基于特征向量中心性概念计算句子重要性的方法。"}
{"pid": "C04-1111", "zh_sum": "尽管大量的文本数据可以免费获得，但许多NLP算法仅利用其中的一小部分。在本文中，我们研究了在太规模上工作的挑战。我们提出了一种为terascale设计的挖掘is关系的算法，该算法的性能与最先进的语言丰富的方法相似。我们关注这两个系统的准确性与处理时间和语料库大小的关系。我们提出了一种基于编辑距离技术的类似、高度可扩展的方法来学习词典词性模式，显示出良好的性能和效率。我们将is-a关系获取扩展到terascale，并通过最小编辑距离自动识别超名称模式。在提取is-a关系的场景中，我们提出了一种基于模式的方法，并将其与基线句法分布相似性方法（在他们的论文中称为句法共现）进行了比较。"}
{"pid": "W04-2705", "zh_sum": "NomBank项目：一份中期报告本文描述了NomBank项目，该项目将为宾夕法尼亚州Treebank II语料库中的常见名词实例提供参数结构。NomBank是向宾夕法尼亚州Treebank II语料库添加额外注释层的更大努力的一部分。宾夕法尼亚大学的PropBank、NomBank和其他注释项目结合在一起，将有助于创建更好的文本自动分析工具。本文详细描述了NomBank项目，包括其规范和创建资源所涉及的过程。我们为宾夕法尼亚树库中的一些所有格结构提供了粗略的注释，但只有那些符合其标准的所有格结构。"}
{"pid": "H93-1052", "zh_sum": "以往的研究[盖尔、丘奇和亚洛夫斯基，1992年]表明，多义词很可能在每一篇话语中都有一个意义。本文表明，对于某些搭配定义，一个多义词在本质上每个搭配只有一个意义。我们检验了这一经验假设对意义和搭配的几种定义，发现它对二元歧义的准确率为90-99%。我们在消歧算法中利用了这一特性，使用非常局部上下文的组合模型，该算法的精度达到92%。我们将搭配定义为两个单词在定义的关系中的共现。为了分析和比较几种搭配的行为，我们使用了熵的度量以及将搭配组织为决策列表标记heldout数据时得到的结果。我们发现，动词的宾语比主语在WSD中起着更为主导的作用，名词从名词或形容词修饰语中获得了更为稳定的消歧信息。"}
{"pid": "P93-1001", "zh_sum": "Char Align：一个在字符层面对齐平行文本的程序最近有许多关于在句子层面对齐平行文本的论文，例如Brown等人（1991年）、Gale和Church（即将出现）、Isabelle（1992年）、Kay和R/Ssenschein（即将出现）、Simard等人（1992年）、Warwick Armstrong和Russell（1990年）。在干净的输入上，如加拿大汉萨德（Canadian Hansards），这些方法非常成功（至少96%的句子正确）。不幸的是，如果输入有噪声（由于OCR和/或未知的标记约定），那么这些方法往往会崩溃，因为噪声会使段落边界很难找到，更不用说句子了。本文描述了一个新的程序charalign，该程序基于Simard等人提出的同源方法，在字符级而不是句子/段落级对齐文本。我们表明，利用正交同源词可以实现文本片段的廉价对齐。Char\\u align是为共享公共字母表的语言对设计的。"}
{"pid": "A00-2024", "zh_sum": "基于剪切和粘贴的文本摘要我们提出了一种基于剪切和粘贴的文本摘要生成器，它使用来自对人类书面摘要分析的操作。摘要器编辑提取的句子，使用约简删除不必要的短语，并使用组合将重新排序的短语合并为连贯的句子。我们的工作包括一个基于统计的句子分解程序，该程序可以识别摘要中的短语在原始文档中的来源，生成一个对齐的摘要和文章语料库，我们用来开发摘要生成器。我们首先提取句子，然后删除多余的短语，并使用（手动）重组规则产生连贯的输出。我们手动分析了30个人写的摘要，发现19%的句子不能通过源文本的剪切和粘贴操作来解释。"}
{"pid": "J95-2002", "zh_sum": "一种计算前缀概率的高效概率上下文无关解析算法我们描述了Earley的随机上下文无关语法解析器的扩展，该解析器在给定随机上下文无关语法和输入字符串的情况下计算以下数量：a）语法生成连续前缀的概率；b） 非终结符生成子字符串的概率，包括语法生成的整个字符串；c） 字符串的最可能（Viterbi）解析；d） 根据重新估计规则概率的要求，每个语法生成的后预期应用程序数。概率（a）和（b）在输入上从左到右的一次传递中递增计算。与标准的自底向上的SCFGs解析方法相比，我们的算法通过使用Earley自顶向下的控制结构有效地处理稀疏语法。它可以处理任何上下文无关的规则格式，而无需转换为某种标准格式，并将（a）到（d）的计算合并到一个算法中。最后，该算法有一些简单的扩展，用于处理部分括号内的输入，以及在非语法输入上查找部分解析及其可能性。Earley图表用于跟踪与输入一致的所有派生。"}
{"pid": "W98-1115", "zh_sum": "基于边的最佳优先图解析最佳优先概率图解析试图通过处理一些概率优值（FOM）判断为“最佳”的边来高效地进行解析。最近的工作使用概率上下文无关文法（PCFG）为成分分配概率，并将这些概率用作FOM的起点。本文将这种方法扩展到使用概率FOM来判断边（不完整成分），从而对解析工作提供更细粒度的控制。我们将展示如何使用将PCFG二值化的常见思想，以一种特别简单的方式实现这一点。所获得的结果比之前的最佳结果大约提高了20倍，也就是说，我们的解析器使用二十分之一的边数获得了相同的结果。此外，我们还表明，这种改进在解析精度和召回水平上优于穷举解析。我们引入overpassing作为一种技术，通过在找到第一个完整的解析树后继续解析来提高解析精度。"}
{"pid": "P07-1055", "zh_sum": "用于从细到粗情感分析的结构化模型在本文中，我们研究了一种用于在不同粒度级别上对文本情感进行联合分类的结构化模型。模型中的推理基于标准序列分类技术，使用约束维特比来确保一致的解决方案。这种模型的主要优点是，它允许文本中一个级别的分类决策影响另一个级别的决策。实验表明，相对于孤立训练的模型，该方法可以显著降低分类误差。我们表明，联合学习细粒度（句子）和粗粒度（文档）情感可以提高这两个层面的预测。"}
{"pid": "P85-1008", "zh_sum": "本体论混乱为了便于语篇解释，英语句子的逻辑形式应该既接近英语，又在句法上简单。在本文中，我提出了一种一阶非张量的逻辑表示法，对于这种逻辑表示法，语义翻译可以简单地组合。关键的举措是扩展本体中允许的实体类型，而不是使逻辑符号、句子的逻辑形式或语义翻译过程复杂化。本文考察了三个经典问题——不透明状语、de re和de ditto信念报告之间的区别以及内涵语境中的同一性问题——为这种逻辑符号带来的困难，并表明这些困难是可以克服的。本文最后陈述了这种方法所预设的语义学观点。我们提倡本体论上混杂的表示，包括各种类型的实体。我们认为，自然语言的语义表示不必是高阶的，因为本体论的混乱可以解决这个问题。"}
{"pid": "P08-1086", "zh_sum": "机器翻译中大规模基于类的语言建模的分布式词聚类在统计语言建模中，一种减少数据稀疏问题影响的技术是将词汇划分为等价类。在本文中，我们研究了将这种技术应用于在大型语料库上训练的高阶n-gram模型的效果。我们对exchange聚类算法进行了改进，提高了某些部分基于类的模型的效率，并引入了该算法的分布式版本，以使用此类大型训练语料库（>300亿标记）高效地获得大型词汇（>100万词）的自动单词分类。然后将得到的聚类用于训练部分基于类的语言模型。我们表明，在最先进的统计机器翻译系统的对数线性模型中，将它们与基于单词的n-gram模型相结合，可以提高翻译质量，如BLEU评分所示。我们介绍了预测类二元图模型。"}
{"pid": "W03-1730", "zh_sum": "基于HHMM的汉语词法分析器ICTCLAS本文介绍了中国科学院计算技术研究所（Inst.of Computing Tech.，CAS）在ACL SIGHAN赞助的第一次国际汉语分词烘焙活动中的结果。作者介绍了我们的汉语词法分析器ICTCLAS基于HHMM的统一框架，并解释了六轨的操作。然后给出评价结果并进行进一步分析。对ICTCLAS的评估表明，其性能具有竞争力。与其他系统相比，ICTCLAS在CTB和PK封闭赛道中均排名第一。在PK公开赛中，排名第二。ICTCLAS BIG5版本仅在两天内从GB版本转换而来；然而，它在两条BIG5封闭轨道上取得了很好的成绩。通过第一次bakeoff，我们可以更多地了解中文分词的发展，并对基于HHMM的方法更有信心。同时，我们在评估过程中确实发现了我们的问题。bakeoff很有趣，也很有帮助。"}
{"pid": "W03-0424", "zh_sum": "与语言无关的NER使用最大熵标记器命名实体识别（NER）系统需要集成多种信息以获得最佳性能。本文证明了最大熵标记器可以有效地对这些信息进行编码，并以非常高的精度识别命名实体。tagger使用可用于多种语言的功能，不仅适用于英语，而且适用于其他语言，如德语和荷兰语。我们在同一文档的前一句中，在同一标记的最新前一个实例的标签上的特定位置设置标记的标签。我们的命名实体识别器在语料库中的pos标记和分块文档上运行，以识别和提取命名实体作为潜在主题。"}
{"pid": "D11-1006", "zh_sum": "多源迁移去泛化依赖解析器我们提出了一种简单的方法，将依赖解析器从具有标记训练数据的源语言迁移到没有标记训练数据的目标语言。我们首先证明了去西化解析器可以在语言之间直接传输，比无监督解析器产生更高的准确率。然后，我们使用约束驱动的学习算法，从并行语料库中提取约束来投影最终的解析器。与之前关于投影语法资源的工作不同，我们展示了引入多种源语言的简单方法可以显著提高生成的解析器的总体质量。与之前研究的跨八种不同语言的无监督和投影解析系统相比，我们系统中的投影解析器具有最先进的性能。我们表明，词性标记包含大量用于未标记依赖项解析的信息。我们演示了一种语法归纳的替代方法，即将具有注释的语言中的引用解析树投影到资源匮乏的语言中。其他语言中的树库仍然可以作为学习的代理，这些特征通常会传递有用的信息。我们证明，从单个oracle选择的语言进行投影可以获得良好的解析性能。"}
{"pid": "D10-1120", "zh_sum": "利用通用语言学知识指导语法归纳，我们提出了一种语法归纳方法，该方法利用句法共性改进跨一系列语言的依赖性分析。我们的方法使用一组手动指定的独立于语言的规则来识别跨语言常见的语法类别对之间的语法依赖关系。在概率模型的推理过程中，我们使用后验期望约束要求我们推断的依赖关系的最小比例是这些规则的实例。我们还自动优化了粗标记输入中给出的语法类别。在六种语言中，我们的方法大大优于最先进的无监督方法。我们的系统是弱监督的，其中手动定义的通用语法规则（USR）用于约束概率贝叶斯模型。"}
{"pid": "D07-1090", "zh_sum": "机器翻译中的大型语言模型本文介绍了大规模统计语言模型在机器翻译中的优势。我们提出了一种分布式基础设施，用于训练多达2万亿个令牌，从而使语言模型具有多达3000亿个n-gram。它能够为快速的单通解码提供平滑的概率。我们引入了一种新的平滑方法，称为愚蠢退避（Study Backoff），该方法在大数据集上训练成本低廉，并且随着训练数据量的增加，其质量接近Kneser-Ney平滑。英语中的5克单词语言模型在各种单语语料库上进行训练。在语言模型的情况下，由于缺乏计算资源，我们常常不得不删除低频词，因为k-gram的特征空间往往非常大，即使在分布式环境中，我们有时也需要截断。为了将LMs扩展到具有高阶依赖关系的大型语料库，我们考虑更容易扩展的分布式语言模型。愚蠢的退避平滑在分布式框架中的训练和部署要比依赖上下文的平滑方案（如Kneser-Ney）高效得多。我们表明，来自新闻领域（用于构建语言模型）的训练数据每增加一倍，就会提高大约0.5个BLEU点。我们每天使用1500台机器从1.8TB的web数据中计算n-gram的相对频率。"}
{"pid": "L08-1093", "zh_sum": "宾夕法尼亚大学话语树库2.0。我们介绍了宾夕法尼亚州语篇树库PDTB-2.0的第二个版本，描述了它在100万字的《华尔街日报》语料库中基于词汇的语篇关系注释及其两个抽象客体论点。我们描述了注释的各个方面，包括（a）话语关系的论点结构，（b）关系的意义注释，以及（c）话语关系及其每个论点的属性。我们列出了PDTB-1.0和PDTB-2.0之间的差异。我们为语料库中注释的几个方面提供了有代表性的统计数据。我们提出了宾州语篇树库（PDTB），这是一个语料库，它按照谓词-变元方法在宾州树库的顶部提供了语篇级注释（Webber，2004）。"}
{"pid": "J97-4005", "zh_sum": "随机属性值语法规则语法和上下文无关语法的概率类比在计算语言学中是众所周知的，目前是深入研究的主题。然而，迄今为止，尚未提出令人满意的属性值语法概率模拟：以前的尝试未能定义适当的参数估计算法。本文定义了随机属性值文法，并给出了计算其参数最大似然估计的算法。估计算法改编自Della Pietra、Della Pietra和Lafferty（1995）。为了估计模型参数，需要计算随机场下某些函数的期望值。在Della Pietra、Della Pietra和Lafferty（代表英语正交约束）讨论的应用程序中，可以使用Gibbs抽样来估计所需的期望值。属性值语法生成受约束的语言这一事实使得Gibbs采样不适用，但我表明可以使用更通用的Metropolis-Hastings算法进行采样。我们为SUBGs提出了一个马尔可夫随机场或对数线性模型。"}
{"pid": "W11-1902", "zh_sum": "斯坦福大学（欧元）™本文详细介绍了斯坦福大学在CoNLL2011共享任务中提交的共指消解系统。我们的系统是一组确定性的共指消解模型的集合，这些模型融合了词汇、句法、语义和话语信息。所有这些模型都通过共享同一集群中提及的属性（如性别和数量）来使用全局文档级信息。我们参与了开放式和封闭式跟踪，并提交了使用预测和黄金提及的结果。我们的系统在两条赛道上都排名第一，封闭赛道得分57.8，开放赛道得分58.3。斯坦福大学共指消解器（Stanford coreference resolver）获得了CoNLL-2011共指消解任务的冠军，它采用了一度流行的基于规则的方法，只需使用编码上述共指传统语言约束的规则来解析代词，如约束约束、性别和数字一致。然而，在这些标准评价语料库中，困难代词的出现频率很低，这丝毫没有削弱它们的重要性。"}
{"pid": "J01-2002", "zh_sum": "通过结合机器学习系统提高词类标注的准确性我们研究了如何利用执行相同NLP任务的不同数据驱动系统所学习的语言模型的差异来获得比最佳单个系统更高的精度。我们在三个不同的标记语料库的基础上，通过涉及形态句法词类标记任务的实验来实现这一点。在相同的语料库数据上训练四个著名的标记生成器（隐马尔可夫模型、基于记忆、转换规则和最大熵）。在比较之后，使用几种投票策略和第二阶段分类器将它们的输出进行组合。所有组合标记器的性能都优于其最佳组件。错误率的降低因所讨论的材料而异，但LOB语料库的错误率可高达24.3%。我们报告了使用Penn Treebank进行词性标注的域内训练数据的arounf准确率为97%。"}
{"pid": "N01-1008", "zh_sum": "用于共指消解的文本和知识挖掘传统上，共指是通过满足显著性、句法、语义和语篇约束的组合来解决的。获取此类知识耗时、困难且容易出错。因此，我们提出了一种从标注文本语料库中挖掘共指规则的知识极简方法。语义一致性证据是协同引用所需的一种知识形式，可以很容易地从WordNet中检索到。额外的一致性知识是通过应用于未标记文本的元引导算法发现的。我们通过Wordnet使用路径，不仅使用同义词和is-a关系，还使用部分、形态派生、光泽文本和多义词，这些都是根据路径元素的关系类型和数量进行加权的。利用WordNet中的路径模式计算NPs之间的语义一致性。"}
{"pid": "P03-1004", "zh_sum": "基于核的文本分析快速方法基于核的学习（如支持向量机）已成功应用于自然语言处理（NLP）中的许多难题。在NLP中，虽然特征组合对于提高性能至关重要，但它们是经过启发式选择的。内核方法改变了这种情况。核方法的优点是，有效的特征组合被隐式地扩展，而不损失通用性和增加计算成本。基于核的文本分析在准确性方面表现出优异的性能；然而，这些方法通常太慢，无法应用于大规模文本分析。在本文中，我们扩展了一个篮子挖掘算法，将基于核的分类器转换为简单快速的线性分类器。在基于英语的NP组块、日语分词和日语依赖句法分析上的实验结果表明，我们的新分类器比标准的基于核的分类器快30到300倍。我们提出了多项式核反转（PKI）。PKI-反向索引，为每个特征存储其出现的支持向量。PKE方法使用篮子挖掘方法从扩展中删减许多特性。PrefixSpan算法（Pei et al，2001）的扩展用于在低次多项式核空间中有效地挖掘特征。"}
{"pid": "N06-1025", "zh_sum": "利用语义角色标记WordNet和Wikipedia进行共指消解，本文提出了一种基于机器学习的共指消解系统的扩展，该系统利用了不同语义知识源的特征。这些特征表示从WordNet和Wikipedia挖掘的知识，以及关于语义角色标签的信息。我们发现，语义特征确实提高了代词和普通名词等不同指称表达类型的表现。我们表明，将谓词参数对作为特征包含可以提高共指解析器的性能。我们训练了一个模型来分类两个提及是否是共同引用的。我们建议从Wikipedia中挖掘语义相关性，这可以解决使用WordNet所遇到的数据稀疏问题。"}
{"pid": "P10-1142", "zh_sum": "有监督的名词短语共指研究：前十五年，计算参考解析的研究重点在过去十年中已经从启发式方法转向机器学习方法。本文综述了自15年前监督共指研究开始以来，监督共指研究的主要里程碑。我们认为，所谓的提及对模型存在一些设计缺陷，这些缺陷源于模型的局部受限视角。"}
{"pid": "N10-1013", "zh_sum": "词义的多原型向量空间模型词汇语义的当前向量空间模型创建单个“原型”向量来表示单词的含义。然而，由于词汇的歧义性，用一个向量来编码词义是有问题的。本文提出了一种使用聚类为每个单词生成多个“特定意义”向量的方法。这种方法提供了一种与上下文相关的词义向量表示，自然地适应了同形异义和多义。与人类对孤立词和句子语境中词的语义相似性判断的实验比较表明，这种方法优于原型和基于范例的向量空间模型。我们介绍了一种多原型VSM，首先通过聚类上下文应用词义识别，然后利用词义标记词的上下文构建原型。"}
{"pid": "W04-3213", "zh_sum": "无监督语义角色标注我们提出了一种无监督的方法来标记动词的变元及其语义角色。我们的bootstrapping算法进行初始明确的角色分配，然后迭代更新未来分配所基于的概率模型。我们方法的一个新方面是使用动词、时隙和名词类信息作为概率模型中后退的基础。我们在知情的基线上实现了50–65%的错误率降低，这表明我们的方法对于迄今为止依赖于大量手动生成的培训数据的任务的潜力。我们提出了一种无监督的方法来标记动词的参数及其语义角色。我们使用手工制作的动词词库代替有监督的语义角色训练数据来执行无监督语义角色标记。"}
{"pid": "W02-0505", "zh_sum": "阿拉伯文本中姓名的机器音译我们提出了一种基于有限状态机的语音和拼写映射的音译算法。音译模型可以在相对较小的姓名列表上进行训练。我们引入了一种新的基于拼写的模型，该模型比最先进的基于语音的模型准确得多，并且可以在更容易获得的训练数据上进行训练。我们将我们的音译算法应用于名称从阿拉伯语到英语的音译。我们报告了基于精确匹配准则和基于人类主观评价的算法的准确性。我们还将系统的准确性与人工翻译人员的准确性进行了比较。我们通过结合基于语音和拼写的模型，将阿拉伯语文本中的命名实体音译为英语，并使用全名网络计数、命名实体共同引用和上下文网络计数对候选人重新排序。我们发现，使用外部语言资源，如WWW上的音译候选数，可以极大地提高音译的准确性。基于Out拼写的模型直接将英语字母序列映射为具有相关概率的阿拉伯语字母序列，这些字母序列在一个小的英语/阿拉伯语姓名列表上进行训练，而不需要英语发音。"}
{"pid": "D07-1097", "zh_sum": "单一麦芽还是混合麦芽？多语言解析器优化研究我们描述了在CoNLL 2007依赖项解析共享任务的多语言跟踪中，针对十种语言的MaltParser系统的两阶段优化。第一阶段包括通过优化解析算法、特征模型和学习算法的参数来调整每种语言的单个解析器系统。第二阶段是构建一个集成系统，该系统结合了六种不同的语法分析策略，并根据每种语言的最佳参数设置进行推断。在官方测试集上进行评估时，集成系统的性能明显优于单一解析器系统，并获得了最高的平均标记附件分数。我们将两阶段方法扩展到三阶段体系结构，其中解析器和标签器生成n个最佳的解析器列表，然后重新排序。我们指出，中文的官方结果包含一个bug，我们系统的实际性能实际上要高得多。我们实现了一个从左到右的arc渴望解析模型，解析器从左到右扫描输入序列，并将右侧依赖项尽快连接到它们的头部。"}
{"pid": "W09-1401", "zh_sum": "BioNLP概述；09关于事件提取的共同任务尽管人们对单词的语义定向和大型极性词汇的创建给予了相当大的关注，但情感分析的研究必须依赖有限的小型情感词汇。在本文中，我们展示了如何使用Mechanical Turk创建高质量、中等大小的情感词汇。除了关于术语引发的情绪的问题外，我们还展示了如何包含词语选择问题来阻止恶意数据输入，帮助识别注释者可能不熟悉目标术语的实例（允许我们拒绝此类注释），并帮助获得感官层面（而非词语层面）的注释。我们对注释进行了广泛的分析，以更好地理解由不同词类的术语引起的情绪分布。我们确定哪些情绪倾向于由同一术语同时诱发，并表明某些情绪确实是密切相关的。BioNLP 09事件提取共享任务是对生物医学事件提取系统的首次大规模评估，吸引了24个小组的参与，并建立了标准的事件表示方案和数据集。BioNLP 09共享任务是第一个为提取此类生物关系提供一致数据集和评估工具的共享任务。"}
{"pid": "P03-1003", "zh_sum": "一种用于问答的噪声信道方法我们介绍了一种用于问答的概率噪声信道模型，并展示了如何在端到端QA系统中利用该模型。我们的嘈杂通道系统优于使用类似资源的最先进的基于规则的QA系统。我们还表明，我们提出的模型足够灵活，可以在一个数学框架内容纳许多QA特定的资源和技术，从WordNet、结构化和半结构化数据库的利用到推理和解释。我们为QA开发了一个噪声信道模型，该模型解释了如何通过一系列随机操作将包含给定问题答案的句子改写为该问题。提出了高级QA的另一种组合策略：（1）使用最大熵框架将基于知识的Q/a实现与（2）用于Q/a的统计噪声信道算法和（3）基于模式的方法相结合，该方法基于句法/语义处理。"}
{"pid": "C10-1011", "zh_sum": "高精度和快速依赖性解析并不矛盾。除了高精度之外，解析和训练时间短是解析器最重要的特性。然而，解析和训练时间仍然相对较长。为了确定原因，我们分析了依赖关系解析器的时间使用情况。我们说明，在支持向量机中，特征到其权重的映射是时间复杂度的主要因素。为了解决这个问题，我们将被动攻击感知机算法作为哈希核来实现。哈希内核大大提高了解析时间，并考虑了在训练期间构建的负面示例的特性。这导致了更高的精确度。我们可以通过并行特征提取和并行解析算法进一步提高解析和训练速度。我们确信，哈希内核和并行化可以成功地应用于其他NLP应用程序，如基于转换的依赖关系解析器、短语结构解析器和机器翻译。我们表明，由于解析器使用了额外的负面特性，哈希内核提高了解析速度和准确性。Mateparser是一种高效的二阶依赖关系解析器，它对兄弟姐妹以及孙子姐妹之间的交互进行建模。"}
{"pid": "A97-1014", "zh_sum": "自由语序语言的注释方案我们描述了一个注释方案和一个为非配置语言创建语言注释语料库而开发的工具。由于这种形式主义的要求与配置语言的要求不同，因此添加了几个特性，影响了方案的体系结构。由此产生的方案反映了语言的分层概念，并且仅对特定代表层之间的相互关系做出了最低限度的假设。我们发布了NEGRA语料库，这是一个手工解析的德语报纸文本语料库，包含大约20000个句子。"}
{"pid": "N01-1006", "zh_sum": "快速通道中的基于转换的学习基于转换的学习已成功地用于解决许多自然语言处理问题。它在许多自然语言处理任务上实现了最先进的性能，并且不容易过度训练。然而，它确实有一个严重的缺点：训练时间往往相当长，尤其是在NLP中经常使用的大型语料库上。在本文中，我们提出了一种新颖而现实的方法，可以在不牺牲学习成绩的情况下加快基于转换的学习者的训练时间。本文将我们改进后的学习者与其他两个系统：基于标准转换的学习者和ICA系统（Hepple，2000）所需的训练时间和取得的成绩进行了比较和对比。这些实验的结果表明，我们的系统能够在训练时间上取得显著改善，同时仍能达到与标准转换学习者相同的性能。这对在执行的任何部分利用基于转换的学习的系统和算法来说都是一个有价值的贡献。我们提出了fnTBL工具包，它在规则学习中实现了一些优化，从而大大加快了培训所需的时间。"}
{"pid": "P98-1046", "zh_sum": "本文研究了基于交叉Levin类的规则意义扩展，具体探讨了动词多义的问题，以及如何通过附加特定的句法短语来实现意义的规则扩展。我们认为动词类是对意义的规则扩展进行概括的关键。目前的英语分类方法Levin classes和WordNet在适用性方面存在局限性，阻碍了它们作为一般分类方案的实用性。我们提出了一种改进的Levin类，即相交集，它是一种更细粒度的分类，具有更连贯的语法框架集和相关的语义组件。我们有初步迹象表明，与最初的Levin类相比，相交集的成员资格将与WordNet更加兼容。我们还开始研究葡萄牙语的相关类别，发现这些动词表现出类似的连贯句法和语义特性。我们表明，在某些情况下，多个列表可以被解释为规则意义扩展和定义的相交Levin类，这是对基本Levin类的更细粒度、语法和语义一致的细化。我们认为，使用句法框架和动词类可以简化不同动词意义的定义。"}
{"pid": "P07-1030", "zh_sum": "通过Web挖掘实现概念特定关系的完全无监督发现我们提出了一种Web挖掘方法，用于发现和增强特定概念（词类）参与的关系。我们发现一系列关系都集中在给定的概念上，而不是像以前大多数工作中那样的一般已知关系。我们的方法基于包含概念词和其他相关词的聚类模式。我们在三个不同的丰富概念上对该方法进行了评估，发现在每种情况下，该方法都能生成各种各样的关系，具有良好的精度。我们将介绍术语频率模式用于关系发现。作为提取类对之间关系的先决条件，我们的方法从非结构化Web文档中提取类实例，方法是将实例对作为查询提交，并分析Web搜索引擎返回的前1000个文档的内容。我们提出了一种无监督发现概念特定关系的方法，需要初始单词种子。"}
{"pid": "P08-1028", "zh_sum": "基于向量的语义合成模型本文提出了一个在向量空间中表达短语和句子意义的框架。我们的方法的核心是向量合成，我们根据加法和乘法函数来操作它。在此框架下，我们引入了一系列的合成模型，并在句子相似性任务中进行了实证评估。实验结果表明，与人类的判断相比，乘法模型优于加法模型。我们提出了一个通用框架，在该框架中，通过组合复杂表达式中各个单词的向量表示来计算复杂表达式的意义表示。"}
{"pid": "P03-1035", "zh_sum": "改进的汉语分词源通道模型本文提出了一种基于改进的汉语句子生成源通道模型的汉语分词系统。汉语词被定义为以下四种类型之一：词汇词、形态派生词、因素词和命名实体。我们的系统为词级汉语处理的四个基本特征提供了统一的方法：（1）分词，（2）形态分析，（3）因子检测，（4）命名实体识别。该系统的性能在一个手动注释的测试集上进行评估，并与几个最先进的系统进行比较，考虑到汉语单词的定义往往因系统而异。"}
{"pid": "N06-2033", "zh_sum": "通过重新分析进行语法分析器组合我们提出了一种新的语法分析器组合方案，该方案通过在输入语句已经被多个不同的语法分析器分析之后重新分析它们来工作。我们将此思想应用于依赖项和成分解析，生成的结果超过了单个解析器的最先进的精度水平。我们引入一个成分计数阈值，并从所有可能的成分组合中搜索具有最大计数的树。我们组合了五个解析器，得到92.1分，而最好的单个解析器得到91.0分。"}
{"pid": "P04-1018", "zh_sum": "一种基于钟形树的同步共指消解算法本文提出了一种新的共指消解方法，该方法使用钟形树表示搜索空间，将共指消解问题转化为寻找从钟形树根到叶节点的最佳路径。使用最大熵模型对这些路径进行排序。将报告2002年和2003年自动内容提取（ACE）数据的共指性能。我们还利用MUC6数据训练了一个共指系统，并获得了竞争结果。我们在测试时应用beam搜索，但使用静态先行项分配，并使用批学习学习对数线性模型。我们使用一个钟形树，其叶子表示可能将提及的内容划分为实体，然后训练一个模型来搜索该树。为了处理计算复杂性，我们通过钟形树进行波束搜索，启发式地搜索最可能的分区。我们表明，只需将所有提及的内容集中在一起，就可以获得非常高的MUC分数。我们应用ANY谓词为其实体提及模型生成集群级特征，该模型的性能不如提及对模型。"}
{"pid": "P07-1037", "zh_sum": "直到最近，将基于短语的统计机器翻译（PBSMT）扩展为句法结构导致系统性能恶化。在这项工作中，我们表明以超标记的形式结合词汇句法描述可以产生明显更好的PBSMT系统。我们描述了一种新的PBSMT模型，该模型将supertags集成到目标语言模型和翻译模型的目标端。使用了两种超标记：词汇化树邻接语法和组合范畴语法。尽管这两种方法之间存在差异，但supertaggers提供了类似的改进。除了超级标记之外，我们还探讨了基于组合运算符的表面全局语法度量的实用性。我们在阿拉伯语到英语的NIST 2005测试集上进行了各种实验，解决了稀疏性、可扩展性和系统子组件的实用性等问题。与最先进的PBSMT模型相比，我们的最佳结果（0.4688 BLEU）提高了6.1%，与NIST 2005任务中的领先系统相比，该模型具有非常好的优势。"}
{"pid": "C04-1180", "zh_sum": "CCG解析器的宽覆盖语义表示本文展示了如何从宽覆盖CCG解析器生成的派生构造语义表示。与解析器本身返回的依赖结构不同，这些依赖结构可以直接用于语义解释。我们证明，在看不见的《华尔街日报》文本中，超过97%的句子可以产生格式良好的语义表示。我们认为，这是朝着广泛覆盖语义解释迈出的重要一步，这是自然语言处理领域的关键目标之一。我们提出了一种算法，该算法学习具有语义的CCG词汇，但需要在训练数据中完全指定CCG派生。我们考虑了使用CCG构建广泛覆盖语义表示的挑战性问题，但没有学习词汇。"}
{"pid": "N06-1014", "zh_sum": "一致对齐我们提出了一种无监督的对称词对齐方法，其中两个简单的非对称模型联合训练，以最大限度地提高数据可能性和模型之间的一致性。与独立训练模型交叉预测的标准实践相比，联合训练可减少32%的AER。此外，与对称化的IBM model 4预测相比，一对简单高效的HMM对齐器可将AER降低29%。我们在大量的训练数据上使用区分性SMT，在67000个句子上训练150万个特征。"}
{"pid": "J02-1003", "zh_sum": "生成引用表达式：增量算法的布尔扩展本文从逻辑角度分析了引用表达式的生成，解决了该领域现有算法的不完整性。在研究了对单个对象的引用之后，我们讨论了对集合的引用，包括使用求反和不相交属性的布尔描述。为了保证只要存在这样的描述，就可以生成一个可区分的描述，本文对Dale和Reiter（1995）的增量算法进行了推广和扩展。"}
{"pid": "J95-2003", "zh_sum": "中心：话语局部连贯的建模框架本文关注的是关注焦点、指称表达的选择和话语片段内话语的感知连贯之间的关系。它提出了一个框架和最初的中心理论，旨在模拟注意状态的局部成分。本文考察了局部连贯与指称表达选择之间的相互作用；文章认为，在特定的注意状态下，连贯性的差异部分对应于不同类型的指称表达所产生的推理需求。这表明，以中心为模型的注意状态属性可以解释这些差异。我们的中心模型使用在特定句子中实现的话语实体的排序，并计算相邻句子之间的转换，以提供对文本恰当性的洞察。我们的中心理论假设，理解相邻句子时的注意力中心与句法位置和指称形式之间存在着密切的联系。我们的中心理论是一种基于实体的局部连贯理论，它声称话语中提到的某些实体比其他实体更为中心，并且这种属性限制了说话人对某些指称表达的使用。我们的中心理论是近二十年来计算语言学中实体连贯建模的一个有影响力的框架。"}
{"pid": "P03-1051", "zh_sum": "基于语言模型的阿拉伯语分词我们通过一个模型来近似阿拉伯语丰富的词形，即一个单词由模式前缀*-词干后缀*（*表示一个语素的零次或多次出现）中的一系列语素组成。我们的方法由一个手动分割的小型阿拉伯语语料库播种，并使用它引导一个无监督的算法，从一个大型未分割的阿拉伯语语料库中构建阿拉伯语分词器。该算法使用三元语言模型来确定给定输入的最可能语素序列。该语言模型最初是从一个约110000个单词的小型手动分段语料库中估计出来的。为了提高分词精度，我们使用无监督算法从1.55亿单词的未分词语料库中自动获取新词干，并使用扩展的词汇和训练语料库重新估计模型参数。最终的阿拉伯语分词系统在包含28449个单词标记的测试语料库上实现了约97%的精确匹配精度。我们认为这是一种最先进的性能，该算法可以用于许多高度屈折的语言，前提是可以创建感兴趣语言的小型手动分割语料库。我们演示了一种阿拉伯文本分割技术，并将其作为机器翻译中的一个形态学处理步骤。"}
{"pid": "J03-4004", "zh_sum": "使用自动获得的选择偏好对名词、动词和形容词进行消歧选择偏好被词义消歧系统用作消歧信息的来源之一。我们通过对标准测试语料库的英语形容词-名词、主语和直接宾语语法关系的选择偏好来评估WSD。选择偏好是特定于动词或形容词类别的，而不是单独的词形，因此它们可以用来消除共现形容词和动词的歧义，而不仅仅是名词性参数头。我们还研究了如何使用逐语篇一个意义的启发式方法，将一个单词的意义标记传播到当前文档中相同单词的其他位置，以增加覆盖率。虽然与同一语料库上的其他无监督WSD系统相比，这些偏好表现得很好，但结果表明，对于许多应用程序，需要更多的知识源才能达到足够的准确性和覆盖率。除了量化绩效外，我们还分析了结果，以调查选择偏好达到最佳精度的情况，以及每话语一种感觉的启发式提高绩效的情况。我们报告，Word类模型在无监督WSD中表现良好。"}
{"pid": "H94-1046", "zh_sum": "利用语义一致性进行语义识别，本文提出了自动语义识别系统的基准。在一个文本语料库中，开放类单词在句法和语义上都被标记，用于探索三种意义识别的统计策略：猜测启发式、最频繁启发式和共现启发式。当没有关于感觉频率的信息可用时，使用WordNet中替代感觉的数量进行猜测的启发式方法在45%的时间内是正确的。当词义频率的统计数据来自语义一致性时，假设每个词都使用其最频繁出现的词义，则正确率为69%；当仅对多义词进行计算时，这个数字就下降到了58%。当共现启发式方法利用同一句话中先前同时出现的单词时，几乎没有观察到改善。语义一致性仍然太小，无法估计共现启发式的潜在限制。我们准备了一个语义标记的语料库SEMCOR，其中包含一个用WORDNET的精练语义标记的棕色语料库的实质子集。"}
{"pid": "P97-1005", "zh_sum": "文本体裁的自动检测随着用户可用的文本数据库变得越来越大、越来越异构，体裁作为主题和结构分类原则的补充，对计算语言学来说变得越来越重要。我们提出了一种将体裁视为与各种表面线索相关的面束的理论，并认为基于表面线索的体裁检测与基于深层结构特性的体裁检测一样成功。我们认为，句法分析和词义消歧也可以受益于体裁分类。我们避免使用结构化标记，因为它们需要标记或解析的文本，并用字符级标记（例如标点符号计数）和派生标记（即，从词汇和字符级标记的度量中派生出的比率和变化度量）替换它们。"}
{"pid": "W03-0425", "zh_sum": "通过分类器组合进行命名实体识别本文提出了一个命名实体识别的分类器组合实验框架，其中在不同条件下组合了四种不同的分类器（鲁棒线性分类器、最大熵、基于变换的学习和隐马尔可夫模型）。在没有使用地名录或其他额外培训资源的情况下，组合系统的英语发展数据表现为91.6F；综合姓名、位置和人员地名录，以及根据其他更一般的数据培训的命名实体系统，可将英语数据的F-测量误差减少15%至21%。我们测试了组合四个系统结果的不同方法，发现鲁棒风险最小化效果最好。"}
{"pid": "J00-3003", "zh_sum": "对话行为建模用于会话语音的自动标记和识别我们描述了一种用于建模会话语音中对话行为的统计方法，即类似于言语行为的单元，如陈述、问题、反向通道、同意、不同意和道歉。我们的模型基于词汇、搭配和韵律线索以及对话行为序列的语篇连贯性来检测和预测对话行为。对话模型基于将对话的话语结构视为隐马尔可夫模型，而个人对话则作为模型状态的观察结果。对可能的对话行为序列的约束通过对话行为n-gram建模。统计对话语法与单词n-gram、决策树和神经网络相结合，对每个对话行为的特殊词汇和韵律表现进行建模。我们开发了语音识别与对话建模的概率集成，以提高语音识别和对话行为分类的准确性。使用一个大型手动标记数据库对模型进行训练和评估，该数据库包含1155个会话，这些会话来自自发的人对人电话语音的交换台语料库。我们实现了良好的对话行为标记准确率（65%基于错误的自动识别单词和韵律，71%基于单词转录本，而机会基线准确率为35%，人类准确率为84%），并且单词识别错误略有减少。我们使用HMMs作为话语的一般模型，并将其应用于会话中的言语行为（或对话行为）。"}
{"pid": "P10-1110", "zh_sum": "线性时间增量解析的动态规划由于其高效性，诸如shift-reduce等增量解析技术已广受欢迎，但仍存在一个主要问题：与动态规划相比，搜索是贪婪的，只探索整个空间的一小部分（即使使用波束搜索）。我们发现，令人惊讶的是，通过基于特征值合并“等效”堆栈，动态编程实际上可以用于许多移位减少解析器。从经验上看，我们的算法比最先进的移位减少依赖关系解析器的速度提高了五倍，同时不会降低准确性。更好的搜索也会带来更好的学习，我们的最终解析器优于之前报道的所有英语和汉语依赖性解析器，但速度要快得多。在本文中，我们遵循Huang和Sagae（2010）开始的研究路线，将动态规划应用于（投影）基于转换的依赖解析（Nivre，2008）。我们计算的基本思想是（在计算维特比得分时）将sh转换的得分委托给la/ra的推理规则。"}
{"pid": "D07-1031", "zh_sum": "为什么他们找不到好的HMM POS标签？本文研究了为什么期望最大化（EM）估计的HMM会产生像词性标记器（POS）这样糟糕的结果。我们发现，EM估计的HMM通常为每个隐藏状态分配大致相同数量的单词标记，而标记到POS标记的经验分布是高度倾斜的。这激发了一种贝叶斯方法，该方法使用稀疏先验，从而使估计器偏向于这种偏态分布。我们研究了Gibbs采样（GS）和变分Bayes（VB）估计量，结果表明，对于这项任务，VB的收敛速度比GS快，并且VB显著提高了EM的1对1标记精度。我们还表明，当隐HMM状态的数量显著减少时，EM的效果几乎与VB一样好。我们还指出，所有这些估计量的方差都很高，并且它们需要比通常认为的更多的迭代来接近收敛。我们通过贝叶斯模型证明了无监督词性标注（使用词典）的良好性能。"}
{"pid": "P00-1041", "zh_sum": "基于统计翻译提取摘要技术的标题生成无法生成短于一句话的文档摘要，这是经常需要的。理想的摘要系统应该理解每个文档，并直接从理解的结果中生成适当的摘要。解决这个问题的更实际的方法是使用近似方法：将摘要视为类似于统计机器翻译的问题。然后，问题就变成了从更详细的源文档中以更简洁的语言生成目标文档的问题。本文给出了使用这种方法的实验结果，其中术语选择和术语排序的统计模型被联合应用，以从训练语料库中学习的方式生成摘要。我们用高斯近似长度分布。我们从机器翻译中汲取灵感，使用统计模型生成标题，用于内容选择和句子实现。"}
{"pid": "C94-2195", "zh_sum": "一种基于规则的介词短语附着消歧方法本文描述了一种新的基于语料库的介词短语附着消歧方法，并给出了该算法与其他基于语料库的方法的性能比较结果。我们在《华尔街日报》的12766个四元组上训练了一种基于变换的学习算法。我们使用基于监督转换的学习方法和从WordNet派生的词汇和概念类，在500个随机选择的示例上实现了82%的准确率。"}
{"pid": "W02-0301", "zh_sum": "调整支持向量机用于生物医学命名实体识别我们探讨了支持向量机（SVM）在生物医学命名实体识别中的应用。为了使支持向量机的训练具有可用的最大语料库GENIA语料库的可处理性，我们提出利用词性信息将非实体类划分为子类。此外，我们还探索了一些新特性，如单词缓存和通过无监督学习训练的HMM的状态。在GENIA语料库上的实验表明，我们的类分割技术不仅可以使用GENIA语料库进行训练，而且可以提高准确率。提出的新特征也有助于提高精度。我们将基于支持向量机的识别系统与使用最大熵标记方法的系统进行了比较。对于蛋白质名称识别，精确性、召回率和f分数分别为0.492、0.664和0.565。我们使用一个包含词汇信息、词性标记、词缀及其组合的特征集，以便将术语识别并分类为GENIA项目中使用的一组通用生物类（GENIA，2003）。"}
{"pid": "N07-4013", "zh_sum": "TextRunner：Web上的开放式信息抽取传统的信息抽取系统侧重于满足来自小型同质语料库的精确、狭窄、预先指定的请求。相反，TEXTRUNNER系统展示了一种新的信息提取，称为开放信息提取（OIE），在这种信息提取中，系统对整个语料库进行单一的、数据驱动的传递，并提取一大组关系元组，而不需要任何人的输入。（Banko等人，2007年）TEXTRUNNER是一个完全实现的、高度可扩展的OIE示例。TEXTRUNNER的提取被编入索引，允许快速查询机制。我们首次公开演示TEXTRUNNER系统，展示了在1.17亿个网页上执行OIE的结果。它从TEXTRUNNER提取的原始事实数量以及使用我们新的评估机制的精确度方面展示了TEXTRUNNER的威力。它显示了使用大型抽取集自动确定同义关系和对象的能力。我们为查询结果构建了更快的用户界面。我们提供了TextRunner的在线演示。"}
{"pid": "D08-1016", "zh_sum": "通过信念传播进行依赖分析我们将依赖分析描述为一个带有全局约束的新成分的图形模型。我们展示了如何应用循环信念传播（BP），这是一种简单有效的近似学习和推理工具。作为一种句法分析算法，BP算法在渐近和经验上都是有效的。即使使用二阶特征或潜在变量，这将使精确解析变得相当缓慢或NP困难，BP只需要O（n3）时间，常数因子很小。此外，与精确的一阶方法相比，这些特性显著提高了解析精度。合并其他特性将增加运行时间，而不是成倍增加。我们可以将常见的动态规划算法封装在专用因子中，以有效地全局约束变量配置。DEP-TREE是一个全局组合因子，它附加到所有链接（i，j）变量上，当链接变量的配置形成一个有效的投影依赖图时，同样贡献一个因子1。"}
{"pid": "P90-1010", "zh_sum": "对话中的混合主动性：对话语分割的研究两个人之间的对话通常是混合主动性的，对对话的控制权从一个人转移到另一个人。我们对总共1862回合的4组对话应用一套控制转移规则。控制规则的应用使我们可以导出与领域无关的话语结构。派生结构表明，主动性在话语结构中起着重要作用。为了探讨控制和主动性与中心等语篇过程的关系，我们分析了两组数据集中四类不同回指的分布情况。此分布表明某些控制段在层次上与其他控制段相关。分析表明，话语参与者通常都同意改变话题。我们还比较了任务导向对话和提供建议对话中的主动性，发现两种对话类型的控制权分配和控制权转移方式都有根本不同。这些差异可以用协作规划原则来解释。我们发现，当主动性在话语参与者之间来回传递时，对对话的控制也同样从一个说话者转移到另一个说话者。我们开发了基于话语的控制分配规则。"}
{"pid": "P06-2101", "zh_sum": "训练对数线性模型的最小风险退火当训练自然语言系统的参数时，人们倾向于最小化评估集上的1-最佳损失（误差）。由于许多自然语言问题的误差面是分段常数且充满局部极小值，因此许多系统转而优化对数似然，而对数似然是方便可微且凸的。我们建议通过培训将预期损失或风险降至最低。我们使用假设的概率分布来定义这个期望，我们逐渐强化（退火）以关注1-最佳假设。除了之前工作中使用的线性损失函数外，我们还描述了优化非线性函数的技术，如精度或BLEU度量。我们提出的实验训练对数线性组合模型的依赖分析和机器翻译。在机器翻译中，退火最小风险训练比标准最小错误训练在BLEU方面取得了显著的改进。我们还展示了标记依赖项解析的改进。我们使用线性化技术来近似对数BLEU分数的期望值。我们提出了一种确定性退火训练方法，其目标是最小化期望误差（结合熵正则化技术）。我们通过最小风险退火观察测试集增益，该退火包含一个温度参数，该参数在目标的平滑度和它反映潜在分段常数误差曲面的程度之间进行权衡。"}
{"pid": "W97-1306", "zh_sum": "CogNIAC：知识和语言资源有限的高精度共指本文提出了一种高精度代词消解系统，该系统能够以60%的准确率达到90%以上，并且对某些代词具有更好的回忆能力。有人建议，该系统正在解决一个子集的照应，不需要一般的世界知识或复杂的语言处理成功解决。该系统通过对歧义非常敏感，并且只有在满足非常高的可信度规则时才能解析代词来做到这一点。该系统能够“注意”歧义，因为它要求在显著性排名中有一个唯一的先行词，并且显著性排名不是总的顺序，即两个或多个先行词可以同等显著。鉴于系统规则的性质，它们很可能在很大程度上是领域独立的，并且反映了人类用于一般语言理解的处理策略。该系统已在两个不同的实验中进行了评估，支持该方法的总体有效性。我们的方法CogNiac是一种知识贫乏的回指消解方法，它基于一组高置信度规则，这些规则依次应用于所考虑的代词。"}
{"pid": "C90-3045", "zh_sum": "同步树邻接语法树邻接语法（TAG）的独特特性对标记的应用提出了挑战，超出了语法的有限限制，例如，对自然语言的语义解释或自动翻译任务。我们提出了一种称为同步标记的标记变体，它描述了语言之间的对应关系。形式主义的目的是将自然语言的表达与逻辑形式语言中的相关语义联系起来，或与另一种自然语言中的译文联系起来；总之，我们打算允许标记的使用超出其在语法中的作用。我们将讨论同步标记在具体示例中的应用，主要是在传递其解释中出现的一些计算问题时提及。同步树邻接语法主要用于语义，但稍后也将用于翻译。两种语言的两种句法结构的同步派生过程表明两棵树之间的跨语言同构水平（例如，同步树邻接语法）。"}
{"pid": "P04-1043", "zh_sum": "浅层统计分析中的卷积核研究本文设计并实验了一种新的用于谓词参数自动分类的卷积核。它们的主要特性是处理结构化表示的能力。支持向量机（SVM）结合使用此类核和平面特征核，对PropBank谓词参数进行分类，其精度高于当前的参数分类技术。此外，在FrameNet数据上的实验表明，支持向量机对语义角色的分类很有吸引力，即使提出的核没有产生任何改进。我们可以通过对包含谓词及其一个参数的最小子树进行分类来标记语义角色，即所谓的PAF结构。我们的卷积核有两个特点：子范畴结构的语义空间和衡量其相似性的核函数。"}
{"pid": "P03-1056", "zh_sum": "解析中文或中文树库难吗？我们详细调查了针对英语语料库开发的句法分析模型在汉语中的应用所面临的挑战。我们为宾州中文树库开发了一个因子模型统计解析器，展示了华尔街日报和中文树库之间的总体统计差异对解析器适应的最一般方法的影响。然后，我们详细分析了该语料库统计分析错误的主要来源，说明了它们的原因和相对频率，并表明，虽然某些类型的错误是由于汉语语法固有的难以理解的歧义，但其他类型的错误是由于树库注释实践造成的。我们展示了如何通过对解析模型的最大似然估计PCFG因子的独立性假设进行简单、有针对性的更改来解决每种类型的错误，这将我们的开发集上的F1从80.7%提高到82.6%，并实现了接近中文解析最佳公布数字的解析精度。我们认为，仔细的错误分类可以揭示可能的改进。在PCTB数据的PCFG分析中，名词/动词错误标记是一个常见的错误案例，在汉语中，由于缺乏虚词和词法，这种错误更加严重。汉语和英语之间存在着许多语言差异，它们对应的树库之间也存在着结构差异，其中一些使得汉语的语法分析变得更加困难。"}
{"pid": "J03-4003", "zh_sum": "自然语言分析的头驱动统计模型本文描述了三种自然语言分析的统计模型。该模型将方法从概率上下文无关语法扩展到词汇化语法，从而产生了将解析树表示为对应于树的头部中心的自顶向下派生的决策序列的方法。然后，独立性假设会产生一些参数，这些参数编码X-bar模式、子类别、补语的顺序、附加词的位置、二元词法依赖性、wh移动和紧密连接的偏好。所有这些偏好都是以词头为条件的概率来表达的。宾夕法尼亚州华尔街日报Treebank对这些模型进行了评估，表明其准确性与文献中的其他模型具有竞争力。为了更好地理解这些模型，我们还提供了不同组成类型的结果，以及在恢复各种类型的依赖关系时精度/召回结果的细分。我们通过对句法分析准确性的实验、收集树库中各种结构的频率以及基于语言动机的例子来分析模型的各种特征。最后，我们将这些模型与其他已用于解析树库的模型进行比较，旨在对各种模型的性能差异进行一些解释。我们建议首先生成短语的头部，然后使用马尔可夫过程生成其姐妹，从而利用头部/姐妹依赖关系。"}
{"pid": "C92-1019", "zh_sum": "汉语句子是由一串没有空格的字符组成的，用来标记单词。然而，句子分析和理解的基本单位是单词。因此，处理汉语句子的第一步是识别单词。识别单词的困难包括：（1）识别复杂单词，如确定性度量、重叠、派生词等；（2）识别专有名称；（3）解决歧义切分。本文针对上述困难提出了可能的解决方案。我们采用了一种具有6种不同启发式规则的匹配算法来解决歧义，成功率达到99.77%。统计数据表明，最大匹配算法是最有效的启发式算法。我们提出了前向最大匹配算法。"}
{"pid": "P94-1012", "zh_sum": "将平行英汉语料库与词汇标准进行统计对齐我们描述了我们在平行英汉文本中自动对齐句子的经验。我们的报告涉及三个相关主题：（1）科大英汉平行双语语料库的研究进展；（2） 针对Gale和Church（1991）基于长度的统计方法适用于涉及非印欧语言的对齐任务的实验；（3）一种改进的统计方法，该方法还结合了特定领域的词汇线索。我们发现，英语和汉语文本的长度相关性不如法语-英语任务中的高，导致基于长度的对齐器的成功率较低。"}
{"pid": "P07-1098", "zh_sum": "利用句法和浅层语义核进行问答分类，我们研究了句法和浅层语义信息在问答自动分类和答案重新排序中的影响。我们定义了（a）基于谓词参数结构（PASs）中编码的浅层语义的新树结构和（b）新的核函数，以利用支持向量机对此类结构的表示能力。我们的实验表明，句法信息有助于问题/答案分类等任务，而当可以从答案中提取出一组可靠的通行证时，浅层语义会做出显著贡献。我们的浅层语义表示具有更紧凑的信息，可以防止深层结构方法的稀疏性和BOW模型的弱点。"}
{"pid": "D08-1011", "zh_sum": "基于间接HMM的机器翻译系统输出合并假设对齐本文提出了一种新的多机器翻译系统输出合并假设对齐方法。提出了一种间接隐马尔可夫模型（IHMM）来解决假设对齐中的同义词匹配和词序问题。与通过最大似然估计（MLE）训练参数的传统HMM不同，IHMM的参数是从各种来源间接估计的，包括单词语义相似度、单词表面相似度和基于距离的失真惩罚。在我们对NIST基准数据集的实验中，基于IHMM的方法明显优于基于TER的最新校准模型。在2008年NIST开放式机器翻译评估的受限训练轨道中，我们使用所提出的方法的组合式SMT系统取得了最佳的中英文翻译结果。我们建议使用间接隐马尔可夫模型（IHMM）对系统输出进行成对对齐。"}
{"pid": "W02-0908", "zh_sum": "自动同义词库提取的改进在现代自然语言处理系统中，语义资源的使用很普遍，但提取词汇语义的方法直到最近才开始表现出足够好的实用性。我们评估了现有和新的同义词表提取相似性度量，并对提取性能和效率之间的权衡进行了实验。我们提出了一种基于规范属性和粗、细粒度匹配的近似算法，该算法在性能损失不大的情况下降低了同义词表提取的时间复杂度和执行时间。我们发现，随着输入数据量的增加，使用分布相似度对词汇语义资源进行同义词提取的准确性会不断提高。我们证明，大幅增加用于提取上下文的文本数量可以显著提高同义词的质量。在距离度量的比较中，我们发现JACCARD度量和TTEST权重的性能最好。"}
{"pid": "W10-2805", "zh_sum": "分布语义学中形容词-名词组成性的回归模型本文探讨了分布语义学模型中组成性的计算模型。特别是，我们对英国国家语料库中相邻的英语形容词和名词对的语义构成进行了建模。我们从BNC的一个引理化版本构建了一个基于向量的语义空间，其中最常见的a-N引理对被视为单个标记。然后，我们外推了三种不同的成分模型：简单相加模型、逐点乘法模型和偏最小二乘回归（PLSR）模型。我们为实现的模型提出了两种评估方法。我们的研究得出的结论是，基于回归的组合性模型的性能通常优于加法和乘法方法，并且还显示出一些优势，使它们在未来的研究中非常有前景。我们对带有名词的形容词的语义构成的方法借鉴了蒙塔戈夫形式语义理论传统（Montague，1974）中对形容词的经典分析，在此基础上，形容词被视为高阶谓词，并将形容词建模为应用于名词向量的权重矩阵。我们的主要创新点是利用语料库中观察到的ANs的共现向量来训练监督合成模型。我们通过观察语料库获取的短语向量来学习合成函数，这些合成函数应该自动派生出这样的合成向量。"}
{"pid": "P06-1121", "zh_sum": "近几年来，基于统计机器翻译的上下文丰富句法翻译模型的可扩展推理和训练取得了很大的进展，但现有的翻译模型在重排序和目标语言流利性方面存在不足。句法方法试图解决这些问题。在本文中，我们采用了从对齐的树-字符串对中获取（Galley et al.，2004）的多层句法翻译规则的框架，并给出了其方法的两个主要扩展：首先，我们构建了大量派生，其中包括更丰富的上下文规则，而不是仅计算一个派生，以最小程度地解释句子对，并解释未对齐单词的多种解释。其次，我们提出了概率估计和加权这些规则的训练程序。我们在实例上对比了不同的方法，表明我们基于多重推导的估计有利于在语言学上更有动机的短语重新排序，并确定我们的较大规则比最小规则增加了3.63个BLEU点。我们使用xRS形式主义允许使用具有多级目标树注释和不连续源语言短语的翻译规则。我们的规则组合方法由两个或多个具有共享状态的最小GHKM或SPMT规则组成，以形成更大的规则。"}
{"pid": "J00-3004", "zh_sum": "一种基于压缩的中文分词算法中文是在不使用空格或其他分词符的情况下编写的。尽管文本可能被视为一个对应的单词序列，但在边界的位置上存在着相当多的歧义。将文本解释为单词序列有助于执行某些信息检索和存储任务：例如，全文搜索、基于单词的压缩和关键短语提取。我们描述了一种使用文本压缩标准的自适应语言模型推断单词边界适当位置的方案。它在预分段文本的语料库上进行训练，当应用于新文本时，会插入单词边界，以最大限度地提高压缩效果。这种简单而通用的方法对于专门的中文分词方案表现良好。我们基于n-gram生成语言建模的方法不使用领域知识。"}
{"pid": "P03-1069", "zh_sum": "概率文本结构：句子排序信息的实验是自然语言生成应用程序的一项关键任务。在本文中，我们提出了一种特别适合于文本到文本生成的信息排序方法。我们描述了一个从特定领域文本语料库中学习句子顺序约束的模型和一个在多个备选方案中产生最可能顺序的算法。我们根据语料库中的作者文本和被要求模拟模型任务的人类受试者来评估自动生成的排序。我们还评估了这种模型在多文档摘要中的适用性。我们构建了一个跨相邻句子的词的条件模型，重点关注具有特定语义角色的词。我们提出了一种计算两个句子相邻概率的算法，用于排序句子。作为特征，我们提出了相邻句子中内容词的笛卡尔积。"}
{"pid": "P04-1075", "zh_sum": "基于多准则的主动学习用于命名实体识别本文提出了一种基于多准则的主动学习方法，并将其有效地应用于命名实体识别。主动学习的目标是通过选择标记示例来最小化人类注释工作。为了最大限度地发挥所选示例的作用，我们考虑了多个标准：信息性、代表性和多样性，并提出了量化这些标准的措施。更全面地说，我们使用两种选择策略合并所有标准，这两种策略都比基于单一标准的方法产生更少的标签成本。MUC-6和GENIA中的命名实体识别结果表明，在不降低性能的情况下，标记成本至少可以降低80%。我们考虑了基于支持向量机的实体识别的主动学习（AL）。我们的基于多样性的层内采样方案考虑了K个不同的邻居，目的是最大化一个层中所有种子的训练效用。"}
{"pid": "P11-1098", "zh_sum": "不使用模板的基于模板的信息提取基于模板的信息提取（IE）的标准算法需要预定义的模板模式和经常标记的数据，以学习提取其插槽填充符（例如，大使馆是轰炸模板的目标）。本文描述了一种基于模板的IE方法，该方法消除了这一需求，并在不预先知道模板结构的情况下执行抽取。相反，我们的算法从原始文本自动学习模板结构，将模板模式归纳为与语义角色相关联的一组链接事件（例如，爆炸事件包括引爆、引爆和销毁事件）。我们还解决了标准IE任务，使用归纳的语法模式从特定文档中提取角色填充符。我们在MUC-4恐怖主义数据集上进行了评估，结果表明，我们诱导的模板结构与手工制作的黄金结构非常相似，并且我们提取了F1分数为的角色填充物。40，接近需要完全了解模板的算法的性能。我们从外部资源获取事件词，将事件词分组以形成事件场景，并将不同事件角色的提取模式分组。"}
{"pid": "P07-1065", "zh_sum": "统计机器翻译的随机语言建模布鲁姆过滤器（BF）是一种用于集合成员查询的随机数据结构。它的空间要求明显低于无损信息理论的下限，但它会产生一些可量化概率的误报。在这里，我们探讨了在统计机器翻译中使用BFs进行语言建模。我们展示了一个包含n-gram的BF如何使我们能够在SMT系统中使用更大的语料库和更高阶的模型来补充传统的n-gram LM。我们还考虑（i）如何在BF中有效地包含近似频率信息，以及（ii）如何通过首先检查候选n-gram中的低阶子序列来降低这些模型的错误率。我们在这两种情况下的解决方案都保留了BF的单侧错误保证，同时利用类似Zipf的词频分布来减少空间需求。我们提出了一种将静态频率信息与BF中的一组n-gram有效关联的方案。"}
{"pid": "W05-1203", "zh_sum": "本文提出了一种基于知识的文本语义相似度度量方法。虽然之前有大量的工作专注于发现概念和单词的语义相似性，但这些面向单词的方法在文本相似性中的应用尚未探索。在本文中，我们介绍了一种将词到词的相似性度量结合到文本到文本度量的方法，并且我们证明了该方法优于传统的基于词汇匹配的文本相似性度量。我们结合现有的六种基于知识的方法，提出了一种混合方法。"}
{"pid": "P98-1013", "zh_sum": "Berkeley FrameNet项目FrameNet是一个为期三年的NSF支持的基于语料库的计算词典编纂项目，现已进入第二年（NSF IRI-9618838，“词典构建工具”）。该项目的主要特点是（a）致力于语义和句法概括的语料库证据，以及（b）目标词（主要是名词、形容词和动词）的配价表示，其中语义部分利用框架语义。生成的数据库将包含（a）描述所描述单词含义的语义框架，以及（b）几千个单词和短语的配价表示（语义和句法），每个词和短语都附有（c）一组有代表性的带注释的语料库证明，它们共同举例说明了观察到的“框架元素”与其句法实现之间的联系（例如语法功能、短语类型和其他句法特征）。本报告将介绍项目的目标和工作流程，以及有关为这项工作而在内部调整或创建的计算工具的信息。我们介绍了FrameNet项目，在该项目中，我们为英语核心词汇开发了一个框架语义词汇。"}
{"pid": "D08-1027", "zh_sum": "便宜又快€“但它好吗？评估自然语言任务的非专家注释人类语言注释对于许多自然语言处理任务至关重要，但可能会很昂贵和耗时。我们探索使用亚马逊的Mechanical Turk系统，这是一种更便宜、更快的方法，可以通过Web从大量付费非专家贡献者那里收集注释。我们在调查五项任务：情感识别、词语相似度、文本蕴涵识别、事件时间顺序和词义消歧。对于所有这五种，我们显示了Mechanical Turk非专家注释和专家标签商提供的现有金标准标签之间的高度一致性。对于情感识别任务，我们还表明，使用非专家标签来训练机器学习算法可以与使用专家的金标准注释一样有效。我们提出了一种偏差校正技术，可以在两个任务上显著提高注释质量。我们的结论是，许多大型标记任务可以通过这种方法有效地设计和执行，所需费用仅为通常费用的一小部分。我们比较了非专家土耳其人制作的标签和专家为各种NLP任务制作的标签的质量，发现他们每个项目只需要四个响应来模拟专家注释。我们表明，获得多个低质量标签（通过Mechanical Turk）可以接近高质量的编辑标签。我们使用多数规则，即随机均匀打破联系，并报告观察到多数规则与金本位89.7之间的一致性（准确性）。"}
{"pid": "H94-1048", "zh_sum": "介词短语连接的最大熵模型我们构建了一个从《华尔街日报》语料库中提取的27937个pp连接四元组的基准数据集。我们在从《华尔街日报》语料库中提取的（v，n1，p，n2）四倍体上训练了一个最大熵模型，准确率达到81.6%。我们的最大熵方法使用互信息聚类算法。"}
{"pid": "P07-1091", "zh_sum": "统计机器翻译中基于语法的概率重排序方法受以往SMT预处理方法的启发，本文提出了一种新的概率重排序方法，该方法结合了基于语法和基于短语的SMT的优点。给定一个源语句及其解析树，我们的方法通过树操作生成一个n-最佳的重新排序输入列表，然后将其馈送到标准的基于短语的解码器以产生最佳翻译。实验表明，对于NIST MT-05的汉英翻译任务，该方案使BLEU提高了1.56%。我们使用最大熵系统来学习二叉树的重新排序规则（即，是否为每个节点保持或重新排序）。我们使用具有表面和句法特征的最大熵模型对解析树节点进行重新排序，以实现汉英翻译。"}
{"pid": "E03-1009", "zh_sum": "将分布信息和形态学信息相结合用于词性归纳本文讨论了基于分布信息和形态学信息，使用无监督算法将未标记文本中的单词分类的算法。我们展示了如何使用形态学信息来提高稀有词的性能，并且这在各种语言中都是可靠的。我们提出了一种基于困惑度的词性归纳算法质量测试方法。我们发现多对一精度有几个缺陷。"}
{"pid": "W00-0403", "zh_sum": "基于质心的多文档摘要：基于句子提取实用程序的评估和用户研究我们提出了一种多文档摘要生成器MEAD，它使用主题检测和跟踪系统生成的聚类质心生成摘要。我们还描述了两种基于句子效用和包含的新技术，并将其应用于单个和多个文档摘要的评估。最后，我们描述了两个测试我们的多文档摘要模型的用户研究。我们的基于质心的提取摘要器根据句子级别和句子间特征对句子进行评分，这表明句子作为摘要句子的质量。"}
{"pid": "W06-1670", "zh_sum": "本文将词义消歧和信息提取作为一个统一的标注问题进行研究。该任务包括使用41个Wordnet supersense类为名词和动词定义的标记集对文本进行注释。由于标记集与Wordnet语法集直接相关，因此标记器返回部分词义消歧。此外，由于名词标记包括标准的命名实体检测类–人员、位置、组织、时间等–标记器作为副产品返回扩展的命名实体信息。我们将超感知标记问题视为一个连续的标记任务，并使用经过区分训练的隐马尔可夫模型对其进行了实证研究。对可用的主要感官注释数据集（即Semcor和Senseval）的实验评估表明，与最著名的“第一感觉”基线相比，有了相当大的改进。我们的supersense标记器使用一组46个标签的WNSS类别标记对文本进行注释。"}
{"pid": "C88-1016", "zh_sum": "语言翻译的统计方法概述了一种利用从大型数据库中提取统计信息的自动翻译方法。该方法基于一对相互翻译的大型对应文本的可用性。在我们的例子中，文本是英文和法文的。这项技术的基础是一个复杂的固定语汇对应词汇表。建议的翻译过程的步骤是：（1）将源文本划分为一组固定的词组。（2） 使用词汇表和上下文信息，将相应的固定词组选择到形成目标句子的序列中。（3） 将目标固定词的单词按顺序排列，形成目标句。我们开发了统计技术，促进了词汇表的自动创建和三个翻译步骤的执行，所有这些都是基于两个文本中相应句子的对齐。虽然我们还不能提供法语/英语翻译的例子，但我们在词汇表创建和目标词序列安排方面给出了一些令人鼓舞的中间结果。我们开发了所谓的IBM模型，实现了一组基本操作，如移动、复制和翻译，这些操作独立作用于源语句中的单个单词。通常，统计机器翻译系统由三部分组成：语言模型、翻译模型和解码器。语言模型说明给定句子在源语言中的可能性，翻译模型说明特定目标句子是给定源句子翻译的可能性，而解码器则实际将源句子作为输入，并将其翻译作为输出。"}
{"pid": "P07-1096", "zh_sum": "双向序列分类的引导学习本文提出了一种新的双向序列分类学习框架——引导学习。将学习推理顺序和训练局部分类器的任务动态地合并到一个类似感知器的学习算法中。我们将这种新的学习算法应用于词性标注。它在标准PTB测试集上获得了2.67%的错误率，与相同数据集上以前的最佳结果相比，相对错误减少了3.3%，同时使用了较少的特征。我们的模型在标记准确率方面与CRF有竞争力，但需要的训练时间要少得多。我们开发了基于easiet-first策略和感知器算法的新算法。"}
{"pid": "N06-1041", "zh_sum": "序列模型的原型驱动学习我们研究了主要用于无监督序列建模的原型驱动学习。通过提供每个目标注释标签的一些规范示例，以声明方式指定先验知识。然后，利用对数线性生成模型中的分布相似性特征在语料库中传播稀疏的原型信息。在英语和汉语的词性归纳以及信息提取任务中，原型特征提供了比竞争基线大幅降低的错误率，并且优于以前的工作。例如，仅使用每个标记的三个示例，并且不受词典约束，我们就可以实现80.5%的英语词性标记准确率。我们还比较了半监督学习，并讨论了系统的误差趋势。原型驱动学习（PDL）优化了使用每个标签的原型输入特征标记的数据的联合边际可能性。我们要求用户为每个类推荐几个原型（示例），并将其用作功能。"}
{"pid": "D07-1007", "zh_sum": "利用词义消歧改进统计机器翻译我们首次表明，将词义消歧系统的预测纳入典型的基于短语的统计机器翻译（SMT）模型，可以持续提高所有三个不同的IWSLT汉英测试集的翻译质量，不仅根据BLEU，而且根据所有八个最常用的自动评估指标，在更大的NIST中英文机器翻译任务上取得了统计上的显著改进，而且在任何测试集上都不会影响性能。最近的工作挑战了词义消歧（WSD）系统对SMT有用的假设。然而，SMT的翻译质量仍然明显受到词汇选择不准确的影响。在本文中，我们通过研究一种将WSD集成到SMT系统中的新策略来解决这个问题，该策略可以实现短语多词消歧。我们没有直接合并Senseval风格的WSD系统，而是重新定义WSD任务，以匹配基于短语的SMT系统所面临的完全相同的短语翻译消歧任务。我们的结果提供了第一个已知的实证证据，证明词汇语义确实对SMT有用，尽管有相反的说法。我们提供了一个机器翻译系统，该系统将短语翻译的WSD概率作为对数线性模型中的额外特征。我们使用基于位置、语法和局部搭配的丰富上下文特征来动态调整每个句子的词汇，并方便选择更长的短语。我们使用最先进的WSD引擎（结合朴素贝叶斯、最大熵、boosting和核PCA模型）动态确定所考虑短语对的分数，从而使短语选择适应句子的上下文。"}
{"pid": "P03-1058", "zh_sum": "利用平行文本进行词义消歧：一项实证研究词义消歧（WSD）的一个中心问题是缺乏监督学习所需的手动词义标记数据。在本文中，我们评估了一种从英汉平行语料库中自动获取语义标记训练数据的方法，然后将其用于SENSEVAL-2英语词汇样例任务中的名词消歧。我们的研究表明，这种获取感知标记数据的方法很有前景。在最困难的SENSEVAL-2名词子集上，两种方法之间的准确度差异仅为14.0%，如果我们忽视手动语义标记数据在语义覆盖方面的优势，差异可能进一步缩小到6.5%。我们的分析还强调了域依赖性问题在评估WSD项目中的重要性。我们通过手动注释WordNet词义及其在目标语言（中文）中的翻译来解决词义消歧问题，然后将IBM模型应用于双语语料库，自动提取标记示例进行词义消歧。当一个英语单词的多个词义被同一个汉语单词翻译时，我们可以对这些词义进行折叠，以获得更粗粒度、集中的词义清单。"}
{"pid": "P07-2045", "zh_sum": "Moses：统计机器翻译开源工具包我们描述了一个统计机器翻译开源工具包，它的新贡献是：（a）支持语言因素，（b）混淆网络解码，以及（c）翻译模型和语言模型的高效数据格式。除了SMT解码器之外，该工具包还包括各种各样的工具，用于培训、调整和将系统应用于许多翻译任务。我们的Moses解码器实现了基于分解短语的翻译模型。"}
{"pid": "D08-1020", "zh_sum": "重温可读性：预测文本质量的统一框架我们结合词汇、句法和语篇特征，生成人类读者对文本可读性判断的高度预测模型。这是第一个考虑到各种语言因素的研究，也是第一个实证证明话语关系与文本感知质量密切相关的研究。我们发现，在我们的《华尔街日报》语料库中，通常预期与可读性相关的各种表面度量并不能很好地预测可读性判断。我们还发现，可读性预测因子的行为因任务而异：预测文本可读性或对可读性进行排名。我们的实验表明，话语关系是在这两个任务中表现出稳健性的一类特征。我们提出了一个由词汇、句法、词汇衔接要素、实体连贯和语篇关系组成的统一框架来衡量语篇质量。当可读性针对成人有能力的语言使用者时，话语特征起着更为突出的作用。五位注释员对每篇文章的整体文本质量进行了1到5分的评估。我们发现每个句子的平均单词数和每个单词的平均字符数没有显著的相关性。"}
{"pid": "J96-1002", "zh_sum": "自然语言处理的最大熵方法最大熵的概念可以追溯到圣经时代。然而，直到最近，计算机才变得足够强大，允许将这一概念广泛应用于统计估计和模式识别中的实际问题。本文描述了一种基于最大熵的统计建模方法。我们提出了一种自动构造最大熵模型的最大似然方法，并以自然语言处理中的几个问题为例，描述了如何有效地实现这种方法。我们提出了一种增益信息选择方法。"}
{"pid": "W95-0115", "zh_sum": "自动评估和统一过滤级联法提取N个最佳翻译词汇本文介绍了如何利用双语文本语料库的统计特性和四个外部知识源从双语文本语料库中提取N个最佳翻译词汇。知识源被转换为过滤器，以便它们的任何子集都可以级联到统一的框架中。采用一种新的客观评价方法来比较不同滤波器级联产生的词典的质量。与普通的统计方法相比，最佳滤波器级联可将词典质量提高137%，接近人的表现。当使用这些知识源时，大幅减少训练语料库的大小对词典质量的影响要小得多。这使得在无法获得大型双语语料库的情况下，可以为语言对使用小型手工构建语料库进行培训。此外，四个过滤器中的三个甚至在用于大型培训语料库时也很有用。我们使用最长公共子序列比（LCSR）来度量相似度。"}
{"pid": "C96-1079", "zh_sum": "信息理解会议-6：简史我们最近完成了一系列“信息理解会议”中的第六次会议，旨在促进和评估信息提取方面的研究。MUC-6比之前的MUC引入了多项创新，尤其是在进行评估的不同任务范围内。我们描述了新格式的一些动机，并简要讨论了一些评估结果。我们证明了基于语法的IE系统在许多场景中都是有效的。我们介绍了名称识别和分类任务。"}
{"pid": "P07-1036", "zh_sum": "用约束驱动学习指导半监督近年来，自然语言处理机器学习的两个主要研究方向是研究半监督学习算法，作为在标记数据不足时训练分类器的方法，以及研究如何在结构化学习任务中利用知识和全局信息。本文提出了一种在半监督学习算法中加入领域知识的方法。我们的新框架统一并可以利用多种特定于任务的约束。信息提取领域的实验结果表明，应用约束有助于模型在学习过程中产生更好的反馈，因此该框架允许在这些任务上使用比以前少得多的训练数据进行高性能学习。我们引入了约束驱动学习（CoDL）。我们在多个级别上使用约束，例如句子级别的约束来指定字段边界，并使用全局约束来确保关系级别的一致性。"}
{"pid": "J02-1002", "zh_sum": "对文本分割评估指标的批判和改进Pk评估指标最初由Beeferman、Berger和Lafferty（1997）提出，现在正成为评估文本分割算法的标准指标。然而，对该指标的理论分析发现了几个问题：该指标对误报的惩罚比误报更重，对未遂事件的惩罚过大，并且受片段大小分布变化的影响。我们建议对Pk度量进行简单修改，以解决这些问题。这个称为WindowDiff的新度量标准在文本中移动一个固定大小的窗口，并在窗口中的边界数与该文本窗口的真实边界数不匹配时惩罚算法。作为分割质量的衡量标准，我们开发了WindowDiff，它只评估分割边界，而不评估分配给它们的标签。"}
{"pid": "J98-3005", "zh_sum": "从多个在线来源生成自然语言摘要我们提出了一种方法，以简报的形式总结有关当前事件的新闻，包括适当的背景（历史）信息。我们开发的系统，召唤，使用为DARPA信息理解会议开发的系统的输出，生成关于相同或相关事件的多个文件的摘要，呈现信息源之间的相似性和差异、矛盾和概括。我们描述了系统的各个组成部分，展示了如何将多篇文章中的信息组合起来，组织成一个段落，最后实现为英语句子。我们工作的一个特点是提取实体（如人员和地点）的描述以供重用，以增强简报。我们将信息提取和自然语言处理相结合。"}
{"pid": "P07-1031", "zh_sum": "将名词短语结构添加到Penn Treebank中Penn Treebank不在基本名词短语（NPs）中进行注释，只致力于忽略英语NPs复杂性的at结构。这意味着，接受树库数据培训的工具无法了解NPs的正确内部结构。本文详细介绍了在Penn Treebank的每个名词短语中添加金标准括号的过程。然后，我们检查注释的一致性和可靠性。最后，我们使用这些资源使用几种统计方法确定NP结构，从而证明语料库的实用性。这为许多NLP应用程序所必需的Penn树库添加了详细信息。我们的注释方案插入NML和JJP括号来描述正确的NP结构。我们在注释过程中使用NE标记，因为我们发现基于NER的特征在统计模型中很有用。"}
{"pid": "J86-3001", "zh_sum": "注意意图与语篇结构本文探讨了一种新的语篇结构理论，强调目的和加工在语篇中的作用。在这一理论中，话语结构由三个独立但相互关联的部分组成：话语序列结构（称为语言结构）、目的结构（称为意向结构）和注意力集中状态（称为注意状态）。语言结构由话语自然聚合成的话语片段组成。意向结构抓住了话语相关的目的，表达在每一个语言片段中，以及它们之间的关系。注意状态是话语展开时参与者注意力焦点的抽象。注意力状态是动态的，它记录了话语中每一点上突出的对象、属性和关系。这些成分之间的区别对于充分解释线索短语、指称表达和中断等话语现象至关重要。本文以大量实例论述了注意、意图和话语聚合理论。本文描述了语篇的各种性质，并对线索短语、指称表达和中断的行为进行了解释。这一理论为描述语篇中话语的处理提供了一个框架。语篇处理需要认识到语篇中的话语如何聚合成片段，认识到语篇中表达的意图以及意图之间的关系，并通过与注意状态相关的机制来跟踪语篇。这种处理描述在这些识别任务中指定了来自话语和参与者领域知识的信息的作用。我们提出了一种话语结构理论来解释为什么要说一句话以及它的意思。"}
{"pid": "D09-1101", "zh_sum": "共指消解的监督模型传统的基于学习的共指消解器通过训练提及对分类器来确定两个提及是否相互关联。最近有两条独立的研究路线试图改进这些提及对分类器，一条是通过学习提及排序模型来对给定回指的前面提及进行排序，另一条是通过训练实体提及分类器来确定前面的簇是否与给定提及相关。我们提出了一种聚类排序方法来解决共指问题，该方法结合了提及者和实体提及模型的优点。此外，我们还展示了我们的聚类排序框架如何自然地允许话语新实体检测与共指消解联合学习。ACE数据集上的实验结果表明，其性能优于其他方法。在每个查询中，我们包含一个空的集群实例，以允许联合学习话语新检测。我们表明，CR模型比MP模型更强。我们的集群排名模型以从左到右的方式进行，并将当前的话语旧提及添加到得分最高的前一个集群中。"}
{"pid": "J81-4003", "zh_sum": "外位语法外位语法是定分句语法的扩展，在逻辑分句中也有类似的定义。扩展的形式主义使得描述成分的左外位置变得很容易，这是自然语言句法的一个重要特征。虽然中心语法提供了动词前置和跨序列依赖的描述，但我们介绍了外位语法，重点是英语中名词短语的移位。"}
{"pid": "D11-1062", "zh_sum": "分而治之：众包跨语言文本蕴涵语料库的创建我们通过众包的方式来创建跨语言文本蕴涵语料库。我们的目标是定义一种廉价且可复制的数据收集方法，该方法可以最大限度地减少专家注释员的手动工作，而无需借助预处理工具或已注释的单语数据集。与最近强调大规模注释工作对文本蕴涵的需要的工作一致，我们的工作旨在：i）解决可用于培训和评估系统的数据不足问题，以及ii）促进求助于众包，作为一种有效的方式，在不牺牲质量的情况下降低数据收集成本。我们表明，一个复杂的数据创建任务，即使是专家通常也具有较低的一致性分数，可以有效地分解为分配给非专家注释器的简单子任务。由此产生的数据集是从一条通往亚马逊机械土耳其人（Amazon Mechanical Turk）的不同工作管道中获得的，包含1600多对英语、意大利语和德语文本假设的组合。"}
{"pid": "P04-1021", "zh_sum": "机器音译的联合源-通道模型大多数外国名字被音译成汉语、日语或韩语，具有近似的语音等价物。音译通常通过中间音位映射来实现。本文提出了一种新的框架，通过一个联合源-通道模型，即n-gram音译模型（TM），允许两种不同语言之间的直接正交映射（DOM）。利用n-gram-TM模型，我们自动化了正交对齐过程，从双语词典中获得对齐的音译单元。DOM框架下的n-gram TM大大减少了系统开发工作量，并在音译精度方面比其他最先进的机器学习算法有了巨大的飞跃。通过对英汉语言对的实验验证了该建模框架的有效性。我们发现，没有汉语音素的英汉音译比有汉语音素的英汉音译效果更好。我们的基于字形的方法将音译视为单调约束下的统计机器翻译问题，旨在获得直接正交映射（DOM），以减少在多次转换中可能引入的错误。基于音位的方法通常不够好，因为名称实体有不同的词源，而且音译并不总是由发音决定。许多音译词是专有名称，其发音规则可能因来源语言而异。我们的直接正字法映射利用单个汉字字形，可以克服这个问题，并直接为字符选择建模。"}
{"pid": "W96-0102", "zh_sum": "MBT：基于内存的词性标记生成器我们介绍了一种基于内存的词性标记方法。基于记忆的学习是一种基于相似性推理的监督学习形式。单词在特定上下文中的词性标记是根据记忆中最相似的情况推断出来的。当标记语料库可以作为标记者所需输出的示例时，监督学习方法非常有用。基于这样的语料库，tagger生成器自动构建一个tagger，该tagger能够以相同的方式标记新文本，大大缩短了构建tagger的开发时间。基于记忆的标记与其他统计或机器学习方法具有相同的优势。基于记忆的方法的其他优势包括（i）相对较小的标记语料库大小，足以进行训练，（ii）增量学习，（iii）解释能力，（iv）案例表示中信息的灵活集成，（v）其非参数性质，（vi）在不进行形态学分析的情况下，对未知单词取得相当好的结果，（vii）快速学习和标记。在本文中，我们证明了基于记忆的方法的大规模应用是可行的：当使用IGTree（一种用于索引和搜索大型案例库的基于树的形式主义）时，我们获得了与已知统计方法相当的标记精度，并且具有诱人的空间和时间复杂性特性。IGTree的使用还有一个额外的优点，即动态计算消歧的最佳上下文大小。我们的标记器使用非常细粒度的标记集。"}
{"pid": "P11-1038", "zh_sum": "短信的词法规范化：Makn Sens a#推特推特推特可实时访问大量数据，但噪音大，妨碍了其在NLP中的应用。本文针对短消息中的词汇表外单词，提出了一种识别和规范格式错误单词的方法。我们的方法使用分类器来检测格式错误的单词，并基于形态音素相似性生成校正候选词。然后利用词语相似度和上下文来为该词语选择最可能的修正候选词。该方法不需要任何注释，在SMS语料库和基于Twitter的新数据集上实现了最先进的性能。我们使用分类器来检测格式错误的单词，然后根据形态音素相似性生成修正候选词。"}
{"pid": "J82-3004", "zh_sum": "处理句法歧义或如何把方块放在桌子上的句子比人们想象的要模糊得多。对于某些非常自然的英语句子，可能有成百上千的句法分析树。这一事实一直是自然语言处理面临的一个主要问题，尤其是当语义/语用处理过程中大量的语法分析树被枚举时。在本文中，我们提出了一些处理句法歧义的方法，这些方法利用了可选解析树之间的某些规律。这些规律将表示为ATN网络的线性组合，以及形式幂级数的和和和积。我们相信，这种对歧义的编码将增强处理能力，无论是按顺序单独处理句法和语义约束，还是交织在一起。一个句子可能的二元分支解析数由加泰罗尼亚数定义，加泰罗尼亚数是一个指数组合函数。"}
{"pid": "N09-1003", "zh_sum": "基于分布式和基于WordNet的相似性和相关性研究本文介绍并比较了基于WordNet和基于分布式的相似性方法。讨论了每种方法在相似性和相关性任务方面的优缺点，并提出了一种组合。我们的每一种方法在RG和WordSim353数据集上都独立地提供了同类中最好的结果，并且它们的监督组合在所有数据集上产生了最好的发布结果。最后，我们开创了跨语言相似性的先河，表明我们的方法很容易适应跨语言任务，损失很小。我们使用PageRank推导了一个基于WordNet的度量，并使用支持向量机将其与多个基于语料库的向量空间模型相结合。通过检查每对单词之间的关系，我们进一步将该数据集分为相似对（WS-sim）和相关对（WS-rel），其中前者包含同义词、反义词、相同单词和上下义词/上义词，而后者捕获其他单词关系。"}
{"pid": "P08-2007", "zh_sum": "短语对齐问题的复杂性许多短语对齐模型都是在双射短语对齐的组合空间上运行的。我们证明了在这个空间中寻找最优对齐是NP难的，而计算对齐期望是#P难的。另一方面，我们证明了寻找最佳对齐的问题可以转化为一个整数线性规划，这为短语对齐模型的维特比推理提供了一种简单的声明性方法，在经验上相当有效。"}
{"pid": "P07-1007", "zh_sum": "基于主动学习的词义消歧领域自适应当词义消歧（WSD）系统在一个领域上训练，但应用到另一个领域时，准确率经常下降。这突出了领域适应对词义消歧的重要性。在本文中，我们首先证明了一种主动学习方法可以成功地用于WSD系统的域自适应。然后，通过使用期望最大化（EM）预测的优势感知并采用计数合并技术，我们改进了基本主动学习方法实现的原始自适应过程的有效性。我们对DSO语料库中手动选择的21个名词子集进行监督域自适应。我们显著地表明，即使在使用主动学习进行自适应后，检测领域感知先验建模的主要感知变化也可以改善感知消歧。"}
{"pid": "D07-1096", "zh_sum": "CoNLL 2007年关于依赖解析的共享任务计算自然语言学习会议有一个共享任务，参与者在相同的数据集上训练和测试他们的学习系统。与2006年一样，2007年的共享任务一直致力于依赖关系解析，今年有多语言跟踪和域自适应跟踪。在本文中，我们定义了不同轨迹的任务，并描述了如何从十种语言的现有树库创建数据集。此外，我们还描述了参与系统的不同方法，报告了测试结果，并对这些结果进行了初步分析。我们注意到，具有自由词序和高形态复杂性的语言是最难进行依赖性分析的语言。形态丰富的语言带来了新的挑战，因为在巴斯克语、希腊语或土耳其语等语言中，将最先进的解析器用于更具配置性和非屈折性的语言（如英语）并没有达到类似的性能水平。"}
{"pid": "J01-2004", "zh_sum": "概率自顶向下分析和语言建模本文介绍了一种覆盖范围广泛的概率自顶向下分析器的功能及其在语音识别语言建模问题中的应用。本文首先介绍了语言建模和概率句法分析中的关键概念，并简要回顾了以前使用句法结构进行语言建模的一些方法。然后提出了一种词典化的概率自顶向下解析器，相对于最好的广泛覆盖统计解析器，该解析器在返回解析的准确性和发现解析的效率方面都表现得非常好。在此基础上，提出了一种新的基于概率自顶向下句法分析的语言模型，实验结果表明，该模型改进了以往在测试语料库复杂度方面的工作。与其他模型观察到的改善相比，使用三元模型的插值产生了异常的改善，这表明我们的解析模型捕获的信息与三元模型捕获的信息正交的程度。一个小的识别实验也证明了该模型的实用性。我们的解析器从左到右遍历整个句子，并放弃动态编程，支持波束搜索，通过添加下一个单词，然后重新修剪，保留了大量扩展的可能性。对于字符串中的每个单词，我们的自顶向下解析器提供了对梁中部分分析的加权集的访问。"}
{"pid": "W04-3212", "zh_sum": "校准语义角色标记的特征本文对语义角色标记文献中使用的特征进行了批判性研究，并表明输入中的信息（通常是语法分析树）尚未得到充分利用。我们提出了一组额外的特性，我们的实验表明，这些特性可以显著改善我们执行的任务。我们进一步表明，不同的子任务需要不同的特性。最后，我们表明，通过使用最大熵分类器和较少的特征，我们获得的结果与之前报道的使用SVM模型获得的最佳结果相当。我们认为，这清楚地表明，开发能够捕获正确类型信息的功能对于提高语义分析的技术水平至关重要。"}
{"pid": "W01-0514", "zh_sum": "文本分割的潜在语义分析本文描述了一种线性文本分割方法，该方法比最先进的方法更精确或至少同样精确（Utiyama和Isahara，2001；Choi，2000a）。通过潜在语义分析（LSA）估计句间相似度。通过分裂聚类发现边界位置。测试结果表明，LSA是一种更准确的相似度度量。我们使用所有词汇计算低维文档向量。"}
{"pid": "D10-1119", "zh_sum": "将概率CCG语法从逻辑形式归纳为高阶统一形式本文讨论了在给定由自然语言句子及其意义的逻辑表示组成的训练数据的情况下，学习将句子映射到逻辑形式的问题。以前的方法是针对特定的自然语言或特定的意义表示而设计的；在这里，我们提出了一种更通用的方法。该方法归纳出一种概率CCG语法，该语法表示单个单词的含义，并定义如何将这些含义组合起来分析完整的句子。我们使用高阶统一定义了一个包含所有与训练数据一致的语法的假设空间，并开发了一种在线学习算法，该算法可以有效地搜索该空间，同时估计对数线性解析模型的参数。实验表明，在四种语言的具有两种不同意义表示的基准数据集上具有较高的准确性。我们提出了一种独立于语言学习的方法，用基于高阶统一的词汇归纳法代替手工指定的模板。我们在他们的学习算法中使用跨单词和意义元素的语料库范围对齐统计信息来初始化词汇权重。"}
{"pid": "W06-1615", "zh_sum": "基于结构对应学习的领域自适应判别学习方法在自然语言处理中有着广泛的应用。当这些方法的训练和测试数据来自同一分布时，它们的效果最好。然而，对于许多NLP任务，我们面临着标记数据稀少或不存在的新领域。在这种情况下，我们试图将现有模型从资源丰富的源域调整为资源贫乏的目标域。我们引入结构对应学习来自动归纳来自不同领域的特征之间的对应关系。我们在词性标记上测试了我们的技术，并显示了在不同数量的源和目标训练数据下的性能提高，以及使用改进的标记器在目标域解析精度方面的改进。接下来（Blitzer et al，2006），我们介绍了结构对应学习在非投射依赖解析中的应用（McDonald et al，2005）。我们的方法是训练一个单独的域外解析器，并使用它在有监督和无监督的域内数据上生成额外的特征。我们介绍了SCL，这是一种对某些高维NLP问题有效的特征表示方法，包括词性标注和情感分类。我们将（Ando和Zhang，2005）的多任务算法应用于NLP中的域自适应问题。我们使用预测的数据轴（即出现在源域和目标域中的单词）附加标记源域的数据，以使词性标记符适应目标域。"}
{"pid": "P03-1002", "zh_sum": "使用谓词参数结构进行信息提取在本文中，我们提出了一种新的、可定制的IE范式，该范式利用谓词参数结构。我们还介绍了一种自动识别谓词参数结构的新方法，这是IE范式的核心。它基于：（1）一组扩展的特征；归纳决策树学习。实验结果证明了我们的观点，即准确的谓词参数结构可以获得高质量的IE结果。我们应用语义分析来捕获谓词参数句子结构。"}
{"pid": "D07-1091", "zh_sum": "分解翻译模型我们提出了一种基于短语的统计机器翻译模型的扩展，该模型支持在单词级别上直接集成其他注释-可能是语言标记或自动生成的单词类。在大量的实验中，我们表明，考虑因素的翻译模型在自动评分和语法连贯性方面都能带来更好的翻译性能。任何实施语言约束的方法都会减少对数据的需求，并最终在数据量相同的情况下生成更完整的模型。我们还提出了同时使用不同单词级表示的框架。我们提出了一种将形态-句法信息紧密集成到翻译模型中的方法，其中引理和形态信息被分别翻译，并且这些信息在输出端被组合起来生成翻译。我们将基于短语的模型对单词的表示从字符串推广到向量，允许在搜索过程中将词性和形态学等附加特征与表面形式相关联，甚至替换表面形式。分解翻译模型有助于采用更面向数据的方法进行协议建模。"}
{"pid": "D08-1089", "zh_sum": "一种简单有效的分层短语重排序模型基于短语的统计机器翻译系统目前提供了最先进的性能，但它们在词序变化方面仍然很弱。当前的短语重新排序模型可以正确处理相邻短语之间的交换，但它们通常缺乏执行基于语法的系统可能实现的长距离重新排序的能力。在本文中，我们提出了一种新的分层短语重排序模型，旨在改进非局部重排序，该模型与基于标准短语的系统无缝集成，且计算效率损失很小。我们表明，该模型可以成功地处理常用于激励基于语法的系统的关键示例，例如介词短语围绕名词短语的旋转。我们将我们的模型与基于短语的系统中常用的重新排序模型进行了对比，并表明我们的方法为两种语言对提供了统计上显著的BLEU点增益：汉语-英语（MT05为+0.53，MT08为+0.71）和阿拉伯语-英语（MT05为+0.55）。我们的分层方向模型通过移位减少算法捕获非局部短语重新排序。我们在解码中引入了一个确定性的移位减少解析器，这样，在当前翻译历史的情况下，解码器总是能够访问可能最大的前一个块。我们介绍了三种词汇化再排序的定向模型：基于词的定向模型、基于短语的定向模型和层次定向模型。"}
{"pid": "W04-0803", "zh_sum": "Senseval-3任务：语义角色的自动标记执行语义角色自动标记的Senseval-3任务旨在鼓励对FrameNet数据集的研究和使用。这项任务是基于自Gildea和Jurafsky对语义角色自动标记进行基线研究以来FrameNet数据的大量扩展。FrameNet数据提供了大量可用于词汇语义研究的“黄金标准”数据，为其在NLP应用中的进一步开发奠定了基础。八个团队参与了这项任务，总共进行了20次跑步。参与者在任务开发过程中的讨论及其跑步评分有助于任务的成功完成。参与者使用了多种技术，调查了FrameNet数据的许多方面。他们取得的结果表明，Gildea和Jurafsky的基线研究取得了相当大的进步。重要的是，他们的努力为使复杂的FrameNet数据集更易于访问做出了巨大贡献。他们充分证明了FrameNet是一种重要的词汇资源，它将允许在未来对NLP应用程序进行广泛的进一步研究和开发。我们在Senseval-3研讨会上进行评估。"}
{"pid": "W01-0513", "zh_sum": "多词单位词典词头的无知识归纳是一个已解决的问题吗？我们寻求一种从文本语料库中归纳多词单位的无知识方法，以用作机器可读的词典标题词。我们对现有的九个搭配发现者进行了两次主要评估，并说明了继续改进的必要性。我们使用潜在语义分析来获得适度的性能提升，但我们展示了在尝试这种方法时遇到的重大挑战。我们表明，WordNet对于MWE检测方法来说，与web一样，是一种有效的评估资源，尽管其固有的大小限制和静态性质。我们从两个方面比较了短语的语义向量及其组成词的语义向量：一种是在构建部分语义向量时包含短语的上下文，另一种是不包含短语的上下文。"}
{"pid": "W07-0734", "zh_sum": "METEOR：机器翻译评估的自动度量与人类判断高度相关METEOR是机器翻译评估的自动度量，已被证明与人类对翻译质量的判断高度相关，显著优于更常用的Bleu度量。这是ACL WMT-07研讨会今年共享任务中使用的几个自动指标之一。本文概述了该指标的技术细节，并描述了该指标的最新改进。最新版本包括改进的度量参数，并扩展了度量，以支持除英语外的西班牙语、法语和德语MT输出评估。在机器翻译评价环境中，语义聚类被集成到机器翻译评价指标（METEOR）中，并增加了该指标与人类对不同语言翻译质量判断的相关性。"}
{"pid": "W10-3001", "zh_sum": "CoNLL-2010共享任务：学习检测自然语言文本中的模糊限制语及其范围CoNLL-2010共享任务致力于检测自然语言文本中的不确定性线索及其语言范围。这项任务背后的动机是，区分文本中的事实信息和不确定信息在信息提取中至关重要。CoNLL-2010共享任务旨在检测文本中的不确定性线索，重点是这些短语，试图确定句子是否包含不确定性信息。CoNLL 2010共享任务的目标也是开发语言范围检测器。"}
{"pid": "N04-1033", "zh_sum": "基于短语的统计机器翻译的改进在统计机器翻译中，目前性能最好的系统以短语或词组为基础。我们描述了基于基线短语的翻译系统和各种改进。我们描述了一种高效的单调搜索算法，其复杂性在输入句子长度上呈线性。我们给出了三个任务的翻译结果：动词mobil、Xerox和加拿大汉莎词典。对于Xerox任务，翻译包含超过10K个单词的整个测试集只需不到7秒的时间。施乐和加拿大汉萨德任务的翻译结果非常有希望。该系统甚至优于对齐模板系统。在我们的方法中，平滑短语概率由词对概率构建，并与非平滑短语表结合在对数线性模型中。"}
{"pid": "N03-1026", "zh_sum": "基于词汇功能语法的歧义包装和随机消歧方法的统计句子压缩我们提出了词汇功能语法的歧义包装和随机消歧技术（LFG）在句子压缩领域的应用。我们的系统包含一个LFG语言解析器/生成器、一个在压缩解析林上操作的解析约简传递组件和一个用于随机输出选择的最大熵模型。此外，我们还提出了使用标准解析器评估方法来自动评估句子浓缩系统的摘要质量。摘要质量的实验评估表明，基于自动解析的评估与生成字符串的手动评估之间存在密切的相关性。由于使用了基于约束的解析器/生成器，拟议系统的总体摘要质量是最先进的，系统输出的语法性得到了保证。我们在LFG解析器的输出上提出了一种区分性句子压缩器，它是可能压缩的压缩表示。我们将语言丰富的LFG语法应用于句子压缩系统。"}
{"pid": "P05-1010", "zh_sum": "带潜在注释的概率CFG本文定义了一个生成的解析树概率模型，我们称之为PCFG-LA。该模型是PCFG的一个扩展，在该模型中，非终端符号增加了潜在变量。通过使用EM算法训练PCFG-LA模型，从解析的语料库中自动归纳出细粒度CFG规则。由于使用PCFG-LA进行精确解析是NP困难的，因此描述了几种近似方法并进行了经验比较。在使用宾夕法尼亚州华尔街日报语料库的实验中，我们的自动训练模型的性能为86.6%（F1，句子<=40个单词），这与使用大量手动特征选择创建的非Exilicalized PCFG解析器的性能相当。在解码过程中，我们使用标记化语法来获得更好的未注解析林，但我们不标记训练数据。我们对树库数据进行右二值化，以构造只有一元和二元乘积的语法。"}
{"pid": "E06-1051", "zh_sum": "利用浅层语言信息从生物医学文献中提取关系我们提出了一种仅基于浅层语言信息从生物医学文献中提取实体之间关系的方法。我们使用核函数的组合来集成两个不同的信息源：（i）关系出现的整个句子，以及（ii）交互实体周围的局部上下文。我们从两个不同的数据集中进行了提取基因和蛋白质相互作用的实验。结果表明，我们的方法优于以往基于句法和语义信息的方法。除了单词特征外，我们还提取词性标记、引理和标记的正交特征等浅层语言信息，用于PPI提取。"}
{"pid": "W97-0301", "zh_sum": "基于最大熵模型的线性观测时间统计解析器本文提出了一种自然语言统计解析器，其解析精度约为87%，召回率约为86%，超过了之前在Wall St.Journal领域发表的最佳结果。解析器本身只需要很少的人工干预，因为它用于做出解析决策的信息是以简洁、简单的方式指定的，并且在最大熵框架下以完全自动的方式进行组合。解析器在测试句子上观察到的运行时间与句子长度呈线性关系。此外，解析器会为一个句子返回多个得分较高的语法分析，本文表明，从20个得分最高的语法分析中选择最佳语法的方案可以显著提高准确率，达到93%的准确率和召回率。我们引入了甲骨文重新排序的思想：假设存在一个完美的重新排序方案，该方案可以神奇地在每个句子的前k个语法中选择F分数最高的最佳语法。我们采用管道方法，即分类器根据单个决策而不是解析器的整体质量进行训练，并链接以生成全局结构。"}
{"pid": "C88-2121", "zh_sum": "“词汇化”语法的句法分析策略：树邻接语法的应用本文中，我们提出了一种通用的句法分析策略，该策略源于标记的Earley类型句法分析算法的发展（Schabes和Joshi，1988）和最近的标记语言学研究（Abeille，1988）。在我们的方法中，基本结构与其词头相关联。这些结构指定了扩展的局部域（与上下文无关语法相比），可以对其声明约束。这些约束要么包含在基本结构本身中，要么指定可以使用给定的基本结构组成哪些其他结构。我们陈述了在不改变最初产生的语言结构的情况下，基于上下文无关的语法可以“词汇化”的条件。我们认为，即使将CFG的局部域扩展到树，仅使用替换也不能自由选择每个结构的头部。我们展示了附加语如何允许我们自由地“词汇化”CFG。然后，我们展示了“词汇化”语法是如何从标记位置的扩展域自然产生的，并展示了我们方法的一些语言学优势。讨论了一种新的“词汇化”语法的通用句法分析策略。在第一阶段，解析器构建与输入句子相对应的集合结构，在第二阶段，根据该集合解析句子。该策略独立于所采用的语言理论和潜在的语法形式主义。然而，我们将注意力集中在标签上。由于解析输入句子所需的树集应该是有限的，因此解析器原则上可以使用任何搜索策略。因此，尤其可以使用自顶向下的策略，因为递归结构导致的问题已被消除。解析器还能够使用非本地信息来指导搜索。然后，我们解释如何修改标签的Earley类型解析器以利用这种方法。词汇化语法提供了显著的语法分析优势，因为产出的应用程序数量（即派生步骤）明显受输入字符串长度的限制。"}
{"pid": "H05-1043", "zh_sum": "从评论中提取产品特征和观点消费者通常被迫通过许多在线评论来做出明智的产品选择。本文介绍了OPINE，这是一个无监督的信息提取系统，它可以挖掘评论，以建立一个重要产品特征、评论人对其的评估以及跨产品的相对质量的模型。与之前的工作相比，OPINE在特征提取任务上的准确率提高了22%（召回率仅降低了3%）。OPINE新颖地使用松弛标记来发现上下文中单词的语义方向，这使得他在发现观点短语及其极性的任务上表现出色。我们基于词典的方法利用Wikipedia在查询中查找短语或单个术语的条目页面。我们不仅分析关于产品特性的意见的极性，而且还根据意见的强度对其进行排序。我们提出了一种利用语料库统计、WordNet关系和形态学线索识别产品特征的方法。利用逐点互信息进行相关性排序和提取。"}
{"pid": "P09-1027", "zh_sum": "中文情感语料库的缺乏限制了中文情感分类的研究进展。然而，网上有许多免费的英语情感语料库。本文主要研究跨语言情感分类问题，该问题利用现有的英语语料库作为训练数据对汉语情感进行分类。机器翻译服务用于消除训练集和测试集之间的语言差异，英语特征和汉语特征被视为分类问题的两个独立视图。我们提出了一种使用未标记中文数据的联合训练方法。实验结果表明了该方法的有效性，其性能优于标准归纳分类器和直推分类器。该协回归算法可以在统一的框架内充分利用源语言和目标语言的特征。我们建议使用集成方法在英文标注数据及其中文翻译上训练更好的中文情感分类模型。我们利用现有的英文语料库进行中文情感分类，采用联合训练的方法，在统一的框架内充分利用英文和中文的特征。"}
{"pid": "P05-2008", "zh_sum": "在情感分类的机器学习技术中，使用表情符号来减少依赖性情感分类旨在根据作者对主题的总体感觉来识别文本，无论是正面的还是负面的。传统的机器学习技术已经成功地应用于这个问题，但只有当主题的训练数据和测试数据之间存在良好的匹配时，它们才能很好地工作。本文证明了域和时间匹配也很重要，并用标记有表情符号的训练数据进行了初步实验，该数据具有独立于域、主题和时间的潜力。在情感分析研究中，我们使用新闻组文章中的表情符号来提取与训练极性分类器相关的实例。我们发现，当电子通信的作者使用情感时，他们实际上是在用情感状态标记自己的文本。"}
{"pid": "N07-1023", "zh_sum": "用于句子压缩的词汇化马尔可夫语法我们根据（Knight和Marcu，2000）的成功噪声信道方法，提出了一种基于同步上下文无关语法（SCFG）的句子压缩系统。我们定义了一个头部驱动的SCFG删除规则的马尔可夫公式，它允许我们对成分删除的概率进行词汇化。我们还使用了一种健壮的方法来实现任意文档抽象平行语料库之间的树到树对齐，这使得我们能够使用比以前完全依赖于几乎不可用的文档压缩语料库的方法更多的数据来训练词汇化模型。最后，我们评估了不同的马尔可夫模型，并发现我们选择的最佳模型是一个利用头修饰语双尺度化来准确区分附加语和补语的模型，并且该模型生成的句子比之前工作生成的句子更符合语法。"}
{"pid": "H05-1044", "zh_sum": "识别短语级情感分析中的语境极性本文提出了一种新的短语级情感分析方法，该方法首先确定一个表达式是中性的还是极性的，然后消除极性表达式的极性歧义。通过这种方法，系统能够自动识别大量情感表达的上下文极性，获得明显优于基线的结果。我们提出了监督学习，将资源分为先验极性和上下文极性。我们的实验表明，主观性分析的词汇查找方法在一般文本中的成功率有限。我们手动构建极性词汇，其中每个词条都带有主观性程度（强、弱）和情感极性（正、负、中性）的注释。我们的MPQA词典包含独立的主观性线索、强化词和配价转移词词典，用于识别意见根、修饰词和否定词。"}
{"pid": "P06-1038", "zh_sum": "使用对称模式和高频词的有效无监督词类发现我们提出了一种新的方法来发现词类，词组共享其意义的重要方面。我们利用高频词和内容词的元模式来发现模式候选。然后使用基于图的度量来识别对称模式，并基于图团集创建单词类别。我们的方法是第一种基于模式的方法，它不需要语料库注释或手动提供种子模式或单词。我们使用人类判断和基于WordNet的评估，在两种语言的大型语料库上评估我们的算法。我们的完全无监督结果优于之前使用POS标记语料库的工作，并且对于大型语料库的计算时间比之前报道的要快几个数量级。我们发现，经常以对称模式出现在一起的成对单词往往属于同一类（也就是说，它们在语义上有一些值得注意的方面）。"}
{"pid": "W00-0726", "zh_sum": "CoNLL-2000共享任务简介：分块我们提供数据集的背景信息，概述参与共享任务的系统，并简要讨论其性能。该数据集从WSJ Penn Tree bank中提取，包含211727个训练示例和47377个测试实例。"}
{"pid": "N03-1021", "zh_sum": "多文本语法和同步解析器多文本语法（MTG）通过任意长度的产生式规则生成任意多个并行文本。普通MTG及其双线性子类都允许相对高效的解析器。然而，MTG比文献中描述的解析器所使用的其他同步形式更具表现力。更大的表达能力和相对较低的推理成本使MTG成为翻译对等实用模型的一个有吸引力的基础。我们提出了具有更复杂语法的同步解析算法，讨论了如何解析具有大于二进制分支的语法以及同步语法的词汇化。我们讨论了所谓的钩子技巧在分析双出式多文本语法中的适用性。"}
{"pid": "P05-1017", "zh_sum": "利用旋转模型提取单词的语义方向我们提出了一种提取单词语义方向的方法：合意或不合意。将语义取向视为电子自旋，我们使用平均场近似来计算系统的近似概率函数，而不是难以处理的实际概率函数。我们还提出了基于磁化强度的参数选择准则。在英语词汇实验中，该方法只需少量种子词，就能准确地提取语义方向。结果与有史以来报告的最佳值相当。我们不仅从共现资源，而且从包括同义词库在内的其他资源构建词汇网络。我们根据自旋模型确定术语方向（日语），即一组电子的物理模型，每个电子在两个可能的自旋方向之间有一个电子，并且电子将其自旋方向传播到相邻电子，直到系统达到稳定配置。"}
{"pid": "W11-0705", "zh_sum": "推特数据的情绪分析我们检查推特数据的情绪分析。本文的主要贡献有：（1）介绍了词性的先验极性特征。（2） 我们探索如何使用树内核来避免繁琐的特征工程。新特性（与先前提出的特性一起）和树内核的性能大致相同，都优于最先进的基线。在我们的工作中，我们对人工标注的推文语料库进行了研究。"}
{"pid": "W07-0733", "zh_sum": "统计机器翻译领域适应性实验WMT 2007共享任务的特殊挑战是领域适应性。当大多数培训数据来自不同的领域（此处：欧洲议会演讲）时，我们借此机会尝试了将统计机器翻译系统调整到特定领域的各种方法（此处：新闻评论）。本文还描述了爱丁堡大学对共享任务的提交情况。分解翻译模型用于整合领域适应。我们使用两种语言模型和两种翻译模型：一种域内模型和另一种域外模型来适应系统。我们通过最小化调整（开发）集和插值模型的复杂性，分别学习用域内和域外数据训练的语言模型的混合权重。"}
{"pid": "W06-2915", "zh_sum": "你站在哪一边？在文档和句子层面识别视角在本文中，我们研究了一个新问题，即识别文档写作的视角。所谓观点，我们指的是一种观点，例如，从民主党人或共和党人的角度来看。计算机能学会识别文档的透视图吗？并不是每个句子都从一个角度写得很好。计算机能学会识别哪些句子强烈地表达了特定的观点吗？我们开发了统计模型，以捕捉在文件和句子层面上如何表达观点，并评估关于以巴冲突文章的拟议模型。结果表明，所提出的模型成功地学习了透视图在单词使用中的反映，并且能够高精度地识别文档的透视图。我们使用层次贝叶斯模型进行意见建模（Lin等人，2006年）。我们的实验是在政治辩论语料库中进行的（Lin et al 2006）。我们探讨了句子级和文档级分类之间的关系，以实现一个类似姿态的预测任务。我们将内隐情感引入计算语言学中的一个研究主题，即识别视角，尽管在政治学领域，类似的工作早就开始了。"}
{"pid": "P99-1071", "zh_sum": "多文档摘要中的信息融合我们提出了一种通过识别和合成多个文档中相关文本中的相似元素来自动生成简明摘要的方法。我们的方法是独特的，它使用语言生成来重新制定摘要的措辞。我们观察到，对于新闻文章的多文档摘要任务，提取可能是不合适的，因为它可能会产生过于冗长或偏向某些来源的摘要。"}
{"pid": "J00-2004", "zh_sum": "单词间的翻译对等模型平行文本（BITEXT）具有区别于其他类型平行数据的特性。首先，大多数单词只能翻译成另一个单词。第二，双文本通信通常只是部分的-每个文本中的许多单词在另一个文本中没有明确的对等词。本文介绍了对统计翻译模型进行偏移以反映这些特性的方法。关于独立人类判断的评估已经证实，以这种方式偏向的翻译模型比基线无知识模型要准确得多。本文还展示了统计翻译模型如何利用关于特定语言对的现有知识。即使是最简单的特定语言知识，例如内容词和虚词之间的区别，也可以可靠地提高翻译模型在某些任务中的性能。反映模型领域知识的统计模型结合了理性主义和经验主义范式的优点。我们使用最长公共子序列比（LCSR）来度量正交相似性。我们将直接关联定义为两个单词之间的关联，其中两个单词实际上是相互翻译的。我们提出了竞争链接算法（CLA）来对齐单词以构建混淆网络。我们使用竞争链接贪婪地构建匹配，其中配对分数是词与词之间关联的度量。我们认为，有一些方法可以确定一些多词短语的边界，允许将多个词视为单个标记。"}
{"pid": "N03-1017", "zh_sum": "基于统计短语的翻译我们提出了一种新的基于短语的翻译模型和解码算法，使我们能够评估和比较以前提出的几种基于短语的翻译模型。在我们的框架内，我们进行了大量的实验，以更好地理解和解释为什么基于短语的模型优于基于单词的模型。我们的实证结果适用于所有被检查的语言对，表明通过相对简单的方法可以获得最高水平的性能：从基于单词的对齐中启发式学习短语翻译和短语翻译的词汇权重。令人惊讶的是，学习超过三个单词的短语和从高精度单词级对齐模型学习短语对性能没有太大影响。只学习句法上有动机的短语会降低系统的性能。我们建议将STIR作为从英语到日语的最先进的基于短语的翻译系统中的一个预排序步骤。"}
{"pid": "J03-1005", "zh_sum": "本文描述了一种基于动态规划（DP）的高效统计机器翻译波束搜索算法。搜索算法使用Brown等人（1993）提出的翻译模型。从基于DP的旅行商问题解决方案出发，我们提出了一种新的技术来限制源语言和目标语言之间可能的单词重新排序，以实现高效的搜索算法。提出了对德语到英语的翻译方向特别有用的单词重新排序限制。对这些限制条件进行了推广，并引入了一组四个参数来控制单词的重新排序，从而可以很容易地将这些参数用于新的翻译方向。beam搜索程序已在Verbmobil任务（德语到英语，8000单词词汇）和加拿大Hansards任务（法语到英语，100000单词词汇）上成功测试。对于中等规模的Verbmobil任务，一个句子可以在几秒钟内翻译，只会发生少量的搜索错误，并且按照本文中使用的单词错误标准衡量，没有性能下降。在我们的工作中，一种用于TSP的波束搜索算法分别适用于IBM-4基于单词的模型和基于短语的模型。"}
{"pid": "P02-1034", "zh_sum": "解析和标记的新排序算法：离散结构上的核和投票感知器本文介绍了基于感知器算法的自然语言处理的新学习算法。我们展示了如何将这些算法有效地应用于解析树的指数级表示，例如（Bod 1998）描述的“所有子树”（DOP）表示，或跟踪标记句子的所有子片段的表示。我们给出的实验结果表明，在两项任务上有显著的改进：解析华尔街日报文本和从web数据中提取命名实体。卷积核用于隐式定义树子结构空间。提出了树核用于句法分析重排序。树核根据重叠来评估两棵树之间的相似性，通常以公共子结构的数量来衡量。"}
{"pid": "M95-1005", "zh_sum": "模型理论共指评分方案本说明描述了MUC6中共指任务的评分方案。它改进了原来的方法l：（1）根据模型建立评分方案；（2） 产生更直观的回忆和精确分数；（3）不需要显式计算共指的传递闭包。主要的概念区别在于，我们已经从基于以下共指链接的句法评分模型转移到了由这些链接的模型理论定义的方法。简言之，该方案通过比较密钥和响应中的链接定义的等价类来运行，而不是链接本身（因此，目前这仅适用于标识链接）。当然，这些类是等同关系的模型，出于许多原因，这种策略更可取，其中一个原因是分数独立于用于编码等同关系的特定链接。分数本身是通过确定响应的最小扰动来获得的，这些扰动是将其相应的等价类转换为键的等价类所需的。具体而言，通过计算需要添加到响应（分别是键）的链接的最少数量来找到召回（分别是精度）错误项，以便使类对齐。虽然乍一看，这似乎是组合爆炸性的，因为引用了等价关系的最小生成子集，但事实证明，它可以通过一个非常简单的计数方案来实现。我们为MUC-6和MUC 7共同参考任务介绍了基于链接的MUC评估指标。"}
{"pid": "H91-1026", "zh_sum": "为了减少对内存的需求并确保概率估计的稳健性，我们提出了一种不估计和存储所有词对概率的替代算法。我们提出了一种统计方法来衡量源词和目标词之间的相关性。我们使用phi2统计量作为词对的对应级别，并表明它比互信息更有效。"}
{"pid": "N03-1028", "zh_sum": "使用条件随机场进行浅层解析用于序列标记的条件随机场比生成模型（如HMM）和在每个序列位置应用的分类器都具有优势。在语言处理中的序列标记任务中，随着标准评估数据集的发展和方法之间的广泛比较，浅层分析受到了广泛的关注。我们在这里展示了如何训练条件随机场，以在CoNLL任务中获得与任何已报道的基本名词短语组块方法一样好的性能，并优于任何已报道的单一模型。基于现代优化算法的改进训练方法是实现这些结果的关键。我们对模型和训练方法进行了广泛的比较，证实并加强了以往关于浅层句法分析的结果，并将最大熵模型的训练方法CRF应用于这项工作中的名词短语组块任务，取得了令人印象深刻的实证结果。"}
{"pid": "P02-1033", "zh_sum": "基于平行语料库的无监督词义标注方法我们提出了一种利用平行语料库中翻译对应关系的无监督词义消歧方法。这项技术利用了一个事实，即同一概念的跨语言词汇化往往是一致的，保留了其语义的一些核心元素，但也可变，反映了不同的译者偏好和语境的影响。使用平行语料库会给评估带来额外的复杂性，因为很难找到既有意义标记又与另一种语言平行的语料库；因此，我们使用机器翻译系统创建的伪翻译，以便能够根据标准测试集对方法进行评估。结果表明，词级翻译对应是语义消歧的重要信息来源。我们提出了一种无监督的WSD方法，该方法利用平行语料库中的翻译对应关系，平行语料库是通过在语义标记的英语语料库上应用商业机器翻译系统人工创建的。本文以跨语言标注为目标，提出了一种利用WordNet词义库对平行双语语料库的源文本和目标文本进行词义标注的方法。"}
{"pid": "W07-0702", "zh_sum": "组合分类语法（CCG）中的CCG Supertags为基于短语的机器翻译提供了一个在单词层面上访问丰富句法信息的机会。挑战在于将这些信息纳入翻译过程。分解翻译模型允许在源语言或目标语言中包含SuperTag作为一个因子。我们表明，这提高了翻译质量，而在基于短语的平面结构模型中，句法超标记的价值很大程度上是由于更好的局部重新排序。我们利用基于分解短语的翻译模型将每个单词与supertag关联，supertag包含构建完整解析所需的大部分信息。"}
{"pid": "J03-1002", "zh_sum": "对各种统计对齐模型的系统比较我们提出并比较了使用统计或启发式模型计算单词对齐的各种方法。我们考虑了Brown、Della Pietra、Della Pietra和Mercer（1993）提出的五种对齐模型、隐马尔可夫对齐模型、平滑技术和改进。将这些统计模型与基于骰子系数的两种启发式模型进行了比较。我们提出了不同的方法来组合单词对齐，以实现有向统计对齐模型的对称化。作为评估标准，我们使用产生的维特比比对的质量与手动生成的参考比对进行比较。我们评估了德语-英语Verbmobil任务和法语-英语Hansards任务中的模型。我们对统计对齐系统的各种设计决策进行详细分析，并在各种规模的培训语料库上对这些决策进行评估。一个重要的结果是，与简单的启发式模型相比，具有一阶依赖性和生育率模型的精细对齐模型产生的结果要好得多。在附录中，我们为提出的对齐模型提供了一种有效的训练算法。试验数据和测试数据在单词水平上进行了手动对齐，注意到特定的单词对要么是“确定”的对齐，要么是“可能”的对齐。"}
{"pid": "W00-0717", "zh_sum": "基于上下文分布聚类的句法类别自动归纳本文研究了从未标注语料库中自动归纳句法类别的问题。以前的技术可以获得很好的效果，但无法很好地处理歧义或罕见的单词。提出了一种可自然扩展的上下文分布聚类算法（CDC）。我们在无知识的环境中应用句法聚类和降维来获得有意义的聚类。在我们的自举方法中，我们首先对分布最可靠的词进行聚类，然后用分布上与群集中已有词相似的词对每个集群进行增量扩充。"}
{"pid": "P06-1009", "zh_sum": "基于条件随机场的区分性词语对齐本文提出了一种从句子对齐数据中归纳词语对齐的新方法。我们使用条件随机场（CRF），这是一种判别模型，它是在一个小的监督训练集上估计的。通用报告格式以源文本和目标文本为条件，因此允许对这些数据使用任意和重叠的特征。此外，CRF具有高效的训练和解码过程，两者都能找到全局最优解。我们将此对齐模型应用于法语-英语和罗马尼亚英语语言对。我们展示了如何将大量高预测性特征轻松纳入CRF，并证明即使只有几百个单词对齐的训练句子，我们的模型也比当前最先进的模型有所改进，两个任务的对齐错误率分别为5.29和25.8。"}
{"pid": "P09-1011", "zh_sum": "在较少的监督下学习语义对应关系扎根语言习得的一个中心问题是学习丰富世界状态与引用该世界状态的文本流之间的对应关系。为了解决这种情况下存在的高度歧义，我们提出了一种生成模型，该模型可以同时将文本分割成话语，并将每个话语映射到基于世界状态的意义表示。我们表明，我们的模型概括了三个日益困难的领域——Robocup体育广播、天气预报（一个新领域）和NFL概述。我们提出了一种概率生成方法来生成NL和MRs之间的维特比对齐。我们使用一种分层半马尔可夫生成模型，该模型首先确定要讨论的事实，然后从所选事实的谓词和参数生成单词。"}
{"pid": "A92-1021", "zh_sum": "简单的基于规则的词性标注器自动词性标注是自然语言处理领域的一个领域，统计技术比基于规则的方法更为成功。在本文中，我们提出了一种简单的基于规则的词性标记器，它可以自动获取其规则和标记，其准确度与随机标记器相当。与这些标记器相比，基于规则的标记器具有许多优势，包括：所需存储信息的大幅减少、一小组有意义的规则的清晰性、易于查找和实现对标记器的改进，以及更好地从一个标记集、语料库类型或语言移植到另一个标记集、语料库类型或语言。这项工作最大的贡献可能是证明了随机方法不是词性标注的唯一可行方法。自动学习规则的简单基于规则的标记器可以表现得如此出色，这应该鼓励研究人员进一步探索基于规则的标记，在下面描述的简单但有效的主题上搜索更好、更具表现力的规则模板集和其他变体。我们基于规则的词性标注方法从训练语料库中提取规则，并使用这些规则对新句子进行标注。我们还表明，为每个词汇项指定最常见的词性可以提供90%的准确率。"}
{"pid": "C88-2128", "zh_sum": "解析和生成的统一体系结构使用单一语法进行解析和生成是一个具有一定优雅性的想法，一些研究人员已经注意到这一想法的可取性。在本文中，我们讨论了一种更激进的可能性：不仅一个语法可以被从事不同“方向”处理的不同进程使用，而且一个相同的语言处理体系结构可以用于以各种模式处理语法。特别是，解析和生成可以看作是由一个参数化定理证明器参与的两个过程，用于对形式主义进行逻辑解释。我们将讨论这种体系结构的当前实现，这种体系结构以这样一种方式进行参数化，即它可以与以PATR形式编写的语法一起用于任何目的。此外，该体系结构允许微调以反映不同的处理策略，包括旨在模拟心理语言学现象的解析模型。这种调优允许解析系统在与以前的体系结构相同的效率范围内单独进行解析，但在参与其他处理机制时具有更大的灵活性。我们声明，为了保证在图表中使用预计算条目的完整性，该条目必须包含自顶向下生成的公式。"}
{"pid": "J98-2002", "zh_sum": "提出了一种从大型语料库中自动获取案例框架模式的新方法。特别地，将动词格框格槽值的泛化问题视为词分区上条件概率分布的估计问题，并提出了一种基于最小描述长度（MDL）原则的泛化方法。为了提高效率，该方法利用了现有的同义词库，并将其注意力限制在那些在同义词库树中作为“切割”出现的分区上，从而将泛化问题简化为估计同义词库树的“树切割模型”的问题。给出了一种有效的算法，在MDL意义下，该算法可证明地获得给定时隙频率数据的最优树割模型。利用该方法得到的案例框架模式来解决PP依附模糊问题。实验结果表明，该方法改进了现有方法，或至少与现有方法相当。我们使用基于最小描述长度的算法来为每个分类问题找到WordNet上的最优树，发现词汇关联（Hindle和Rooth，1993）和概念关联的改进，并平衡基于转换的结果。我们提出了一个模型，其中根据最小描述长度原则选择合适的切割c；该原则通过最小化模型描述长度和数据描述长度之和，明确说明了泛化和准确性之间的权衡。"}
{"pid": "J97-1002", "zh_sum": "对话结构编码方案的可靠性本文描述了一种基于话语功能、游戏结构和高级事务结构的对话结构编码方案的可靠性，该方案已应用于一个自发的面向任务的口语对话语料库。我们通过确定编码器对粗段开始位置的一致性，以及对于一致的开始位置，通过计算编码器对粗段结束位置的一致性，来计算构建在更细段顶部的粗段水平上的一致性。"}
{"pid": "P99-1014", "zh_sum": "通过基于EM的聚类归纳语义标注的词典我们提出了一种基于EM统计估计框架中隐藏类归纳的子类别框架时隙注释自动归纳技术。通过一般决策测试对模型进行经验评估。子范畴化框架的时隙标记的归纳是通过EM的进一步应用来完成的，并在分析大型语料库得到的框架观察上进行了实验应用。我们概述了学习表征作为理论语言分解词条的解释。我们测试了3000对随机的动词-名词对，要求erbs和名词在训练中出现30到3000次。我们使用软聚类形成类进行泛化，在我们的选择偏好归纳方法中不使用任何手工资源。"}
{"pid": "J93-1007", "zh_sum": "从文本中提取搭配：Xtract自然语言充满了搭配，这是一种反复出现的词语组合，其出现频率比预期的要高，与任意的词语用法相对应。最近的词典编纂工作表明，搭配在英语中很普遍；显然，它们在所有类型的写作中都很常见，包括技术和非技术文体。有几种方法可以从大量文本数据的分析中检索各种类型的搭配。这些技术会自动生成大量搭配以及统计数字，以反映关联的相关性。然而，这些技术都不能提供搭配的功能信息。此外，产生的结果往往包含不恰当的单词联想，反映了训练语料库中一些不真实的方面，这些方面并不代表真正的搭配。在本文中，我们描述了一套基于统计方法的技术，用于从大型文本语料库中检索和识别搭配。这些技术产生了广泛的搭配，并基于一些原始的过滤方法，允许产生更丰富和更高精度的输出。这些技术已经实现，并产生了一个词典编纂工具Xtract。本文描述了这些技术，并在一个1000万字的股市新闻语料库上给出了一些结果。对作为搭配检索工具的Xtract进行了词典学评价，估计其准确率为80%。我们开发了术语提取系统Xtract。我们提出了一个统计模型，通过测量具有较高强度的共现词对分布的扩散。在实际的MWE识别系统方面，我们提出了一种众所周知的方法，使用基于统计方法的一组技术，从词频计算，来识别语料库中的MWE。"}
{"pid": "W00-1201", "zh_sum": "两种统计分析模型在中文树库中的应用本文首次将统计分析模型应用于新推出的中文树库。我们采用了两种模型，一种是从BBN的SIFT系统（Miller et al.，1998）中提取和改编的，另一种是基于标签的解析模型，改编自（Chiang，2000）。对于单词少于40个的句子，前者的准确率为69%，召回率为75%，后者的准确率为77%，召回率为78%。我们的解析器在单词级别上运行，假设输入句子是预分段的。"}
{"pid": "N12-1067", "zh_sum": "我们提出了一种新的语法纠错评估方法。我们的方法的核心，我们称之为MaxMatch（M2），是一种有效计算源语句和系统假设之间短语级编辑序列的算法，该算法实现了与金标准注释的最大重叠。随后使用F1度量对该最佳编辑序列进行评分。我们在帮助我们自己（HOO）共享的任务数据上测试了M2记分器，结果表明我们的方法可以更准确地评估语法错误更正。我们提出了一种替代的评估方案，该方案根据令牌而不是字符偏移量进行操作。"}
{"pid": "H05-1021", "zh_sum": "统计机器翻译中的局部短语重排序模型我们描述了可并入统计机器翻译（SMT）系统的局部短语移动的随机模型。这些模型在重新排序的短语序列上提供了正确表述的、非缺陷的概率分布。它们由加权有限状态传感器实现。我们描述了在包含重排序的完整翻译模型下基于短语对齐的EM风格参数重新估计过程。我们的实验表明，重新排序模型在阿拉伯语-英语和汉语-英语机器翻译任务的翻译性能上有了显著的提高。我们还表明，该过程随着位文本大小的增加而扩展。我们提出了一种多项式时间策略。我们为他们的翻译模板模型（TTM）定义了两个局部重新排序模型：第一个模型称为MJ-1，只允许相邻短语交换，移动必须在2个窗口内完成。"}
{"pid": "P05-1072", "zh_sum": "语义角色标注是用语义标签对文本中的谓词论元结构进行标注的过程。本文提出了一种基于支持向量机分类器的最新基线语义角色标注系统。我们通过以下方式对此系统进行了改进：i）添加新的特征，包括从依赖关系分析中提取的特征，ii）执行特征选择和校准，以及iii）结合使用不同语法视图训练的语义解析器获得的分析。对基线系统的错误分析表明，大约一半的参数识别错误是由语法错误造成的，其中没有与正确参数对齐的语法成分。为了解决这个问题，我们将Minipar语法分析和分块语法表示的语义分析与基于Charniak分析的原始基线系统相结合。所有报告的技术都带来了性能改进。我们结合了基于短语结构分析、依赖关系分析和浅层分析的系统。我们为内核使用与成分、谓词和谓词成分相关的特性，从而获得最佳性能。我们结合多个解析器的输出来提取可靠的语法信息，并将其转换为特征，用于分配语义角色的机器学习实验。"}
{"pid": "P08-1108", "zh_sum": "集成基于图和基于转换的依赖关系解析器之前对数据驱动依赖关系解析的研究表明，解析错误的分布与用于学习和推理的模型的理论属性相关。在本文中，我们将展示如何通过集成基于图的模型和基于转换的模型来利用这些结果来提高解析精度。通过让一个模型为另一个模型生成特征，我们不断提高两个模型的准确性，从而在CoNLL-X共享任务的数据集上进行评估时，显著提高了最先进的水平。我们首先展示了MST解析器（McDonald et al，2005）和Malt解析器（Nivre et al，2007）是如何通过将每个解析器叠加在另一个解析器的预测上来改进的。在之前的工作中，Nivre和McDonald（2008）通过将一个解析器的输出作为特征输入到另一个解析器中，集成了MST解析器和Malt解析器。"}
{"pid": "J97-1005", "zh_sum": "人工和自动化的语篇分割方法在语篇研究文献中，对语篇结构和语言特征之间的关系进行建模的必要性几乎是公认的。然而，对于话语结构的单位是什么，或者是识别和生成这些单位的标准，人们只有微弱的共识。我们使用自发的叙事独白语料库呈现了一项分为两部分的定量研究结果。本文的第一部分提出了一种实证验证多话语单元的方法，称为语篇片段。我们报告了天真受试者的切分结果，其中说话人意图的常识概念是切分标准。在我们研究的第二部分，从受试者的切分中提取的数据作为评估两组使用话语特征进行切分的算法的目标。在第一个算法集上，我们评估并比较了语篇切分与三种语言线索（指称名词短语、线索词和停顿）的相关性。然后，我们使用两种方法开发第二组：错误分析和机器学习。在一个新的数据集上测试新算法表明，当同时使用多个语言知识源时，算法性能会提高。我们描述了一个实验，七名未经培训的解说员被要求在一个关于电影的转录叙事语料库中找到话语片段。"}
{"pid": "J10-4006", "zh_sum": "分布式记忆：基于语料库的语义学研究的一般框架基于语料库的语义学专注于开发特殊模型，将单个任务或一组密切相关的任务视为不相关的挑战，通过从语料库中提取不同类型的分布信息来应对。作为这种“一个任务，一个模型”方法的一种替代方法，分布式记忆框架以一组加权的单词链接单词元组的形式一次性地从语料库中提取分布信息，这些元组排列成一个三阶张量。然后从张量中生成不同的矩阵，它们的行和列构成自然空间来处理不同的语义问题。通过这种方式，可以在多个任务中共享相同的分布信息，例如建立单词相似性判断模型、发现同义词、概念分类、预测动词的选择偏好、解决类比问题、对单词对之间的关系进行分类、获取带模式或示例对的质量结构、预测概念的典型特性、，将动词分为交替类。在所有这些领域进行的大量实证测试表明，分布式内存实现在执行相同任务时，与文献中最近报道的任务特定算法以及我们对几种最先进方法的实现相比，具有竞争力。因此，尽管分布式记忆方法的多用途性受到限制，但它仍然是站得住脚的。我们使用基于三阶张量的表示，并为分布式语义提供了一个通用框架，在该框架中，可以使用单个数据结构表示意义的多个方面。"}
{"pid": "P09-1042", "zh_sum": "对于许多资源贫乏的语言，不存在通过双文本投影约束进行依赖语法归纳的广泛覆盖率带注释的树库来训练解析器。平行文本和准确语法分析器在英语中的广泛使用，为通过跨位文本的部分转换进行语法归纳提供了可能性。我们考虑使用词级对齐和源语言解析器（英语）来约束可能目标树的空间的依赖语法归纳的生成和区分模型。与以前的方法不同，我们的框架不需要完全投影解析，允许通过树上分布空间上的线性期望约束进行部分近似传输。我们考虑了几种类型的约束，从一般依赖关系保护到辅助动词分析的特定语言注释规则。我们对保加利亚和西班牙CoNLL共享任务数据的方法进行了评估，结果表明，对于有限的训练数据，我们的方法始终优于无监督方法，并且可以优于有监督学习。我们使用后验正则化（PR）方法，其中使用有监督的英语解析器生成约束，这些约束使用并行语料库进行投影，并用于正则化目标语言解析器。"}
{"pid": "P06-1091", "zh_sum": "统计机器翻译的一种判别式全局训练算法本文针对线性评分块序列翻译模型提出了一种新的训练算法。关键部分是一个新的程序，用于直接优化SMT解码器使用的全局评分函数。在早期的SMT工作中，没有使用翻译、语言或失真模型概率。因此，与以前的方法相比，我们的方法使用较少的领域特定知识，既简单又可扩展。此外，训练过程将解码器视为一个黑盒，因此可以用于优化任何解码方案。训练算法在标准的阿拉伯语-英语翻译任务中进行了评估。我们使用BLEU-oracle解码器对局部重排序模型进行区分性训练。我们使用感知器式的算法来训练大量的特征。我们通过运行传统解码器来计算高BLEU假设，以便在简单的局部重新排序模型下最大化BLEU-4的每句近似值。我们提出了一种直接优化基于短语的解码器使用的全局评分函数的过程，该函数基于翻译的准确性。"}
{"pid": "W07-2006", "zh_sum": "SemEval-2007任务07：粗粒度英语全词任务本文介绍了SemEval-2007的粗粒度英语全词任务。我们描述了我们在制作WordNet语义清单的粗略版本和为任务准备语义标记语料库方面的经验。我们介绍了参与系统的结果，并讨论了未来的方向。我们表明，WSD系统的性能清楚地表明，除非采用粗粒度方法，否则WSD并不容易，然后标记所有单词的系统最多只能比最频繁的感知启发式算法高出几个百分点（Navigli et al，2007）。"}
{"pid": "E09-1013", "zh_sum": "贝叶斯词义归纳法词义归纳法旨在直接从语料库中自动识别词义。先前工作的一个关键假设是，围绕一个歧义词的上下文指示其含义。因此，词义归纳通常被视为一个无监督聚类问题，其目的是将单词的上下文划分为不同的类，每个类代表一个单词的词义。我们的工作将意义归纳放在贝叶斯上下文中，通过将歧义词的上下文建模为来自意义多项式分布的样本，而意义多项式分布又被描述为词的分布。贝叶斯框架提供了一种原则性的方法来整合词汇共现之外的广泛特征，并系统地评估其在感觉诱导任务中的效用。所提出的方法在基准数据集上比最先进的系统产生了改进。我们的潜变量公式为其他语言现象的更稳健模型奠定了基础。我们从一个10字窗口中提取伪文档，该窗口以每种单词类型对应的单词标记为中心。我们使用概率词义归纳模型组合不同的特征集，发现只有一些组合产生了改进的系统。"}
{"pid": "P02-1060", "zh_sum": "基于HMM的组块标记器的命名实体识别本文提出了一种隐马尔可夫模型（HMM）和一种基于HMM的组块标记器，并在此基础上建立了一个命名实体（NE）识别（NER）系统来识别和分类名称、时间和数值。通过HMM，我们的系统能够应用和集成四种类型的内部和外部证据：1）单词的简单确定性内部特征，如大写和数字化；2） 重要触发语的内部语义特征；3） 内部地名索引特征；4） 外部宏上下文功能。这样，就可以有效地解决NER问题。我们的系统对MUC-6和MUC-7英语NE任务的评估分别达到96.6%和94.1%的F-测量。结果表明，该系统的性能明显优于其他任何机器学习系统。此外，性能甚至始终优于基于手工规则的性能。我们的命名实体识别系统识别各种类型的MUC风格命名实体，如组织、位置、人员、日期、时间、金钱和百分比。"}
{"pid": "P91-1017", "zh_sum": "两种语言比一种语言的信息量更大。本文提出了一种利用另一种语言词汇关系的统计数据解决一种语言词汇歧义的新方法。这种方法利用了不同语言中单词到感觉的映射之间的差异。我们主要研究机器翻译中的目标词选择问题，该方法直接适用于该问题，并采用统计模型作为选择机制。使用两组希伯来语和德语示例对该模型进行了评估，发现该模型对于消除歧义非常有用。"}
{"pid": "P07-1121", "zh_sum": "使用Lambda演算学习语义分析的同步语法本文介绍了我们所了解的关于学习生成逻辑形式的同步语法的第一个实证结果。利用统计机器翻译技术，在给定一组训练句子及其正确的逻辑形式的情况下，学习基于同步上下文无关语法并添加lambda算子的语义解析器。结果表明，该解析器是迄今为止数据库查询领域中性能最好的系统。我们证明，我们的意义表示语言优于另一种意义表示语言FUNQL。"}
{"pid": "P09-1074", "zh_sum": "名词短语共指消解中的难题：了解最新技术我们旨在通过区分MUC和ACE任务定义的差异、评估方法中的假设以及文本语料库中的固有差异，阐明NP共指消解中的最新技术。首先，我们研究了在共指消解中起作用的三个子问题：命名实体识别、回指确定和共指元素检测。我们衡量了每个子问题对共指消解的影响，并确认评估方法中关于这些子问题的某些假设可以显著简化总体任务。其次，我们测量了一个最先进的共指消解器在几类回指上的性能，并利用这些结果开发了一个定量度量，用于估计新数据集上的共指消解性能。我们证明了共参考分辨问题可以根据提及的类型分为不同的部分。"}
{"pid": "E06-1002", "zh_sum": "利用百科全书知识进行命名实体消歧，我们提出了一种新的方法来检测和消歧开放域文本中的命名实体。训练一个消歧支持向量机核，以利用在线百科全书中编码的知识的高覆盖率和丰富结构。结果模型的表现明显优于不太了解情况的基线。我们测量了NE提及的文本上下文和候选人的维基百科类别之间的相似性。我们使用上下文匹配将名词短语主题链接到维基百科。"}
{"pid": "H05-1010", "zh_sum": "一种用于词对齐的区分性匹配方法我们提出了一种用于词对齐的基于特征的区分性大间距匹配方法。在这个框架中，成对的单词标记会收到一个匹配分数，该分数基于该对的特征，包括单词之间的关联、位置之间的失真、正交形式的相似性等度量。即使只有100个标记的训练示例和包含大量未标记语料库计数的简单功能，我们也可以在更短的时间内实现接近IBM Model 4的AER性能。将模型4预测作为特征，我们在过相交的模型4路线中实现了22%的相对AER减少。我们通过将结构级别的约束分解为对齐链接级别的约束，使用大幅度方法。我们使用一对一约束，其中任一句子中的单词最多可以参与一个链接。我们将对齐问题转化为最大权重二部匹配问题，其中节点对应于两句话中的单词。"}
{"pid": "J01-3003", "zh_sum": "基于论元结构统计分布的动词自动分类词汇知识的自动获取对于广泛的自然语言处理任务至关重要。尤其重要的是关于动词的知识，动词是句子中关系信息的主要来源——将动作或状态与其参与者（即谁对谁做了什么）联系起来的谓词-论元结构。在这项工作中，我们报告了一项监督学习实验，该实验根据英语动词的论元结构，特别是它们分配给参与者的主题角色，自动对三种主要类型的英语动词进行分类。我们使用从大型标注语料库中提取的语言动机统计指标来训练分类器，对于基线为34%的任务，其准确率达到69.8%，基于专家的上限为86.5%。对算法性能及其错误的详细分析证实，所提出的特征捕获了与动词变元结构相关的属性。我们的结果验证了我们的假设，即关于主位关系的知识对于动词分类至关重要，并且可以通过自动手段从语料库中收集。因此，我们展示了更深层次的语言知识与统计技术的稳健性和可扩展性的有效结合。我们使用决策树和选定的语言线索将英语动词分为三类：非宾格动词、非组织动词和宾语省略动词。"}
{"pid": "W03-0301", "zh_sum": "单词对齐的评估练习本文介绍了单词对齐共享任务的任务定义、资源、参与系统和比较结果，该任务是HLT/NAACL 2003年构建和使用平行文本研讨会的一部分。共享任务包括罗马尼亚英语和英语法语子任务，并吸引了来自世界各地的七个团队的参与。我们提供了一个由447对非重叠句子组成的小数据集，可用于评估单词对齐系统的性能。"}
{"pid": "P08-1101", "zh_sum": "联合分词和词性标注使用单个感知器进行中文词性标注，分词是一个初步步骤。为了避免错误传播并利用词性信息改进切分，可以同时执行切分和标记。这种联合方法的一个挑战是巨大的组合搜索空间，这使得高效解码非常困难。最近的研究探索了分割和词性标注的集成，通过在受限版本的完整组合搜索空间下解码。在本文中，我们提出了一个联合切分和词性标注模型，该模型不会对单词和词性信息之间的交互施加任何硬约束。采用一种新的多波束搜索算法实现了快速解码。该系统使用判别统计模型，使用广义感知器算法进行训练。与传统的流水线方法相比，联合模型的分割准确率降低了14.6%，标记准确率降低了12.2%。我们使用一种近似解码算法来跟踪每个字符的一组部分构建的结构，这可以看作是一个动态规划图，通过修剪可以大大减少。"}
{"pid": "W05-0602", "zh_sum": "一个集成语法和语义的统计语义分析器我们介绍了一个学习语义分析器，剪刀，它将自然语言句子映射到一种详细的、形式化的、意义表示语言。它首先使用一个集成的统计解析器生成一个语义增强的解析树，其中每个非终端节点都有一个语法和语义标签。然后使用组合语义过程将扩充的解析树映射到最终的意义表示中。我们从两个方面对该系统进行了评估，一个是自然语言数据库接口，另一个是机器人足球指导说明的解释器。我们给出的实验结果表明，与之前的几种方法相比，剪刀方法可以产生更精确的语义表示。我们介绍了一种称为剪刀的方法，在这种方法中，意义表示的组成是由语法引导的。"}
{"pid": "W02-1503", "zh_sum": "并行语法项目我们报告了并行语法（ParGram）项目，该项目使用六种语言的XLE语法分析器和语法开发平台：英语、法语、德语、日语、挪威语和乌尔都语。ParGram English LFG是在XLE平台上开发的手工制作的广泛覆盖语法。"}
{"pid": "P98-2177", "zh_sum": "无监督介词短语连接的统计模型我们为介词短语连接任务提出了几种无监督统计模型，这些模型接近于此任务的最佳监督方法的准确性。我们的无监督方法使用一种基于附件邻近性的启发式方法，并从原始文本中进行训练，与附件信息相反，原始文本仅使用词性标记和形态基础形式进行注释。因此，与之前针对该任务提出的基于语料库的算法相比，它的资源密集度更低，可移植性更强。我们给出了英语和西班牙语中介词短语附加的结果。我们首先假设所有的PP都有名词依附，然后将我们的消歧方法应用于所有剩余的PP。"}
{"pid": "J94-2003", "zh_sum": "日语语篇和定心过程本文有三个目的：（1）概括一种称为定心的语篇过程计算描述，（2）将该描述应用于日语语篇处理，以便将其用于机器翻译或语言理解的计算系统，（3）探讨日语句法因素对语篇解释的影响。我们认为，虽然话语解释是一个推理过程，但句法线索制约着这一过程；我们就日语中动词的未表达参数零的解释来证明这个论点。我们研究的日语语篇中的句法线索是语法主题的形态标记、后置wa，以及主语、ga、OBJECT、o和OBJECT2、ni等语法功能的语法标记。此外，我们还研究了说话人移情的作用，这是描述事件的角度。这在句法上通过使用动词复合来表示，即辅助使用诸如kureta、kita等动词。我们的研究结果是基于对母语者对短篇语篇的理解的调查，短篇语篇由最少的对组成，受上述因素之一的影响。我们证明，这些句法线索确实会影响零的解释，但之前作为主题并被视为零也会影响话语实体的显著性。我们提出了一个零主题分配的话语规则，并表明中心化对零何时可以被解释为零主题提供了限制。我们建议对日本人进行中锋排名。"}
{"pid": "P02-1051", "zh_sum": "使用单语和双语资源翻译命名实体命名实体短语是最难翻译的短语之一，因为新短语可能不知从何处出现，而且许多是特定领域的，在双语词典中找不到。我们提出了一种利用容易获得的单语和双语资源翻译命名实体短语的新算法。我们报告了该算法在将阿拉伯语命名实体翻译成英语中的应用和评估。我们还将我们的结果与人工翻译和商业系统的结果进行了比较。我们发现，使用外部语言资源，如WWW中音译候选词的数量，可以极大地提高音译的准确性。描述了一种基于拼写的模型，该模型将英语字母序列直接映射为具有相关概率的阿拉伯语字母序列，这些字母序列在一个小的英语/阿拉伯语姓名列表上进行训练，而不需要英语发音。基于语音和拼写的模型被线性地组合成一个单一的音译模型。我们使用Web统计信息对语言模型生成的翻译候选词进行验证，在阿拉伯文-英文OOV词翻译中获得了72.6%的准确率。"}
{"pid": "P05-1044", "zh_sum": "对比估计：在未标记数据条件随机场上训练对数线性模型（Lafferty et al.，2001）在序列标记任务中非常有效，如浅层解析（Sha和Pereira，2003）和命名实体提取（McCallum和Li，2003）。CRF是对数线性的，允许将任意特征合并到模型中。为了在未标记数据上进行训练，我们需要对数线性模型的无监督估计方法；几乎不存在。我们描述了一种新的方法，对比估计。我们表明，新技术可以直观地理解为利用隐式否定证据，并且计算效率高。应用于序列标记问题-给定标记词典和未标记文本的词性标记-对比估计优于EM（使用相同的特征集），对词典降级更为鲁棒，并且可以通过建模其他特征来很大程度上恢复。为了定义对数线性估计的配分函数近似值，我们的对比估计使用一种技术来生成解析的局部邻域。我们通过移动或删除句子中的一个单词来为对比评估方法生成负面证据。我们在PTB上定义粗粒度POS标记。我们使用所有等于零的权重进行初始化（无信息、确定性初始化），并通过评估未看到、未标记的开发数据上的训练标准，跨平滑参数执行无监督模型选择。为了提高分类器的识别能力，我们试图识别出语义上与正例相似的负例。我们表明，即使从非常小的区分性邻域学习，在无监督语法学习方面也有可能取得良好的性能。我们提出的对比估计技术是全局归一化的，因此能够处理任意特征。"}
{"pid": "E06-1005", "zh_sum": "利用增强的假设对齐计算多机器翻译系统的一致性翻译本文描述了一种从多机器翻译（MT）系统的输出计算一致性翻译的新方法。这些输出被结合起来，可能会产生新的翻译假设。类似于（Fiscus，1997）的成熟的罗孚方法，该方法用于组合语音识别假设，共识翻译是通过在混淆网络上投票来计算的。为了创建混淆网络，我们使用增强的统计对齐算法对原始机器翻译假设进行成对的单词对齐，该算法显式地模拟单词重新排序。为了产生对齐，需要考虑整个翻译文档的上下文，而不是单个句子的上下文。在多个机器翻译任务中，包括一个大词汇量任务中，对所提出的对齐和投票方法进行了评估。该方法也在多源和语音翻译的框架下进行了测试。在所有任务和条件下，我们在翻译质量方面取得了显著的改进，例如BLEU分数相对提高了15%。我们将同义词和不同的词形形式相互对齐，但这是隐式完成的，依赖平行文本来学习单词对齐。我们使用基于HMM对齐模型的对称对齐的成对对齐算法。通过使用GIZA++（Och和Ney，2003）将所有假设对视为一个平行语料库，通过训练对齐模型来考虑不同的词序。我们建议使用统计词对齐算法作为一种更稳健的方法，将（单语）输出对齐到混乱网络中，以进行系统组合。"}
{"pid": "E99-1010", "zh_sum": "统计自然语言处理中双语词类的一种有效确定方法我们经常面临数据稀疏的问题。减少这个问题的一种方法是将单词分组为等价类，这是统计语言建模中的一种标准方法。本文描述了一种确定适合统计机器翻译的双语词类的方法。我们提出了一种基于最大似然法的优化准则，并描述了一种聚类算法。我们将展示使用我们获得的双语单词类可以改进统计机器翻译。我们使用基于模板的翻译模型改进了双语语料库的复杂性和单词翻译的准确性。我们描述了一种确定双语单词类的方法，用于通过类之间的对齐而不仅仅是单词之间的对齐来改进对齐模板的提取。"}
{"pid": "J03-1003", "zh_sum": "基于图的引用表达式生成本文描述了一种生成引用表达式的新方法。我们建议将场景（由一组具有各种属性和关系的对象组成）形式化为带标签的有向图，并将内容选择（在引用表达式中包含哪些属性）描述为子图构造问题。成本函数用于指导搜索过程，并优先考虑某些解决方案。目前的方法有四个主要优点：（1）图结构已经得到了广泛的研究，通过移动到图的角度，我们可以直接访问处理图的许多理论和算法；（2） 许多现有的生成算法可以用图的形式重新表述，这增强了各种方法的比较和集成；（3） 图形透视图允许我们解决许多困扰早期生成引用表达式的算法的问题；（4）图和成本函数的结合使用为基于规则的生成技术与最近的随机方法的集成铺平了道路。基于图形的算法的优点之一是它能够生成涉及对象之间关系的表达式，这些表达式包括空间表达式（紧挨着、在上面等等）。"}
{"pid": "D11-1129", "zh_sum": "对于计算语言学家来说，使用经验分布方法对句子的成分意义进行建模的范畴成分分布模型的实验支持一直是一个挑战。我们使用BNC的数据实现了Coecke et al.（2010）的抽象分类模型，并对其进行了评估。该实现基于关系词矩阵的无监督学习，并将其应用于参数向量。该评估基于Mitchell和Lapata（2008）针对不及物句开发的单词消歧任务，以及针对及物句设计的类似新实验。我们的模型在第一个实验中与竞争对手的结果相匹配，在第二个实验中优于竞争对手。随着句法复杂性的增加，结果的总体改善显示了我们模型的合成能力。我们认为，关系词向量存在于一个空间中，该空间的维数是关系数的函数。"}
{"pid": "C10-1152", "zh_sum": "基于单语树的句子简化翻译模型本文将句子简化视为一种特殊的翻译形式，以复句为源，简单句为目标。我们提出了一种基于树的简化模型（TSM），据我们所知，该模型是第一个完整地涵盖拆分、删除、重新排序和替换的统计简化模型。我们还描述了一种有效的方法，用从Wikipedia和Simple Wikipedia获得的大规模并行数据集来训练我们的模型。评估表明，与一组基线系统相比，我们的模型获得了更好的可读性分数。我们使用一个基于树的简化模型，该模型使用统计机器翻译（SMT）的技术处理该数据集。我们研究了英语维基百科和简单维基百科中成对文档的使用，以实现句子简化任务的数据驱动方法。我们提出，句子简化可以作为一种单语机器翻译任务来处理，其中源语言和目标语言相同，输出的形式应该比输入的形式简单，但意义相似。"}
{"pid": "W94-0319", "zh_sum": "是否出现了一致的NL一代架构，它在心理语言学上是否合理？我调查了一些最近面向应用的NL生成系统，并声称尽管理论背景非常不同，但这些系统在将生成过程划分成的模块、这些模块执行的计算以及模块之间的交互方式方面具有非常相似的架构。我还将应用NLG系统中的这种“共识架构”与人类如何说话的心理语言学知识进行了比较，并认为共识架构的至少某些方面似乎与人类语言生产的已知内容一致，尽管心理语言学的合理性通常不是被调查系统开发人员的目标。大多数生成系统通过管道进行语用、语义、词汇和句法决策。我们表明，心理语言学和工程学方法通常会产生在关键方面相似的系统。"}
{"pid": "P05-1018", "zh_sum": "局部一致性建模：一种基于实体的方法本文考虑局部一致性的自动评估问题。受中心理论的启发，我们提出了一种新的基于实体的语篇表示方法，它可以从原始文本中自动计算出来。我们将连贯性评估视为一个排序学习问题，并表明所提出的语篇表征支持对排序函数的有效学习。我们的实验表明，诱导模型比最先进的相干模型具有更高的精度。为了进一步改进子序列分布的计算，我们将矩阵分为显式矩阵和隐式矩阵。我们证明了基于实体的模型能够准确区分源文本和其排列。我们利用话语实体的分布信息和指称信息来提高摘要连贯性。"}
{"pid": "P10-1001", "zh_sum": "高效的三阶依赖关系解析器我们提出了用于高阶依赖关系解析的算法，这些算法是“三阶”的，因为它们可以评估包含三个依赖关系的子结构，而“高效”的，因为它们只需要O（n4）时间。重要的是，我们的新解析器可以利用兄弟样式和孙子样式的交互。我们在Penn树库和Prague Dependency树库上评估我们的解析器，未标记的依恋得分分别为93.04%和87.38%。使用指数梯度下降训练的一阶解析器生成的边缘对潜在边缘集进行修剪。"}
{"pid": "P99-1041", "zh_sum": "自动识别非合成短语非合成表达式对NLP应用提出了特殊的挑战。我们提出了一种利用文本语料库中非合成表达式的统计特性自动识别非合成表达式的方法。我们的方法基于这样一个假设，即当一个短语是非复合词时，它的互信息与通过用相似的词替换短语中的一个词而获得的短语互信息显著不同。我们使用LSA来区分复合动词助词结构和非复合动词助词结构以及名词-名词复合词。基于自动构建的同义词库，我们定义了一个基于短语互信息变化的非合成短语判定标准，即用一个词替换一个相似的词。"}
{"pid": "W09-1119", "zh_sum": "命名实体识别中的设计挑战和误解我们分析了一些基本的设计挑战和误解，这些挑战和误解是开发高效、健壮的NER系统的基础。特别是，我们解决了诸如文本块的表示、结合本地NER决策所需的推理方法、先验知识的来源以及如何在NER系统中使用它们等问题。在比较这些挑战的几种解决方案的过程中，我们得出了一些令人惊讶的结论，并开发了一个NER系统，该系统在CoNLL-2003 NER共享任务中获得了90.8 F1分数，这是该数据集的最佳报告结果。我们使用IOBES符号来表示NE提及的标签序列，从而将NER形式化为一个多类分类问题，其中给定的标记被分类为IOBES标签。我们研究了命名实体识别的设计挑战，并表明其他设计选择，如输出标签的表示和使用基于外部知识的特征，比学习模型本身更重要。我们已经证明，对于CoNLL-2003共享任务，贪婪解码（即宽度为1的波束搜索）与广泛使用的维特比算法具有竞争力，同时速度要快100多倍。"}
{"pid": "P09-1010", "zh_sum": "将指令映射到动作的强化学习本文提出了一种将自然语言指令映射到可执行动作序列的强化学习方法。我们假设可以访问定义执行操作质量的奖励函数。在培训过程中，学习者反复为一组文档构建动作序列，执行这些动作，并观察得到的奖励。我们使用策略梯度算法来估计对数线性模型的参数，以进行动作选择。我们应用我们的方法来解释两个域中的指令——Windows疑难解答指南和游戏教程。我们的结果表明，这种方法可以与监督学习技术相媲美，而只需要很少或不需要带注释的训练示例。我们证明，使用此函数执行策略梯度相当于训练一个完全监督的随机梯度算法来优化条件似然。"}
{"pid": "P02-1035", "zh_sum": "使用词汇功能语法和判别估计技术分析《华尔街日报》，我们提出了一个由词汇功能语法（LFG）、基于约束的解析器和随机消歧模型组成的随机分析系统。我们报告了将该系统应用于分析UPenn华尔街日报（WSJ）树库的结果。该模型结合了完全和部分解析技术，以对看不见的数据实现完全语法覆盖。树库注释用于提供部分标记数据，以便使用指数模型进行区分性统计估计。通过测量两个不同测试集上谓词-参数关系的匹配来评估消歧性能。根据华尔街日报树库子集手动注释f结构的黄金标准，该评估达到79%的f分数。对布朗语料库数据依赖关系黄金标准的评估达到了76%的F分数。我们描述了一个区分性LFG分析模型，该模型通过将每棵树视为一个完整的LFG分析，并使用一个观察到的c结构和隐藏的f结构，在标准（仅语法）树库注释上进行训练。XLE通过基于对数线性概率模型的随机消歧组件从潜在的大候选集中选择最可能的分析，该模型适用于压缩表示。"}
{"pid": "A92-1018", "zh_sum": "一个实用的词性标记器我们提出了一个基于隐马尔可夫模型的词性标记器的实现。该方法能够在几乎不需要资源的情况下实现健壮和准确的标记。只需要一个词典和一些未标记的培训文本。精确度超过96%。我们描述了实现策略和优化，以实现高速运行。描述了标签的三种应用：短语识别；词义消歧；语法功能赋值。我们的半监督模型利用了标记的训练文本和一些未标记的文本。我们使用未标记数据和期望最大化算法来训练统计模型。我们报告了通过利用手工构建的标记词典和等价类，使用隐马尔可夫模型（HMMs）进行无监督词性标注的非常高的结果（在Brown语料库中为96%）。"}
{"pid": "P05-1020", "zh_sum": "机器学习用于共指消解：从局部分类到全局排序在本文中，我们将共指消解视为对不同共指系统生成的候选分区进行排序的问题。我们提出了一组基于分区的特征来学习用于区分好分区和坏分区的排序模型。当在三个标准的共指数据集上进行评估时，我们的方法优于两个最先进的共指系统。我们强调局部获得的排序聚类的全局优化。我们将共指消解作为一项分类任务，并分两步进行解决：（1）训练分类器以确定两个提及是否是共指的，（2）基于成对预测，使用聚类算法将提及划分为簇。我们的方法根据基础模型在单独调优集上的性能对其进行排序，然后使用排名最高的基础模型对测试文档进行预测。大多数基于学习的共指系统可以由四个元素定义：用于训练共指分类器的学习算法、为学习者创建训练实例的方法、用于表示训练或测试实例的特征集以及用于协调共指分类决策的聚类算法。"}
{"pid": "N03-1016", "zh_sum": "A*解析：快速精确维特比解析选择我们将经典的A*搜索过程扩展到表格PCFG解析。通过保守地估计解析完成的概率，使用*搜索可以显著减少找到最佳解析所需的时间。我们讨论了各种估计，并给出了计算它们的有效算法。对于平均长度的Penn treebank语句，我们最详细的估计将处理的边总数减少到穷举分析所需边总数的3%以下，而更简单的估计需要不到一分钟的预计算，将工作减少到5%以下。与实现这种加速的最佳第一束和有限束方法不同，A*方法保证找到最可能的解析，而不仅仅是近似。我们的解析器比向上传播的最佳优先解析器更容易实现，它适用于广泛的解析器控制策略，并保持最坏情况下的立方时间。我们描述了可接受的启发式和解析的A*框架。"}
{"pid": "H05-1073", "zh_sum": "文本情感：基于文本的情感预测机器学习除了信息之外，文本还包含态度，更具体地说，是情感内容。本文采用有监督机器学习和SNoW学习体系结构对基于文本的情感预测问题进行了实证研究。目的是对儿童童话叙事领域中句子的情感亲和力进行分类，以便后续在文本到语音合成的适当表达呈现中使用。在22个童话故事的初步数据集上进行的初步实验表明，在情感与非情感内容的分类上，叙事基线和弓形方法取得了令人鼓舞的结果，并且在一定程度上依赖于参数调整。我们还讨论了一个三方模型的结果，该模型包括情感配价以及特征集交替。此外，我们还提出了一个更合理的认知顺序模型的计划，并考虑了更大范围的基本情绪。"}
{"pid": "A88-1019", "zh_sum": "无限制文本的随机部件程序和名词短语解析器我们的词性标记器现在可以输入无限制文本，并以相当合理的准确性和效率为每个单词分配最可能的词性。我们的词性标记器不仅进行词性分析，还通过在最简单的名词短语周围插入括号来识别它们，主要是限定词、前置修饰词和名词词头序列。"}
{"pid": "D09-1086", "zh_sum": "语法分析器自适应和投影与准同步语法特性我们将结构化学习中的两种场景联系起来：将在一个语料库上训练的语法分析器自适应到另一种注释样式，以及将语法注释从一种语言投影到另一种语言。我们为这些结构化学习任务提出了准同步语法（QG）特征。也就是说，我们根据树的局部特征和对齐对源树和目标树进行对齐。我们的准同步模型将正概率分配给任何树的任何对齐，与异步语法相反，异步语法将坚持某种形式的结构并行性。在单语依赖解析器自适应中，我们在同一个句子的多个注释样式之间实现了高精度的翻译。关于更困难的跨语言解析器投影问题，我们通过使用双语文本、英语解析器和自动单词对齐来学习目标语言的依赖解析器。我们的实验表明，无监督QG投影比仅使用高精度投影注释训练的语法分析器有更好的性能，远远优于仅从原始目标语言文本学习无监督语法分析器的35%以上的绝对依赖精度。当有几个目标语言解析树可用时，投影提供的提升相当于目标语言树数量的两倍。我们认为跨语言适应是一种无监督的投射，使用单词对齐的平行文本来构建目标语言的训练材料。"}
{"pid": "P01-1025", "zh_sum": "词汇联想测量的定性评价方法本文介绍了词汇联想测量的定性、无偏比较方法，以及我们从德语语料库中提取的形容词-名词对和介词-名词-动词三元组的结果。在我们的方法中，我们将整个候选列表（根据特定度量进行排序）与手动确定的“真正积极”参考集进行比较。我们还展示了如何从随机样本中推断出大量的单性畸形和双重发生的估计。我们从法兰克福Rundschau语料库的语块分析版本中提取德语PP动词组合。我们使用了四种配置度量：逐点互信息（PMI）；T得分；对数可能性；以及语料库中N1 N2的原始频率。在一般的语言搭配抽取研究中，在所有标准的测试方法中，t检验的结果最好。"}
{"pid": "W06-2922", "zh_sum": "通过多语言非投影依赖解析器的实验，我们提出了一种基于确定性分类器的移位/约简解析器。我们开发了基于增量确定性分类器的解析器DeSR。我们提出了一个转换系统，其单个转换只能在有限的范围内处理非投影依赖，这取决于新构造的依赖中所涉及的节点堆栈中的距离。"}
{"pid": "N04-1035", "zh_sum": "翻译规则中有什么？我们提出了一种理论，为平行语料库上定义的词级对齐提供形式语义。我们利用我们的理论引入了一种线性算法，该算法可用于从单词对齐的平行语料库中派生出解释人类翻译数据的最小语法转换规则集。我们描述了如何从经过解析、对齐的中英文语料库中学习数亿条树转换规则，并描述了这些规则的概率估计。我们的翻译规则是从单词对齐的双语文本中学习的，这些文本的源端已经使用语法分析器进行了分析。"}
{"pid": "J92-4007", "zh_sum": "RST的一个问题：需要进行多层次的话语分析我们注意到，修辞结构理论将话语的信息（所传达的信息）和意图（对读者的信仰或态度的影响）两个层次融合在一起。我们认为，信息（语义）和意向关系可以同时独立地存在于小句之间。"}
{"pid": "H05-1045", "zh_sum": "使用条件随机场和提取模式识别意见来源最近开发了用于情感分类、意见识别和意见分析（例如，检测极性和强度）的系统。我们追求意见分析的另一个方面：确定意见、情绪和情感的来源。我们将此问题视为一项信息提取任务，并采用混合方法，将条件随机场（Lafferty et al.，2001）和AutoSlog的变体（Riloff，1996a）相结合。当CRF将源识别建模为序列标记任务时，AutoSlog学习提取模式。我们的结果表明，这两种方法的结合比单独使用任何一种方法都要好。由此产生的系统使用头名词匹配度量识别意见来源，准确率为79.3%，召回率为59.5%，使用重叠度量识别意见来源，准确率为81.2%，召回率为60.6%。"}
{"pid": "N03-1020", "zh_sum": "使用N-Gram共现统计自动评估摘要机器翻译社区最近采用BLEU/NIST评分过程进行自动评估后，我们对评估摘要的类似想法进行了深入研究。结果表明，基于各种统计指标，使用摘要对之间的单图共现进行自动评估与人类评估的相关性令人惊讶；然而，直接应用BLEU评估程序并不总能获得良好的结果。我们是第一个系统地指出大规模DUC评估存在的问题，并通过寻求更可靠的自动替代方案寻求解决方案的公司。我们提出了一种半自动的方法ROUGE，它主要基于自动摘要和人工摘要之间的n-gram共现。"}
{"pid": "C02-1150", "zh_sum": "学习问题分类器为了在大量文本的情况下正确回答自由形式的事实问题，需要对问题进行一定程度的理解，以确定问题对可能答案的一些限制。这些约束可能包括所寻求答案的语义分类，甚至可能建议在寻找和验证候选答案时使用不同的策略。提出了一种基于机器学习的问题分类方法。我们学习由答案类型的分层语义层次结构指导的层次分类器，并最终将问题分类为细粒度类。我们展示了TREC 10中使用的大量自由形式问题的准确结果。根据问题中存在的特征，我们将50种可能的类型中的一种分配给问题。我们开发了一种使用SNoW学习架构的机器学习方法。"}
{"pid": "D07-1111", "zh_sum": "基于LR模型和解析器集成的依赖解析和域适配我们提出了一种用于依赖解析的LR算法的数据驱动变体，并通过概率广义LR依赖解析的最佳优先搜索对其进行了扩展。解析器操作由分类器根据表示解析器当前状态的特征来确定。我们将此解析框架应用于CoNLL 2007共享任务的两个轨道，在每种情况下都利用了由不同学习者训练的多个模型。在多语言轨道中，我们为十种语言中的每种语言训练三个LR模型，并将每个模型的分析结果与最大生成树投票方案相结合。在域自适应跟踪中，我们使用两个模型来解析目标域中的未标记数据，以补充标记的域外训练集，其方案类似于联合训练的一次迭代。我们将共同学习和主动学习相结合，通过在标记的训练数据上训练两个不同的解析器，使用两个解析器解析未标记的域数据，并且仅当两个解析器同意他们的分析时，才向训练数据中添加已解析的句子。通过使用最佳优先搜索策略，我们将标准确定性框架推广到概率分析。"}
{"pid": "P05-1011", "zh_sum": "用于宽覆盖HPSG解析的概率消歧模型本文介绍了用于宽覆盖HPSG解析消歧的对数线性模型的发展。对数线性模型的估计需要很高的计算成本，尤其是对于覆盖范围很广的文法。使用技术来降低估计成本，我们使用Penn Treebank的20个部分来训练模型。一系列实验对估计技术进行了实证评估，并检验了消歧模型在实际句子句法分析中的性能。我们的HPSG解析器计算更深入的分析，例如谓词参数结构。我们还引入了一个混合模型，其中前一个模型的概率乘以超级标记概率，而不是初步的概率模型，通过过滤不太可能的词条来帮助估计过程。"}
{"pid": "P99-1048", "zh_sum": "基于语料库的非回指名词短语的识别共指消解涉及到寻找回指语篇实体的前因，如确定的名词短语。但许多明确的名词短语不是回指，因为它们的含义可以从世界常识中理解（例如，“白宫”或“新闻媒体”）。我们开发了一种基于语料库的非回指名词短语自动识别算法，该算法有可能提高共指消解系统的效率和准确性。我们的算法从训练语料库中生成非回指名词短语列表和名词短语模式，并使用它们识别新文本中的非回指名词短语。使用1600篇MUC-4恐怖新闻文章作为训练语料库，我们的方法在50篇测试文档中识别此类名词短语的召回率为78%，准确率为87%。我们开发了一个识别话语新DDs的系统，该系统除了基于语法的启发式方法（旨在识别谓词和已建立的DDs）之外，还集成了从语料库中挖掘不熟悉的DDs的其他技术，包括专有名称、较大情景和语义功能。我们开发了一种无监督学习算法，该算法可以自动识别明确的NPs，这些NPs是存在的，没有句法修改，因为它们的含义是普遍理解的。"}
{"pid": "W02-0817", "zh_sum": "用开放思维单词专家构建词义标注语料库开放思维单词专家是一个实现的主动学习系统，用于从网络上收集一般公众的词义标注。可在以下网址获取http://teach-computers.org.我们期望该系统以比雇佣词典编纂者的传统方法低得多的成本生成大量高质量的培训数据。因此，我们提出了一个Senseval-3词汇样本活动，其中的训练数据是通过开放式词汇专家收集的。如果成功，收集过程可以扩展，以创建词义信息的最终语料库。最后，在与Wikipedia收集过程相关的工作中，我们实现了开放式单词专家系统，用于通过Web从志愿者贡献者那里收集感官注释。我们提出了另一个有趣的建议，该建议转向Web用户来生成带有语义标记的语料库。"}
{"pid": "P09-1019", "zh_sum": "翻译超图和格的有效最小错误率训练和最小贝叶斯风险解码最小错误率训练（MERT）和最小贝叶斯风险（MBR）解码用于当前最先进的统计机器翻译（SMT）系统。这些算法最初是为了处理N个最佳翻译列表而开发的，最近扩展到了比典型的N个最佳翻译列表编码更多假设的格。我们在这里扩展了基于晶格的MERT和MBR算法，以处理超图，这些超图对机器翻译系统基于同步上下文无关语法生成的大量翻译进行编码。这些算法比前面介绍的基于晶格的算法效率更高。我们展示了如何利用MERT优化MBR解码的参数。我们的实验表明，在多个语言对上，MERT和MBR的加速以及MBR解码的性能改进。我们描述了一种计算n-gram后验概率的有效近似算法。"}
{"pid": "P09-1026", "zh_sum": "识别在线辩论中的立场本文提出了一种用于辩论方分类的无监督意见分析方法，即识别一个人在在线辩论中的立场。为了处理这一类型的复杂性，我们在网络上挖掘，以了解在辩论中表明观点立场的关联。我们将这些知识与话语信息相结合，将辩论侧分类任务表述为一个整数线性规划问题。我们的结果表明，我们的方法明显优于具有挑战性的基线方法。我们把立场定义为一个人对一个物体、想法或主张所持的总体立场。"}
{"pid": "P84-1008", "zh_sum": "特征和价值本文讨论了一种新的通用特征计算工具的语言方面。该课程是与我1983年秋季在德克萨斯大学教授的课程相结合而开发的。这是斯图亚特·希伯（StuartShieber）于1983年春为SRI的PATR-II项目设计的一个系统的推广和扩展版本，后来费尔南多·佩雷拉（FernandoPereira）和我对其进行了修改。与之前的版本一样，“DG{有向图}”软件包的新德克萨斯版本主要用于表示形态学和句法信息，但对于语义表示也可能非常有用。我们提供了否定运算符可能有用的特征结构示例。"}
{"pid": "P06-1055", "zh_sum": "学习精确紧凑且可解释的树注释我们提出了一种树注释的自动方法，其中基本非终结符号被交替拆分和合并，以最大化训练树库的可能性。从一个简单的X-bar语法开始，我们学习一种新的语法，其非终结符是原始非终结符的子符号。与之前的工作相比，我们能够根据数据的实际复杂性，将不同的终端划分到不同的程度。我们的语法自动学习在以前的手动树注释工作中显示的各种语言差异。另一方面，我们的语法比以前关于自动注释的工作要紧凑得多，而且准确得多。尽管它很简单，但我们最好的语法在宾州树库上的F1成绩达到了90.2%，高于完全词汇化的系统。我们使用分层EM训练。我们表明，在使用概率上下文无关语法的句法分析领域，自动诱导的语法细化可以优于利用大量手动连接结构的复杂方法。我们引入了分裂-合并平滑估计。"}
{"pid": "W02-1502", "zh_sum": "语法矩阵：用于快速开发跨语言一致的广泛覆盖精确语法的开源初学者工具包语法矩阵是用于开发广泛覆盖HPSG的开源初学者工具包。通过使用类型层次结构来表示跨语言泛化，并提供与其他开源工具的兼容性，用于语法工程、评估、解析和生成，它不仅有助于快速启动，而且有助于快速发展，以实现强大的自然语言处理所需的广泛覆盖，以及自然语言理解所需的精确解析和语义表示。我们的行话语法矩阵项目既是可重用语言知识的存储库，也是以可扩展的精确实现语法的形式将这些知识传递给用户的方法。"}
{"pid": "P98-2182", "zh_sum": "用于半自动语义词典构建的名词短语共现统计与手动创建语义词典相比，半自动生成语义词典可以节省大量时间。在本文中，我们提出了一种基于小样本集从在线语料库中提取类别潜在条目的算法。我们的算法比以前在这方面的工作发现了更多的正确术语，更少的错误术语。此外，生成的条目可能提供比手动编码条目更广泛的类别覆盖范围。我们的算法发现了Wordnet中没有包含的许多术语（比以前的算法多得多），可以将其视为现有广泛覆盖资源的“增强器”。我们使用本地上下文中的共现统计信息来发现兄弟关系。我们的实验是使用MUC-4和《华尔街日报》语料库（约3000万字）进行的。为了选择种子词，我们按照频率对训练语料库中的所有头部名词进行排序，并手动选择明确属于每个类别的前10个名词。我们发现，通过我们的系统学习的每5个单词中就有3个不在WordNet中。"}
{"pid": "C08-1098", "zh_sum": "基于决策树的条件概率估计及其在细粒度词性标注中的应用我们提出了一种HMM词性标注方法，该方法特别适用于具有大量细粒度标记的词性标记集。它基于三个思想：（1）将词性标记拆分为属性向量，并将HMM的上下文词性概率分解为属性概率的乘积，（2）使用决策树估计上下文概率，以及（3）使用高阶HMM。在德国和捷克数据的实验中，我们的标记器优于最先进的POS标记器。我们的细粒度标记集包含大约800个标记。"}
{"pid": "W07-2009", "zh_sum": "SemEval-2007任务10：英语词汇替代任务本文描述了SemEval的英语词汇替代任务。在该任务中，注释者和系统会在上下文中为目标词找到替代词或短语。这项任务包括查找同义词和消除上下文歧义。参与系统可以自由使用任何词汇资源。有一个子任务需要识别单词作为句子中多词的一部分起作用的情况，并检测该多词是什么。在词汇替换任务中，系统试图生成一个词（或一组词）来替换目标词，从而保留句子的含义。我们为上下文敏感的词汇相似性模型建立了一个基准。"}
{"pid": "W03-1014", "zh_sum": "学习主观表达的提取模式本文提出了一个自举过程，学习主观（固执己见）表达的语言丰富的提取模式。高精度分类器标记未注数据以自动创建一个大型训练集，然后将其提供给提取模式学习算法。然后使用所学的模式来识别更多的主观性句子。自举过程学习许多主观模式，并在保持高精度的同时提高召回率。我们利用当前句子和邻近句子中的强弱主观词的数量，构造了一个高精度的连续句子分类器。我们介绍了一种自举方法，使用高精度句子级主观性分类器和大型未注语料库来学习与特定句法模板匹配的主观性提取模式。"}
{"pid": "P05-1045", "zh_sum": "通过Gibbs抽样将非局部信息纳入信息提取系统大多数当前的统计自然语言处理模型仅使用局部特征，以便在推理中允许动态规划，但这使得它们无法完全解释语言使用中普遍存在的长距离结构。我们展示了如何使用Gibbs抽样来解决这个难题，Gibbs抽样是一种简单的蒙特卡罗方法，用于在因子概率模型中执行近似推理。通过在HMMs、CMMs和CRF等序列模型中使用模拟退火代替维特比解码，可以在保留可处理推理的同时合并非局部结构。我们使用此技术通过远程依赖模型扩展现有的基于CRF的信息提取系统，强制标签一致性和提取模板一致性约束。在两个已建立的信息提取任务上，与最先进的系统相比，该技术可使错误减少多达9%。"}
{"pid": "W10-1703", "zh_sum": "2010年统计机器翻译和机器翻译度量联合研讨会的结果本文介绍了WMT10和度量MATR10共享任务的结果，其中包括翻译任务、系统组合任务和评估任务。我们对104个机器翻译系统和41个系统组合条目进行了大规模手动评估。我们使用这些系统的排名来衡量26个指标的自动指标与人类对翻译质量的判断之间的关联程度。今年，我们还调查了通过亚马逊的Mechanical Turk雇佣非专家注释员来增加人类判断的数量。我们在2010年统计机器翻译研讨会上发布了新闻测试集。"}
{"pid": "P02-1050", "zh_sum": "最近，统计机器翻译模型开始利用句法依赖等更高层次的语言结构来评估翻译对应关系。这些模型的基础是关于两种语言中句子之间翻译对应的方向性的假设；然而，这一假设在多大程度上是有效和有用的，目前还没有很好的理解。在本文中，我们提出了一项实证研究，量化了从英语到汉语直接投射句法分析时，句法依赖的保留程度。我们的结果表明，尽管直接对应假设通常过于严格，但一小部分有原则的、基本的语言转换可以将预测的中文语法分析的质量相对于未改进的基线提高76%。基于直接对应假设（Hwa et al，2002）的依赖投影法DPA（Hwa et al，2005）可以描述为：如果有一对具有依赖关系的源词，则可以认为目标句子中对应的对齐词等效地具有相同的依赖关系。我们使用基于短语的统计机器翻译模型对齐平行句子，然后将对齐投影回解析树。"}
{"pid": "D10-1001", "zh_sum": "关于自然语言处理中的对偶分解和线性规划松弛，本文将对偶分解作为推导自然语言处理问题推理算法的框架。该方法依赖于标准动态规划算法作为子问题的oracle解算器，以及一种简单的方法来强制不同oracle之间达成一致。该方法可证明地解决了全局推理问题的线性规划（LP）松弛问题。它导致算法简单，因为它们使用现有的解码算法；高效，因为它们避免了完整模型的精确算法；而且通常是精确的，从经验上讲，尽管使用LP松弛，他们通常能恢复正确的解。我们在两个问题上给出了实验结果：1）两种词汇化句法分析模型的结合；2）词汇化句法分析模型和三元词性标记器的结合。我们在所有迭代中使用解析子模型的得分最高的输出。"}
{"pid": "N07-1071", "zh_sum": "ISP：学习推理选择偏好语义推理是高级自然语言理解的关键组成部分。然而，现有的自动获取的推理规则集合在文本蕴涵和问答等应用中显示出令人失望的结果。本文介绍了ISP，一组自动学习可应用推理规则的可接受参数值的方法，我们称之为推理选择偏好，以及过滤错误推理的方法。我们评估了ISP，并提供了其有效性的经验证据。污垢的上下文敏感扩展侧重于通过将适当的语义类附加到推理规则的X和Y槽，使污垢规则上下文敏感。我们在一种情况下使用WordNet，在另一种情况下使用CBC聚类算法构建一组语义类；对于每个规则，我们使用输入语料库中发现的填充词重叠作为正确语义类的指标。我们用每个关系的选择偏好（即两个参数的细粒度实体类型）来扩充每个关系，以处理多义现象。"}
{"pid": "C94-1032", "zh_sum": "随机日语词法分析器使用前向-后向-A*N-最佳搜索算法我们提出了一种新的方法，用于将输入句子分割成单词并为单词分配词性。它由一个统计语言模型和一个高效的两遍N-最优搜索算法组成。该算法不需要单词之间的分隔符。因此，它适用于书面日语。提出的日本形态分析仪达到95。在ATR语料库上进行训练和测试时，开放文本的召回率为1%，准确率为94.6%。我们提出了一种搜索N个最佳集的方法。"}
{"pid": "C96-2183", "zh_sum": "文本简化的动机和方法长而复杂的句子被证明是当前依赖NL输入的系统的绊脚石。这些系统可以从语法上简化这些句子的方法中获益。为了简化一个句子，我们需要了解句子的结构，识别要分离出来的成分。显然，可以使用解析器来获得句子的完整结构。然而，完全解析很慢，很容易失败，尤其是在复杂的句子上。在本文中，我们考虑了两种可用于简化的完全解析方法。第一种方法使用有限状态语法（FSG）生成名词和动词组，而第二种方法使用超级标记模型生成依赖关系链接。我们讨论了这两种输入表示对模拟过程的影响。我们介绍了一个两阶段的过程，首先从句子到句法树，然后从句法树到新句子。我们的文本简化技术不仅可以帮助有阅读障碍的读者，还可以帮助NLP系统作为预处理工具。"}
{"pid": "J02-4002", "zh_sum": "总结科学文章：相关性和修辞地位实验在这篇文章中，我们提出了一种总结科学文章的策略，该策略集中于文章中陈述的修辞地位：总结材料的选择应确保总结能够突出源文章的新贡献，并将其与之前的文章相比较工作我们为这类总结提供了一个黄金标准，包括大量计算语言学会议论文语料库，并对文章中每个句子的修辞状态和相关性进行了人类判断。我们提出了几个实验来衡量我们的法官对这些注释的一致性。我们还提出了一种算法，该算法基于带注释的训练材料，从看不见的文章中选择内容，并将其分类为七个修辞类别的固定集合。该提取和分类系统的输出本身可以被视为单个文档摘要；或者，它为生成面向任务和用户定制的摘要提供了起始材料，旨在为用户提供科学领域的概述。我们通过对句子的修辞分析来研究总结科学文章的问题。我们通过选择科学摘要中常见的修辞元素来总结科学文章。"}
{"pid": "W09-0401", "zh_sum": "2009年统计机器翻译研讨会的发现本文介绍了WMT09共享任务的结果，其中包括翻译任务、系统组合任务和评估任务。我们对87个机器翻译系统和22个系统组合条目进行了大规模手动评估。我们使用这些系统的排名来衡量自动指标与人类对翻译质量的判断之间的关联程度，共有20多个指标。我们提出了一种新的评估技术，通过该技术可以编辑和判断系统输出的正确性。我们的Fr En 109语料库汇集了大量来自网络的平行法语-英语句子。我们表明，基于语料库的统计机器翻译（SMT）的性能已经达到了传统的基于规则的方法。"}
{"pid": "W03-1810", "zh_sum": "通过检测短语动词中的连续成分，我们研究了自动获取的同义词库用于指示候选多词动词的成分，特别是使用鲁棒解析器自动识别的英语短语动词。我们使用短语动词的近邻，以及在某些情况下单纯形动词的近邻来检验各种度量，并表明其中一些度量与人类在测试集上的成分排序显著相关。我们还表明，虽然组合性判断与一些常用于提取多词的统计数据相关，但这种关系并不像使用自动构建的同义词库那样强。"}
{"pid": "H05-1011", "zh_sum": "双语词语对齐的判别框架双语词语对齐是大多数统计机器翻译方法的基础。当前的词对齐方法主要基于生成模型。在本文中，我们展示了一种有区别的方法来训练简单的单词对齐模型，该模型在准确性上与通常使用的更复杂的生成模型相当。这些模型的优点是易于添加特征，并允许使用少量带注释的数据快速优化模型参数。LLR仍然可以用于通过在预处理步骤中过滤可能存在负关联的单词来提取正关联。我们训练了两个模型，我们称之为第1阶段和第2阶段，这两个阶段都是从一对句子中提取的特征值的加权线性组合形式，以及它们的拟议单词对齐形式。我们使用对数似然比和条件似然概率等统计量来衡量单词关联。"}
{"pid": "N07-1047", "zh_sum": "将多对多对齐和隐马尔可夫模型应用于字母-音素转换字母-音素转换通常需要对齐字母和音素的训练数据。通常，路线仅限于一对一路线。我们提出了一种新的多对多比对训练技术。字母组块二元预测自动管理双字母和双音素，而不是使用固定列表进行预处理。我们还将HMM方法与局部分类模型相结合，预测给定单词的全局音素序列。与传统的一对一方法相比，多对多对齐带来了显著的改进。我们的系统在多种语言和数据集上实现了最先进的性能。M2M对准器基于期望最大化（EM）算法。M2M aligner是一种基于EM的多对多（M-M）对齐算法，允许将多个字母映射到多个音素。"}
{"pid": "P10-1052", "zh_sum": "实用的超大规模CRF条件随机场（CRF）是一种广泛使用的监督序列标记方法，尤其是由于其能够处理大的描述空间和整合标签之间的结构依赖性。即使对于简单的线性链模型，考虑结构也意味着许多参数和计算工作量随标签集的基数二次增长。在本文中，我们讨论了培训包含数百个输出标签和数十亿个特征的超大CRF的问题。这里的效率源于使用LScript 1惩罚项所导致的稀疏性。根据我们自己的实施情况，我们比较了最近为实施这一正规化战略而提出的三项建议。我们的实验表明，可以有效地训练非常大的CRF，并且非常大的模型能够提高精度，同时提供紧凑的参数集。"}
{"pid": "J98-2004", "zh_sum": "最佳优先概率图解析的新优数自然语言最佳优先解析方法试图通过首先考虑最可能的成分来高效地解析。需要一些优值来比较成分的可能性，而这个值的选择对解析器的效率有很大的影响。虽然文献中描述的一些解析器已经使用了这种技术，但很少有关于其功效的公开数据，更不用说试图判断其相对优点了。我们提出并评估了几个最佳优先解析的优数，并确定了一个易于计算的优数，它可以在各种度量和两种不同语法上提供优异的性能。我们提出了最佳优先解析和优点图，允许根据输入字符串的统计信息来调节启发式函数。"}
{"pid": "D09-1026", "zh_sum": "标签LDA：多标签语料库中信用归因的监督主题模型世界上有很大一部分文本是由社交书签网站上的读者标记的。信用归属在这些语料库中是一个固有的问题，因为大多数页面都有多个标记，但这些标记并不总是在整个文档中以相同的特异性应用。解决信用归属问题需要将文档中的每个单词与最合适的标记相关联，反之亦然。本文介绍了标签LDA，一种通过定义LDA的潜在主题和用户标签之间的一对一对应关系来约束潜在Dirichlet分配的主题模型。这允许标签LDA直接学习单词标记对应关系。我们通过可视化del中的标记网页语料库，展示了标记LDA相对于传统LDA改进的表达能力。ICO。我们在提取特定于标记的文档片段时，标签LDA的性能比SVM高出3比1。作为一个多标签文本分类器，我们的模型在各种数据集上都具有很强的竞争力。L-LDA扩展了标准LDA，以包括对特定目标类别的监督，生成过程包括第二个观察变量，即每个文档都明确标记有目标类别。"}
{"pid": "J93-1001", "zh_sum": "《使用大型语料库的计算语言学特刊》导论这部作品对这种经验复兴进行了历史性的描述。近年来，自然语言处理领域的许多研究都集中在基于语料库的实证方法上。"}
{"pid": "P05-1074", "zh_sum": "使用双语平行语料库进行释义之前的工作已经使用单语平行语料库来提取和生成释义。我们表明，这项任务可以使用双语平行语料库来完成，这是一种更常见的可用资源。利用基于短语的统计机器翻译中的对齐技术，我们展示了如何使用另一种语言中的短语作为轴心来识别一种语言中的意译。我们定义了一个释义概率，该概率允许使用翻译概率对从双语平行语料库中提取的释义进行排序，并展示了如何将其细化以考虑上下文信息。我们使用一组手动单词对齐来评估我们的释义提取和排序方法，并将质量与从自动对齐中提取的释义进行对比。我们根据两个短语在所有可能的枢轴短语中的翻译概率来定义两个短语之间的转述概率。"}
{"pid": "C96-2141", "zh_sum": "基于HMM的统计翻译中的词对齐本文描述了一种新的统计翻译中的词对齐模型，并给出了实验结果。该模型的思想是使对准概率取决于对准位置的差异，而不是绝对位置。为了实现这一目标，该方法将一阶隐马尔可夫模型（HMM）用于词对齐问题，因为它们已成功用于语音识别中的时间对齐问题。与时间对齐HMM的不同之处在于，对于可能的词序没有单调约束。我们描述了模型的细节，并在几个双语语料库上对模型进行了测试。我们使用了一个有用的特性来评估通过源语句的对齐路径的优劣。"}
{"pid": "P09-1088", "zh_sum": "短语同步语法归纳的吉布斯采样器我们提出了一个翻译对等的短语同步语法模型。与之前的方法不同，我们没有求助于单词对齐模型的启发式或约束，而是直接从平行句子对齐语料库中归纳出同步语法。我们使用层次化贝叶斯优先于具有小翻译单元的紧凑语法。推理是使用一种新的吉布斯采样器在同步导数上进行的。这个采样器解决了以前模型的棘手问题，这些模型需要对派生林进行推理。相反，每个采样迭代都是高效的，与以前的方法相比，该模型可以应用于更大的翻译语料库。我们使用Gibbs采样器通过推导空间来学习SCFG（Blunsom et al，2009）。我们提出了一种维护表计数的方法，无需记录每个翻译决策的表分配。我们应用了使用多个处理器执行近似吉布斯采样的技术，我们表明，该技术实现了与精确吉布斯采样器等效的性能。"}
{"pid": "P91-1027", "zh_sum": "从未标记文本中自动获取子类别化框架本文描述了一个实现的程序，该程序以未标记的原始文本语料库作为其唯一输入（无开放类词典），并生成文本中出现的动词的部分列表以及它们出现的子类别化框架（SFs）。基于Rouvret和Vergnaud（1980）的格过滤技术，动词被检测出来。输出列表的完整性随着语料库中每个动词出现的总次数单调增加。假阳性率为观察值的1%至3%。目前检测到五个SF，并计划更多SF。最终，我希望为NLP社区提供一个大型SF词典，并为特定语料库培训词典。"}
{"pid": "J97-1003", "zh_sum": "文本平铺：将文本分割为多段副标题段落文本平铺是一种将文本细分为代表段落或副标题的多段单元的技术。识别主要次主题转移的话语线索是词汇共现和分布的模式。该算法得到了充分的实现，并被证明能够产生与人类对12个文本的副标题边界的判断相一致的分割结果。多段落副标题分割对于许多文本分析任务都很有用，包括信息检索和摘要。我们根据编码器说存在段边界的概率（segt）和不存在的概率（unsegt）计算机会一致性。"}
{"pid": "P92-1008", "zh_sum": "整合多个知识源以检测和纠正人机对话中的修复，我们从10718个句子的语料库中分析了607个包含修复的自发人机语音数据句子。我们在此介绍自动检测修复的存在、位置和进行适当校正的标准和技术。标准包括整合来自多个来源的知识：模式匹配、句法和语义分析以及声学。我们能够正确识别406个包含非平凡修复的话语中的309个，而191个流利的话语被错误识别为包含修复。我们推测，声学信息可能被用来过滤假阳性以进行canditate匹配。我们发现单词片段的位置是检测和纠正不流利的一个非常宝贵的线索。"}
{"pid": "D11-1142", "zh_sum": "识别开放信息提取的关系开放信息提取（IE）的任务是从大量语料库中提取断言，而不需要预先指定的词汇。本文表明，最先进的开放式IE系统的输出充斥着无信息和不连贯的抽取。为了克服这些问题，我们对动词表示的二元关系引入了两个简单的句法和词汇约束。我们在混响开放IE系统中实现了这些约束，相对于以前的提取器（如TEXTRUNNER和WOEpos），该系统使精确回忆曲线下的面积增加了一倍多。30%以上的混响提取精度为0.8或更高，而早期系统几乎没有。本文最后对混响误差进行了详细的分析，并对今后的工作提出了建议。我们发现，动词短语揭示了很大一部分二元谓词，同时减少了不表示任何关系的嘈杂短语的数量。我们开发了一个大规模的基于web的混响语料库，包括谓词模板的元组提取及其参数实例化。我们的混响语料库是一个基于web的大规模公开提取数据集，包含约1500万个独特的模板提取，自动从ClueWeb09网络爬网中提取。"}
{"pid": "P06-1097", "zh_sum": "统计词对齐的半监督训练我们引入了一种统计机器翻译的半监督训练方法，该方法将应用于大型训练语料库的传统期望最大化步骤替换为旨在提高手动词对齐的小型子语料库的词对齐质量的判别步骤。我们表明，我们的算法不仅可以改进对齐，而且可以获得更高质量的机器翻译输出。如果有与人对齐的数据可用，EMD算法可以提供比GIZA++更高的基线对齐，从而提高机器翻译性能。我们将一个词对齐的生成模型与一个对数线性判别模型相结合，该模型训练在一小组手对齐的句子上。我们将对齐问题作为对数线性空间中的搜索问题，其特征来自IBM对齐模型。我们提出了一种用于进行单词对齐的EMD算法，该算法在IBM模型中使用的传统期望最大化算法的每次迭代中应用一个判别步骤。"}
{"pid": "E06-1032", "zh_sum": "重新评估Bleu在机器翻译研究中的作用我们认为机器翻译界过度依赖Bleu机器翻译评估指标。我们证明，提高Bleu分数对于实现翻译质量的实际提高既不必要也不充分，并给出了Bleu与人类质量判断相关性的两个重要反例。这为研究提供了新的潜力，之前由于无法提高Bleu分数而被认为是没有希望的。Blue的问题包括：（1）同义词和释义只有在多个参考译文中才能处理（可用）；（2） 单词的分数是相等的，因此遗漏内容材料不会带来额外的处罚；（3） 简洁性惩罚是一种权宜之计，用来弥补无法计算召回率这一相当严重的问题。Blue在比较不同的机器翻译系统时有一定的缺点，尤其是在比较概念上不同的系统时，例如基于短语的系统和基于规则的系统。我们发现，BLEU和NIST支持基于n-gram的机器翻译模型，如法老（Koehn，2004），因此基于规则的系统生成的翻译在自动评估中得分较低，即使人类法官一贯认为其输出高于法老的翻译。"}
{"pid": "P08-1109", "zh_sum": "有效的基于特征的条件随机场句法分析在自然语言处理中广泛使用基于区别性特征的方法，但句子句法分析仍以生成性方法为主。虽然以前基于特征的动态规划解析器限制了对人工短句的训练和评估，但我们提出了第一个基于条件随机场模型的通用、特征丰富的区分性解析器，该解析器已成功扩展到完整的WSJ解析数据。我们的效率主要归功于随机优化技术的使用，以及并行化和图表预过滤。在WSJ15上，我们获得了90.9%的最先进F分数，与以前的模型相比，误差相对减少了14%，同时速度快了两个数量级。对于长度为40的句子，我们的系统达到了89.0%的F分数，与生成性基线相比，误差相对减少了36%。在我们的模型中，分布式在线学习是在同步环境中完成的，这意味着一小批数据被分割到多个CPU中，当所有CPU都完成处理后，模型就会更新（Finkel et al，2008）。"}
{"pid": "P13-1045", "zh_sum": "使用组合向量语法进行句法分析自然语言句法分析通常是使用NP和VP等小范围的离散类别进行的，但这种表示法并不能充分体现语言短语的句法和语义丰富性，试图通过对短语进行词汇化或分类来改进这一点，但仅部分解决了这一问题，代价是巨大的特征空间和稀疏性。相反，我们引入了一种组合向量语法（CVG），它将PCFGs与一个语法上不相关的递归神经网络相结合，学习句法语义、组合向量表示。CVG将Stanford解析器的PCFG提高了3.8%，从而获得了90.4%的F1分数。作为一个高效的重排程序，它的训练和实现速度很快，比当前的斯坦福分解解析器快大约20%。CVG学习中心词的软概念，并改进需要语义信息（如PP附件）的歧义类型的性能。递归神经网络具有生成树结构输出的能力，已经应用于自然语言解析，我们将其扩展到递归神经张量网络，以探索语义的组成方面（Socher et al，2013）。"}
{"pid": "P05-1073", "zh_sum": "联合学习改进了语义角色标记尽管最近在准确的语义角色标记方面取得了很多进展，但之前的工作主要使用独立分类器，可能通过维特比解码与单独的标签序列模型相结合。这与语言学上的观点形成了鲜明对比，即核心论点框架是一种联合结构，论点之间具有很强的依赖性。我们展示了如何构建参数框架的联合模型，将建模这些交互的新特征合并到区分性对数线性模型中。该系统通过PropBank上的金标准解析树最先进的独立分类器，所有参数的错误减少了22%，核心参数的错误减少了32%。我们为SRL引入了一种联合方法，并证明了对解析树的完整谓词参数结构进行评分的模型可以显著减少每个谓词参数关系的独立分类器的错误。我们在训练中使用分解来提高效率：也就是说，分解允许我们在训练示例的子集上训练分类模型，该子集仅由那些具有大小写标记的短语组成。"}
{"pid": "J93-1006", "zh_sum": "文本翻译对齐我们提出了一种只基于内部证据的文本与译文对齐算法。放松过程基于一个概念，即一个文本中的哪个词对应另一个文本中的哪个词，这基本上是基于它们分布的相似性。它利用词级的部分对齐来诱导句子级的最大似然对齐，然后在下一次迭代中使用该对齐来细化词级估计。该算法似乎只需几次迭代就能收敛到正确的句子对齐方式。我们的形态学算法用于分割潜在的后缀和前缀，并用于获得标准化的单词形式。"}
{"pid": "W10-2903", "zh_sum": "推动世界范围内的语义分析；s响应当前的语义分析方法，即将文本转换为正式意义表示的任务，依赖于带注释的训练数据将句子映射到逻辑形式。提供这种监督是扩展语义解析器的一个主要瓶颈。本文提出了一种新的学习范式，旨在减轻监督负担。我们开发了两种新的学习算法，能够预测仅依赖于基于外部环境的二进制反馈信号的复杂结构。此外，我们重新构造了语义解析问题，以减少模型对语法模式的依赖，从而允许我们的解析器使用较少的监督来更好地扩展。我们的结果令人惊讶地表明，在不使用任何注释的意义表示的情况下，使用弱反馈信号学习能够生成与完全监督的解析器相竞争的解析器。我们通过自动找到问题的语义解释来训练系统，从而生成正确的答案。"}
{"pid": "J99-4004", "zh_sum": "半环语法分析我们将语法分析算法、演绎语法分析和应用于形式语言的代数理论综合为一个通用的语法分析系统。每个解析器使用半环的操作执行抽象计算。该系统允许使用一个简单的表示来描述解析器，这些解析器只需替换不同半环的操作即可计算识别、派生林、维特比、n-best、内部值和其他值。我们还展示了如何使用相同的表示（以不同的方式解释）来计算外部值。该系统可用于描述各种解析器，包括Earley算法、树邻接语法解析、Graham Harrison-Ruzzo解析和前缀值计算。我们展示了如何将解析逻辑与各种半环相结合，以计算有关输入的各种信息。我们用半环权重扩充了这样的逻辑程序，给出了具有相同逻辑结构的算法类之间直观联系的代数解释。"}
{"pid": "N04-1014", "zh_sum": "训练树变换器许多自然语言的概率模型现在都是按照层次树结构编写的。基于树的建模仍然缺乏在（有限状态）基于字符串的建模中被视为理所当然的许多标准工具。树型传感器自动机的理论提供了一个可供借鉴的可能框架，因为它已在大量文献中得出。我们鼓励在自然语言中使用树变换器，并解决概率树到树和树到字符串变换器的训练问题。我们定义了广义树到树和树到串传感器的训练和解码算法。我们描述了广义树到树和树到串传感器的训练和解码算法。"}
{"pid": "P02-1022", "zh_sum": "GATE：一个用于健壮的NLP工具和应用程序的框架和图形开发环境在本文中，我们介绍了GATE，一个框架和图形开发环境，使用户能够以健壮的方式开发和部署语言工程组件和资源。GATE体系结构使我们不仅能够为各种语言处理任务（如信息提取）开发大量成功的应用程序，而且能够构建和注释语料库，并对生成的应用程序进行评估。基于其对Unicode的全面支持，该框架可用于以多种语言开发应用程序和资源。我们将ANNIE IE系统包括在标准GATE分发中，用于文本标记、句子分割和词性标记。我们提出了帮助异构语言模块通过公共XML接口进行通信的机制。"}
{"pid": "W09-1304", "zh_sum": "了解生物医学文本中的对冲提示范围识别生物医学文献中的对冲信息是信息提取中的一个重要子任务，因为将推测信息提取为事实信息会产生误导。在本文中，我们提出了一个机器学习系统，该系统可以在生物医学文本中发现模糊限制语线索的范围。该系统基于一个类似的系统，可以发现否定线索的范围。我们表明，相同的范围发现方法可以应用于否定和对冲。为了研究该方法的稳健性，该系统在代表不同文本类型的BioScope语料库的三个子语料库上进行了测试。我们根据监督序列标记方法开发了一种范围检测器。我们提出了一个元学习系统，可以在生物医学文本中发现模糊限制语线索的范围。我们使用浅层句法特征。"}
{"pid": "J00-1004", "zh_sum": "将依赖转换模型学习为有限状态头部传感器的集合本文定义了加权头部传感器，即执行中-外转换的有限状态机。这些传感器严格来说比标准的从左到右有限状态传感器的特例更具表现力。然后将依赖性转导模型定义为分层应用的加权头部传感器集合。描述了一种动态规划搜索算法，用于寻找输入字符串相对于依赖转换模型的最优转换。提出了一种从一组输入输出示例字符串中自动训练依赖转换模型的方法。该方法首先在相关统计信息的指导下搜索训练样本的层次对齐，然后构建与这些对齐一致的头部传感器的过渡。最后给出了将该训练方法应用于英语、西班牙语和日语翻译的实验结果。我们将翻译视为使用头部转导同时诱导源依赖树和目标依赖树的过程。我们通过有限状态传感器的分层组织集合，提出了两级排列的单词排序和组块排序。我们从无支撑的平行文本中归纳出平行树结构，用有限状态传感器模拟每个节点子节点的生成。"}
{"pid": "W06-2932", "zh_sum": "基于两阶段判别解析器的多语言依赖性分析我们提出了一种两阶段多语言依赖性解析器，并在13种不同语言上对其进行了评估。第一阶段是基于McDonald和Pereira（2006）描述的未标记依赖解析模型，并为一部分语言增加了形态学特征。第二阶段获取第一阶段的输出，并在依赖关系图的组件上使用全局训练的序列分类器，使用适当的语法类别标记依赖关系图中的所有边。我们报告了CoNLL-X共享任务（Buchholz et al.，2006）数据集的结果，并进行了误差分析。我们对非投影依赖项和标签使用后处理。我们将依赖项的标记视为序列标记问题。本研究中研究的特定基于图的模型考虑了对弧（而不仅仅是单个弧）的分数，并使用近乎穷举的搜索来进行未标记的解析，再加上单独的分类器来标记每个弧。"}
{"pid": "P99-1069", "zh_sum": "基于随机统一的语法对数线性模型的估计器为随机“基于统一的”语法（SUBG）和其他类型语法的随机版本提供了一个统计上可靠的框架。我们描述了从句法分析训练语料库中估计这类语法参数的两种计算简便的方法，并将其应用于估计随机版本的词汇功能语法。我们将一般语言学原理纳入对数线性模型。我们使用LFG解析器生成的解析作为MRF方法的输入。"}
{"pid": "N04-1013", "zh_sum": "浅层和深层随机句法分析的速度和准确性本文报道了一些实验，比较了两种随机句法分析系统的准确性和性能。目前流行的Collins解析器是一种浅层解析器，其输出包含比其他此类解析器更详细的语义相关信息。XLE解析器是一个深度解析系统，它将词汇功能语法与对数线性消歧组件相结合，并提供更丰富的表示理论。我们根据PARC 700 dependency bank的金标准测量了这两个系统的准确性，并测量了它们的处理时间。我们报告了一个使用LFG语法的深度解析系统的高解析速度：Penn Treebank第23节的560个句子每秒1.9个句子。"}
{"pid": "N03-2021", "zh_sum": "机器翻译的精确性和召回率机器翻译可以使用精确性、召回率和F-测度进行评估。与最近提出的备选方案相比，这些标准措施与人类判断的相关性明显更高。更重要的是，标准度量具有直观的解释，这有助于深入了解如何改进机器翻译系统。相关软件可公开获取。我们制定了一个衡量翻译准确性的指标，该指标直接以精确度和召回率来衡量，而不是以精确度和简洁性来衡量。"}
{"pid": "D07-1101", "zh_sum": "高阶投影依赖解析器的实验我们给出了一个基于富因子的依赖解析模型的实验。我们的模型表示依赖树，其中包含依赖标记与其子级之间的三种类型的关系。我们扩展了Eisner（1996）的投影解析算法，并使用平均感知器训练模型。我们的实验表明，考虑高阶信息可以显著提高解析精度，但在时间和内存消耗方面都有很高的成本。在CoNLL-2007共享任务的多语言练习中（Nivre et al.，2007），我们的系统获得了英语的最佳准确度，巴斯克语和捷克语的第二高准确度。我们扩展了一阶模型，将树中相邻弧对的分数相加，得到了二阶模型。我们的二阶模型包括头-祖父母关系。我们的二阶算法使用了句子中头和从属之间的从属子对象，以及从属对象到孙对象的边。我们引入最左边和最右边的孙子作为因子。"}
{"pid": "P98-1112", "zh_sum": "动词在文档分析中的作用我们展示了两种方法的结果，这两种方法用于评估新闻文章的事件概况，作为动词类型的函数。这项研究的独特贡献在于关注动词的作用，而不是名词。提出并评估了两种算法，其中一种算法可以根据类型和语义属性（即事件概要）准确区分文档。最初的方法是使用WordNet（Miller et al.1990），产生了文章的多重交叉分类，主要是由于动词树的浓密性质以及语义消歧问题。我们的第二种方法是使用英语动词类和交替（EVCA）Levin（1993）表明，《华尔街日报》中频繁动词的单义分类可以有效地区分文档。例如，我们的研究结果表明，交际动词占主导地位的文章往往是观点文章，而同意动词比例较高的文章往往是关于合并或法律案件的。使用Kendall的tau对结果进行评估。我们提供了令人信服的证据，证明在文档分类中使用动词语义类作为区分标准。我们证明了文档类型与某个EVCA类的许多动词的存在相关。"}
{"pid": "W04-2401", "zh_sum": "自然语言任务中全局推理的线性规划公式给定一组离散随机变量，表示自然语言中学习到的局部预测因子的结果，例如命名实体和关系，我们在存在一般（非顺序）约束的情况下寻求变量的最优全局分配。这些约束的示例包括关系可以采用的参数类型，以及不同关系的相互作用等。我们为这个问题开发了一个线性规划公式，并在同时学习命名实体和关系的背景下对其进行评估。我们的方法允许我们在决策时有效地结合特定领域和任务的约束，从而显著提高推理的准确性和类人质量。我们使用ILP来处理命名实体和关系标识的联合推理问题。我们将ILP模型应用于同时将语义角色分配给句子中提到的实体并识别它们之间的关系的任务。我们描述了一个基于分类的框架，在该框架中，他们共同学习识别命名实体和关系。"}
{"pid": "N03-1030", "zh_sum": "利用句法和词汇信息进行句子级语篇分析我们引入了两种概率模型，用于识别基本语篇单元和构建句子级语篇分析树。这些模型使用语法和词汇特征。实现这些模型的语篇分析算法得到的语篇分析树比最先进的基于决策的语篇分析器的错误率降低了18.8%。一组实证评估表明，我们的语篇分析模型足够复杂，能够生成与人类水平相近的准确度水平的语篇树。在修辞结构理论（RST）中，我们开发了两个概率模型，用于识别小句基本话语单元和在句子层面生成话语树。我们介绍了一种统计语篇切分器，该切分器在RST DT上训练，用于标记有边界或无边界标签的单词。"}
{"pid": "W07-1604", "zh_sum": "介词语法错误的检测本文介绍了非英语母语者介词错误检测方面正在进行的工作。由于介词在ESL（英语作为第二语言）学习者的所有语法错误中占很大比例，开发一个能够可靠检测这些类型错误的NLP应用程序将为ESL学生提供宝贵的学习资源。为了解决这个问题，我们使用最大熵分类器结合基于规则的过滤器来检测学生论文语料库中的介词错误。虽然我们的工作还处于初步阶段，但我们实现了0.8的精度，召回率为0.3。Chodorow等人（2007年）利用最大熵模型，基于25个局部上下文特征，从单词到NP/VP语块，估计了34个介词的概率。上下文由25个词汇特征和4个组合特征表示：在介词周围的2个单词窗口中的词汇标记和POS n-grams，加上前一个动词短语（PV）中的头动词，前一个名词短语（PN）中的头名词和后一个名词短语（FN）中的头名词（如果可用）。"}
{"pid": "C86-1045", "zh_sum": "范畴统一语法范畴统一语法（CUG）体现了统一语法和范畴语法形式主义的本质属性。它们以高效、统一的方式将语言知识编码为易于理解和广泛使用的表示形式，这使得它们对计算应用和语言研究具有吸引力。本文将介绍CUG的基本概念及其应用的简单示例。有人认为，CUG的策略和潜力证明了他们在统一语法研究的更广泛背景下进行进一步探索的合理性。本文讨论了一些语言现象的处理方法，如长距离依赖、附加语、语序和外部位置。"}
{"pid": "N04-1025", "zh_sum": "预测阅读难度的语言建模方法我们展示了一种新的研究方法，通过根据统计语言建模重新预测文本段落的可读性来预测阅读难度。我们基于多项式朴素贝叶斯分类的扩展推导了一个度量，该分类结合了多种语言模型来估计给定段落的最可能等级。生成的分类器并不特定于任何特定主题，可以使用相对较少的标记数据进行训练。我们用英语对单个网页进行预测，并将我们的性能与传统可读性度量中广泛使用的语义变量进行比较。我们表明，只要稍作改动，分类器就可以重新训练，以用于法语Web文档。对于英语和法语，分类器在所有测试集中都与标记的等级水平（0.63到0.79）保持良好的相关性。一些传统的语义变量（如类型标记比率）在商业校准的测试段落上表现最好，而我们的语言建模方法对Web文档和非常短的段落（少于10个单词）提供了更好的准确性。我们使用平滑的单图语言模型来预测网页文档和短文的阅读水平。"}
{"pid": "W04-2406", "zh_sum": "在向量和相似空间中对上下文进行聚类的词义识别本文系统地比较了无监督词义识别技术，该技术利用向量和相似空间对原始文本中出现的目标词实例进行聚类。每个实例的上下文表示为高维特征空间中的向量。通过在向量空间中直接聚类这些上下文向量，以及通过在向量之间找到成对的相似性，然后在相似性空间中聚类，可以实现区分。我们对目标词出现的上下文采用了两种不同的表示。一阶上下文向量将目标词的每个实例的上下文表示为该上下文中出现的特征向量。二阶上下文向量是上下文的间接表示，基于表示上下文中出现的单词的向量的平均值。我们通过使用24个SENSEVAL2单词的语义标记实例和著名的Line、Hard和Serve语义标记语料库进行实验来评估被歧视的聚类。"}
{"pid": "A92-1006", "zh_sum": "应用文本生成我们将生成过程中的任务分为三个阶段：文本规划者只能获取有关交际目标、话语语境和语义的信息，并生成文本结构和内容的非语言表示。句子规划师选择抽象的语言资源。它将抽象的词典语法规范5传递给Realizer，Realizer进行屈折、添加虚词和线性化，从而生成表面字符串。"}
{"pid": "P99-1067", "zh_sum": "自动识别来自无关英语和德语语料库的单词翻译已建立了用于对齐翻译文本中单词的算法。然而，直到最近才有人提出新的方法来识别非平行文本甚至无关文本中的单词翻译。这项任务更加困难，因为大多数在平行文本处理中有用的统计线索不能应用于非平行文本。然而在一些研究中，对于平行文本，高达99%的单词对齐被证明是正确的，而非平行文本的准确率到目前为止约为30%。目前的研究基于不同语言语料库中单词共现模式之间存在相关性的假设，对大约72%正确识别的单词翻译进行了显著改进。我们围绕源语言和目标语言的单词创建单词包上下文向量，然后通过当前的小型翻译词典将源语言投影到英语目标空间。我们过滤出单语频率较低（低于100次）的双语术语对。我们发现，对于seed双语词典中未包含的100个德语名词，可以学习到准确的翻译。"}
{"pid": "P06-1011", "zh_sum": "从非平行语料库中提取平行亚句片段我们提出了一种从可比的非平行双语语料库中提取平行亚句片段的新方法。通过使用受信号处理启发的方法分析潜在的相似句子对，我们可以检测出源句子的哪些片段被翻译成了目标句子中的片段，哪些片段没有被翻译成。这种方法使我们能够从非平行语料库中提取有用的机器翻译训练数据，即使这些语料库不包含平行句子对。我们评估了提取数据的质量，结果表明它提高了最先进的统计机器翻译系统的性能。我们首先使用GI-ZA++（带有grow-diag-final和启发式）获得源单词和目标单词之间的对齐，然后计算对齐单词之间的关联强度。我们首先从可比语料库中提取候选平行句，然后利用领域内概率双语词典从候选平行句中提取准确的亚句子双语片段。我们使用标准信息检索和简单的基于词的翻译进行跨语言信息检索（CLIR），并使用干净的双语词典和平均过滤器从检索结果中提取短语。我们通过将初始信号的干净对齐词典与启发式算法相结合来执行短语提取，以平滑对齐以进行最终片段提取。"}
{"pid": "D09-1098", "zh_sum": "计算Web上所有单词之间的成对语义相似度是一项具有计算挑战性的任务。并行化和优化是必要的。我们提出了一种基于分布相似性的高度可扩展实现，在MapReduce框架中实现，并部署了超过2000亿字的Web爬网。使用200个四核节点，在50小时内计算出5亿个术语之间的成对相似度。我们将学习到的相似度矩阵应用到自动集扩展任务中，并进行了大量的实证研究，以量化语料库大小、语料库质量、种子组成和种子大小对扩展性能的影响。我们公开了一个集扩展分析实验平台，其中包括从维基百科中提取的大量不同实体集。我们的破折号将每个短语的大小写存储在维基百科中。我们发现，在分布相似性模型中，10到20个种子是一个足够的起始集，可以发现尽可能多的新的正确实例。给定种子集S，使用语料库中所有种子出现的周围单词上下文生成种子质心向量。"}
{"pid": "P02-1014", "zh_sum": "改进机器学习方法以解决共指问题，我们提出了一个名词短语共指系统，该系统扩展了Soon等人（2001）的工作，据我们所知，在MUC-6和MUC-7共指问题解决数据集上产生了迄今为止最好的结果-F-测度分别为70.4和63.4。改进来自两个方面：学习框架的语言外变化和特征集的大规模扩展，以包括更复杂的语言知识。在测试阶段，我们使用了最佳的第一聚类。我们将Soon等人（2001）的特征集从12个特征扩展到53个特征。我们提出了一个带有规则剪枝的规则归纳系统。"}
{"pid": "N04-1022", "zh_sum": "统计机器翻译的最小Bayes风险解码我们提出了统计机器翻译的最小Bayes风险（MBR）解码。这种统计方法的目的是在衡量翻译绩效的损失函数下，最大限度地减少预期的翻译错误损失。我们描述了一个损失函数的层次结构，它包含来自字串的不同级别的语言信息、机器翻译系统的单词到单词的对齐以及来自源语言和目标语言句子解析树的句法结构。我们报告了MBR解码器在汉英翻译任务中的性能。我们的结果表明，MBR解码可以用于调整特定损失函数的统计MT性能。相应的最小贝叶斯风险（MBR）程序最大化了系统翻译相对于模型在可能翻译上的分布的预期相似性得分。在SMT中，MBR解码允许最小化单个翻译系统的输出损失。MBR通常是通过重新排列由第一遍解码器生成的N个最佳翻译列表来实现的。"}
{"pid": "P95-1050", "zh_sum": "识别非平行文本中的单词翻译句子和单词对齐的常见算法允许自动识别平行文本中的单词翻译。这项研究表明，对于非平行甚至不相关的文本，单词翻译的识别也应该是可能的。该方法是基于不同语言文本中单词共现模式之间存在相关性的假设。我们提出了一种计算要求很高的矩阵purstation方法，该方法可以最大化两种语言中共现矩阵之间的相似性。我们工作中的一个基本假设是，在一种语言中相关的单词的翻译也在另一种语言中相关。"}
{"pid": "W00-0730", "zh_sum": "支持向量学习在语块识别中的应用"}
{"pid": "E06-1040", "zh_sum": "比较自然语言生成系统的自动评估和人工评估，我们考虑了自然语言生成（NLG）中的评估问题，并给出了评估几个具有类似功能的NLG系统的结果，包括一个基于知识的生成器和几个统计系统。我们比较了人类领域专家、人类非专家和几个自动评估指标（包括NIST、BLEU和ROUGE）对这些系统的评估结果。我们发现，NIST得分与人类判断的相关性最好（>0.8），但我们研究的所有自动指标都偏向于仅基于频率选择的生成器。我们的结论是，自然语言系统的自动评估具有相当大的潜力，特别是在高质量参考文本和只有少数人工评估人员可用的情况下。然而，一般来说，自动评估最好由基于人的评估来支持，或者至少由证明特定指标与给定领域中的人类判断有良好相关性的研究来支持。我们使用几种不同的评估技术（基于人类和语料库）评估五个NLG系统的输出，这些系统生成用于天气预报的风描述。我们证明，如果训练数据集具有高质量，自动度量可以与人工评分高度相关。评估中使用的两个自动指标NIST和BLEU与该领域的专家判断高度相关（皮尔逊相关系数分别为0.82和0.79）。"}
{"pid": "P05-1001", "zh_sum": "一种高性能的文本组块半监督学习方法在机器学习中，能否利用未标记数据构建更精确的分类器（半监督学习）是一个重要的问题。虽然已经提出了许多半监督方法，但它们对NLP任务的有效性并不总是很清楚。本文提出了一种新的半监督学习方法，该方法采用了一种称为结构学习的学习范式。其想法是通过学习数千个自动生成的未标记数据辅助分类问题来发现“好的分类器是什么样的”。通过这样做，可以发现多个分类问题共享的公共预测结构，然后可以将其用于改进目标问题的性能。该方法在CoNLL'00语法组块和CoNLL'03命名实体组块（英语和德语）上的性能高于之前的最佳结果。我们在半监督算法中使用多任务学习器来学习特征表示，这些特征表示在大量相关任务中都很有用。我们的结构学习方法使用交替结构优化（ASO）。出于计算和统计两方面的原因，我们遵循计算枢轴预测空间的低维线性近似。"}
{"pid": "P02-1040", "zh_sum": "Bleu：一种自动评估机器翻译的方法人类对机器翻译的评估虽然广泛，但代价昂贵。人工评估可能需要数月的时间才能完成，并且涉及无法重复使用的人工。我们提出了一种快速、廉价、与语言无关、与人类评价高度相关且每次运行边际成本很低的自动机器翻译评价方法。BLEU是一个自动评估机器翻译的系统。BLEU是基于衡量参考翻译和翻译假设之间的字符串级相似性。"}
{"pid": "C86-1016", "zh_sum": "D-PATR：基于统一的语法的开发环境我们描述的系统中，FSs可能会被默认语句修改，从而使此属性不会自动保持。"}
{"pid": "C04-1080", "zh_sum": "上下文中的词性标记我们提出了一种新的HMM标记器，它利用待标记单词两侧的上下文，并在无监督和有监督的情况下对其进行评估。在此过程中，我们首次对词性标注的无监督方法进行了全面比较，指出迄今为止公布的结果在语料库或词典中还没有可比性。观察到词汇的质量极大地影响了算法所能达到的准确性，我们提出了一种HMM训练方法，当词汇概率的训练不稳定时，该方法可以提高准确性。最后，我们展示了这个新的标记器如何在一个有监督的、非培训密集型的框架中实现最先进的结果。在复制早期的实验时，我们发现性能在很大程度上依赖于使用从标记中收集的统计信息清理标记词典。我们证明了双标记HMMs的期望最大化算法在仅给定一个词典（标记词典）和某些有利条件的情况下，对于获取准确的词性标记是有效的。我们观察到，由于使用了优化的词汇，早期的无监督HMM-EM结果被人为地提高了，其中只保留了对每个单词足够频繁的分析。"}
{"pid": "P84-1018", "zh_sum": "功能统一语法：机器翻译的形式主义功能统一语法为在一个形式主义和计算系统中包含机器翻译系统的各个部分提供了一个机会，这些部分通常被单独处理，自然地分析、传递和合成。这种形式主义的许多优点来自于这样一个事实，即它是单调的，允许数据结构随着计算中不同的不确定性替代方案的追求而不同地增长，但决不会以任何方式进行修改。这个系统的一个显著特点是它是基本可逆的，只有当b可以翻译为A时，才允许A翻译为b。"}
{"pid": "H92-1045", "zh_sum": "每个语篇有一个意义众所周知，有些多义词如句子，其“意义”或“意义”取决于使用的语境。我们最近报道了两个新的词义消歧系统，一个是双语材料（加拿大汉莎词典）培训系统，另一个是单语材料培训系统（罗格特的同义词库和格罗利尔的百科全书）。由于这项工作即将完成，我们观察到了非常强烈的话语效果。也就是说，如果一个多义词（如句子）在一篇写得好的文章中出现两次或两次以上，那么它们很可能都有相同的意思。本文描述了一个实验，证实了这一假设，并发现在同一篇文章中分享意义的倾向非常强烈（98%）。这个结果可以作为一个额外的约束源来提高词义消歧算法的性能。此外，它还可以用来帮助评估没有使用话语约束的消歧算法。我们声称，基于语料库分析，在很大程度上，一个词在整个文本中保持相同的含义。"}
{"pid": "P05-1052", "zh_sum": "使用核方法提取具有集成信息的关系实体关系检测是一种信息提取形式，它可以发现文本中实体对之间的预定义关系。本文描述了一种关系检测方法，该方法使用核方法将来自不同语法处理级别的线索结合起来。本文考虑了来自三个不同处理层次的信息：标记化、句子分析和深度依赖分析。每个信息源都由核函数表示。然后，开发复合核来集成和扩展单个核，以便在一个级别上发生的处理错误可以被其他级别的信息克服。我们在2004年ACE关系检测任务中使用支持向量机对这些方法进行了评估，并表明每个层次的语法处理都为该任务提供了有用的信息。当根据官方测试数据进行评估时，我们的方法产生了非常有竞争力的ACE值分数。我们还比较了支持向量机和KNN在不同核上的性能。在ACE RDC 2004语料库的7种关系类型上，我们定义了多个基于特征的复合核来集成各种特征进行关系提取，并实现了70.4的F-度量。我们表明，在深层句法信息中添加局部信息可以改善IE结果。我们提取两次提及之间单词的二元图，旨在提供两次提及之间标记的更多顺序信息。"}
{"pid": "N12-1047", "zh_sum": "统计机器翻译的批量优化策略最近，与传统的MERT方法相比，SMT优化算法能够处理更大的特征集，这方面的工作越来越多。我们从句子级损失函数的角度分析了这些算法，这激发了一些新的方法，包括结构化支持向量机。我们在各种环境下对八种不同的调优策略（包括MERT）进行了实证比较。在其他结果中，我们发现一个简单而高效的批处理版本的MIRA至少可以与在线培训一样好，并且始终优于其他选项。"}
{"pid": "P02-1047", "zh_sum": "无监督的话语关系识别方法我们提出了一种无监督的话语关系识别方法，用于识别文本任意跨度之间的对比、解释-证据、条件和阐述的话语关系。我们表明，在从大量文本中自动提取的示例上训练的话语关系分类器可以用来区分其中一些关系，准确率高达93%，即使这些关系没有明确的提示短语标记。我们使用基于模式的方法从未标记语料库中提取话语关系实例，如对比和细化。我们提出了一种使用在庞大语料库上训练的朴素贝叶斯分类器来识别文本片段之间话语关系的方法。"}
{"pid": "C02-1114", "zh_sum": "无监督词汇习得的图模型本文提出了一种利用图算法从词性标注语料库中组装语义知识的无监督方法。图模型是通过连接参与特定语法关系的单词对来构建的。我们关注列表中同时出现的成对名词之间的对称关系。根据WordNet类评估，使用这部分图的增量聚类构建算法在词汇习得任务中的准确率达到82%。该模型自然地将领域和语料库特定的歧义作为围绕歧义词的图形中的不同组件来实现。我们试图找到内部连接比外部连接更多的图区域。"}
{"pid": "C04-1073", "zh_sum": "利用自动学习的重写模式改进统计机器翻译系统当前基于束的统计机器翻译系统在词序方面有两个局限性：首先，它们缺乏一种表达和使用泛化的机制，从而导致语言短语的重新排序。其次，在这样的系统中，目标词的排序不尊重语言短语的边界。为了解决这些限制，我们建议使用自动学习的重写模式对源语句进行预处理，使其具有与目标语言相似的词序。我们的系统是一个混合系统。基本模型是统计模型，但我们以两种方式使用广泛覆盖的基于规则的解析器——在学习重写模式的培训期间，以及在运行时重新排列源语句。我们的实验表明，Bleu测量值相对提高了10%。我们描述了一种从法语到英语的翻译方法，其中自动获取重新排序规则。我们的重新排序规则是通过对齐源语句和目标语句的解析树自动学习的。"}
{"pid": "E06-1025", "zh_sum": "为观点挖掘确定术语主观性和术语方向观点挖掘是计算语言学的一门新兴分支学科，它与文档所涉及的主题无关，而与文档所表达的观点有关。为了帮助从文本中提取观点，最近的工作解决了确定文本中所含“主观”术语的方向的问题，即确定带有自以为是内容的术语是否具有积极或消极的含义。这被认为对于确定文件的方向至关重要，即确定文件对其主题表达的是积极的还是消极的意见。我们认为，明确确定术语的方向并不是一个现实问题，因为它始于一个非现实的假设，即我们已经知道一个术语是否主观；这意味着可以获得将术语标记为“主观”或“客观”的语言资源，但通常情况并非如此。在本文中，我们面临的任务是确定一个给定的术语是否有积极的内涵，或消极的内涵，或根本没有主观的内涵；因此，这个问题包括确定主观性的问题和确定方向的问题。我们通过测试三种不同的半监督方向检测方法来解决这个问题。我们的结果表明，确定主观性和方向比单独确定方向要困难得多。"}
{"pid": "W06-2501", "zh_sum": "本文利用基于WordNet的上下文向量来估计概念的语义相关性，通过将WordNet的结构和内容与来自原始文本的共现信息相结合，提出了一种基于WordNet的语义相关性度量方法。我们使用共现信息以及WordNet定义来构建对应于WordNet中每个概念的光泽向量。通过测量一对概念各自光泽度向量之间角度的余弦，将相关性的数字分数分配给一对概念。我们表明，在人类对语义关联性的判断方面，该度量优于其他度量，并且当用于依赖语义关联性的词义消歧算法时，该度量表现良好。这一衡量标准是灵活的，因为它可以在任何两个概念之间进行比较，而不考虑它们的词性。此外，它可以适应不同的领域，因为任何纯文本语料库都可以用来获取共现信息。我们通过在WordNet gloss中添加单词的共现向量来创建WordNet意义上的聚合共现向量。我们引入向量度量来确定概念对之间的相关性。"}
{"pid": "P91-1030", "zh_sum": "结构歧义和词汇关系我们提出，歧义介词短语依附可以根据介词与名词和动词的关联相对强度来解决，这是根据大型语料库中的单词分布来估计的。这项工作表明，分布式方法可以有效地解决显然需要复杂推理的解析问题。我们发现，当人类仲裁者被赋予整个句子而不仅仅是相关的四个单词时，他们在命题短语依附方面会达到更高的一致性。"}
{"pid": "P07-1106", "zh_sum": "中文分词采用基于词的感知器算法中文分词的标准方法将问题视为标记任务，为序列中的字符指定标签，指示字符是否标记了单词边界。使用基于局部字符特征的鉴别训练模型进行标记决策，维特比解码找到得分最高的分割。在本文中，我们提出了另一种基于单词的分词器，它使用基于完整单词和单词序列的特征。将广义感知器算法用于区分性训练，并使用波束搜索解码器。对第一个和第二个SIGHAN bakeoffs进行的封闭测试表明，我们的系统与文献中最好的系统具有竞争力，在许多语料库中取得了最高的报告F分数。我们还为中文分词提供了一个特征模板。"}
{"pid": "W03-1809", "zh_sum": "动词助词语义的统计方法本文描述了动词助词结构（如put up、make off）语义的分布方法。我们首先报告实施和评估此类模型的框架。然后，我们报告了一些使用从语料库数据中获得的统计模型来推断动词助词结构含义的技术的实现。"}
{"pid": "P96-1025", "zh_sum": "一种新的基于二元词法依赖关系的统计解析器本文描述了一种新的统计解析器，它基于解析树中头词之间依赖关系的概率。标准的二元概率估计技术被扩展到计算词对之间依赖的概率。使用《华尔街日报》数据进行的测试表明，该方法的性能至少与SPATTER（Magerman 95；Jelinek et al.94）一样好，这是统计解析器在这项任务中发布的最佳结果。该方法的简单性意味着该模型可以在15分钟内训练40000个句子。使用beam搜索策略，解析速度可以提高到每分钟200多句，准确度损失可以忽略不计。我们使用后退平滑技术来缓解稀疏数据问题。"}
{"pid": "E09-1005", "zh_sum": "个性化词义消歧PageRank在本文中，我们提出了一种新的基于图的方法，利用基于WordNet的LKB中的知识进行无监督的词义消歧。我们的算法有效地使用了LKB的完整图，在英语全词数据集中的性能优于以前的方法。我们还表明，该算法可以很容易地移植到其他语言，并取得了良好的效果，只需要有一个wordnet。此外，我们还对该算法的性能进行了分析，表明该算法是有效的，并且可以调整为更快。我们提出了个性化PageRank（PPR），试图在所使用的词汇信息量和总体效率之间进行权衡。我们将顶点的秩初始化为统一值（对于有N个顶点的图，通常为1/N）。我们提出了一种新的使用PageRank进行词义消歧的方法。关键思想是调整矩阵初始化步骤，以利用可用的上下文证据。"}
{"pid": "P08-1119", "zh_sum": "利用下位词模式链接图从Web学习语义类我们提出了一种从Web学习弱监督语义类的新方法，使用一个强大的下位词模式结合图结构，它捕获了与基于模式的抽取相关的两个属性：流行性和生产性。直观地说，如果一个候选者在下义词模式中被其他实例多次发现，那么它就很受欢迎。如果一个候选者经常导致发现其他实例，那么它就是有效率的。这两个度量值结合在一起，不仅捕获了出现的频率，而且还交叉检查候选对象是否出现在类名附近和其他类成员附近。我们开发了两种算法，从一个类名和一个种子实例开始，然后自动生成新类实例的排序列表。我们在四个语义类上进行了实验，始终取得了较高的准确率。我们介绍了一种使用双锚定模式（DAP）引导图排序的自举方案。"}
{"pid": "H01-1035", "zh_sum": "本文描述了一个系统和一组算法，用于自动为任意外语生成独立的单语词性标记、基本名词短语括号、命名实体标记和形态分析器。案例研究包括法语、汉语、捷克语和西班牙语。现有的英语文本分析工具应用于双语文本语料库，并通过统计得出的单词对齐将其输出投射到第二语言上。然而，简单的直接注释投影非常嘈杂，即使使用最佳对齐。因此，本文提出了噪声鲁棒标记器、括号器和柠檬化器训练程序，能够从噪声和不完整的初始投影中精确地进行系统自举。应用于法语的诱导独立词性标记器的性能达到了96%的核心词性（POS）标记准确率，相应的诱导名词短语括号超过91%的F-测度。诱导形态分析器在完整的法语动词系统上实现了99%以上的柠檬化准确率。这一成就尤其值得注意，因为它完全不需要给定语言的手工标注培训数据，而且除了原始文本之外，几乎不需要特定语言的知识或资源。性能也大大超过了通过直接注释投影获得的性能。我们通过单词对齐的平行双语语料库，完成了从英语到法语和捷克语的词性标记标注的跨语言投影的早期工作。"}
{"pid": "W05-0625", "zh_sum": "基于多语义角色标注系统的广义推理我们提出了一种语义角色标注（SRL）方法，该方法通过解决一个优化问题，将多个变元分类器的输出合并为一个连贯的谓词变元输出。通过整数线性规划解决的优化阶段考虑了分类器的建议和一组特定于问题的约束，因此用于清理分类结果和确保最终角色标签的结构完整性。通过这一推断，我们说明了SRL整体性能的显著改善。我们采用了多个SRL系统的输出（每个系统在一棵解析树上），并通过解决优化问题将它们组合成一个连贯的谓词参数输出。"}
{"pid": "W11-1802", "zh_sum": "BioNLP共享任务2011中Genia事件任务概述Genia事件任务是一项生物分子事件提取任务，是BioNLP共享任务2011的主要任务之一。作为第二次安排社区范围的重点工作，它旨在衡量自2009年以来社区的进步，并评估该技术在全文论文中的推广情况。经过3个月的系统开发期，15个团队提交了测试用例的性能结果。结果表明，社区在性能改进和推广方面都取得了显著进步。"}
{"pid": "P06-1010", "zh_sum": "具有可比语料库的命名实体音译在本文中，我们研究了使用可比语料库的中英文名称音译，这种语料库中，两种语言的文本涉及一些相同的主题，因此共享对命名实体的引用，但不是彼此的翻译。我们提出了两种不同的音译方法，一种是使用语音音译，另一种是使用候选对的时间分布。这些方法中的每一种都非常有效，但通过组合这些方法，可以获得更好的结果。然后，我们提出了一种新的分数传播方法，利用音译对在文档对中的共现。与上一步的最佳结果相比，此传播方法实现了进一步的改进。我们比较了可比和同期中英文文本中的姓名，通过训练学习算法来比较姓名对中的音位表示，并考虑姓名对随时间的频率分布，从而对匹配进行评分。"}
{"pid": "W04-2407", "zh_sum": "基于记忆的依赖分析本文报告了使用基于记忆的学习来指导无限制自然语言文本的确定性依赖分析的实验结果。使用来自瑞典小型树库的数据，构建了基于内存的分类器，用于预测解析器的下一个动作。这样的分类器的准确性是根据从树库导出的保留数据来评估的，而其作为解析器指南的性能是通过解析树库的保留部分来评估的。评估结果表明，基于记忆的学习比以前基于最大条件似然估计的概率模型有了显著的改进，词汇特征的加入进一步提高了准确性。"}
{"pid": "P01-1005", "zh_sum": "扩展到非常大的语料库以消除自然语言歧义现成的在线文本数量已经达到数千亿个单词，并且还在继续增长。然而，对于大多数核心自然语言任务，算法在仅包含100万单词或更少单词的语料库上进行训练后，仍在继续优化、测试和比较。在本文中，我们评估了不同学习方法在原型自然语言消歧任务（混淆集消歧）上的性能，该任务在比以前使用的标记数据多几个数量级的情况下进行训练。幸运的是，对于这个特定的应用程序，正确标记的培训数据是免费的。由于情况往往并非如此，因此我们研究了在标记数据代价高昂时有效利用大型语料库的方法。我们认为，与改进使用现有较小训练语料库的方法相比，开发大型训练语料库可能更有效地促进经验自然语言处理的进展。我们表明，即使使用一个非常简单的算法，结果仍然会随着更多的训练数据而线性改善log，甚至达到十亿字。"}
{"pid": "P96-1041", "zh_sum": "语言建模平滑技术的实证研究我们对语言建模领域中的几种平滑技术进行了广泛的实证比较，包括Jelinek和Mercer（1980）、Katz（1987）以及Church和Gale（1991）所描述的平滑技术。我们首次调查了训练数据大小、语料库（例如，Brown vs Wall Street Journal）和n-gram顺序（bigram vs trigram）等因素如何影响这些方法的相对性能，我们通过测试数据的交叉熵来衡量这些方法的相对性能。此外，我们还介绍了两种新的平滑技术，一种是Jelinek-Mercer平滑的变体，另一种是非常简单的线性插值技术，这两种方法都优于现有的方法。我们的平滑技术可以将单元、二元、三元和可能更高的n元序列的预测平滑在一起，以在数据稀疏的情况下获得准确的概率估计。"}
{"pid": "H05-1053", "zh_sum": "特定领域的词义分布和主要词义习得分布通常是高度倾斜的。词义消歧（WSD）系统利用了这一事实，当上下文线索不够强时，该系统会退回到单词的主要词义。文档的域对单词的意义分布有很大的影响，但为每个感兴趣的域生成大型手动注释语料库是不可行的。本文以一个英语单词为例，描述了在不同领域构建三义标注语料库的过程。我们应用现有的方法从原始文本中自动获取主要的语义信息，对于我们的示例，我们证明：（1）从混合域语料库中自动获取此类信息比从SemCor中获取更准确，（2）从与目标域相同域的文本中自动获取此类信息表现最好。我们还表明，对于所有单词的WSD任务，这种自动方法最好集中于该领域中突出的单词，以及与从平衡语料库中获取的单词相比，该领域中具有不同获得主导意义的单词。我们的数据集由3个文档集组成：一个领域中立语料库（BNC）和两个领域特定语料库（体育和金融）。"}
{"pid": "P99-1059", "zh_sum": "有效解析双出式上下文无关语法和头部自动机语法最近的一些随机解析器使用双出式语法，其中每种单词类型都会以特殊的头部单词偏好特定的补语。我们提出了两种双射形式的O（n4）解析算法，改进了O（n5）的先验上界。对于一个已知允许O（n3）解析的常见特例（Eisner，1997），我们提出了一个具有改进语法常量的O（n3）算法。我们证明了字典化PCFG的动态规划算法需要O（m3）状态。"}
{"pid": "W97-0311", "zh_sum": "自动发现并行数据中的非成分化合物自动将文本分割成最小的内容承载单元是一个尚未解决的问题，即使对于英语这样的语言也是如此。单词之间的空格提供了一种简单的一级近似，但这种近似对于机器翻译（MT）来说不够好，因为机器翻译中许多单词序列都不是逐字翻译的。本文提出了一种高效的自动发现作为一个单元翻译的单词序列的方法。该方法通过比较两种语言中平行文本的成对统计翻译模型来进行。它可以在每次迭代中发现数百种非成分化合物，并从较短的化合物中构造出较长的化合物。对一个简单机器翻译任务的客观评价表明，该方法有可能提高机器翻译输出的质量。该方法很少对数据进行假设，因此它可以应用于平行文本以外的平行数据，例如单词拼写和发音。我们提出了一种基于翻译模型预测值的双语多词复合词识别方法。我们研究了在英法平行语料库中识别非成分复合词的技术，并强调考虑非成分复合词的翻译模型更准确。"}
{"pid": "W10-0204", "zh_sum": "常见单词和短语引发的情绪：使用Mechanical Turk创建情绪词汇尽管人们对单词的语义定向和大型极性词汇的创建给予了相当大的关注，但情绪分析的研究必须依赖有限的小型情绪词汇。在本文中，我们展示了如何使用Mechanical Turk创建高质量、中等大小的情感词汇。除了关于术语引发的情绪的问题外，我们还展示了如何包含词语选择问题来阻止恶意数据输入，帮助识别注释者可能不熟悉目标术语的实例（允许我们拒绝此类注释），并帮助获得感官层面（而非词语层面）的注释。我们对注释进行了广泛的分析，以更好地理解由不同词类的术语引起的情绪分布。我们确定哪些情绪倾向于由同一术语同时诱发，并表明某些情绪确实是密切相关的。我们关注常见单词和短语引发的情绪。我们探索使用机械突厥语来构建基于人类判断的词汇。我们创建了一个众包术语情感联想词典，由超过10000个词义对的联想组成，其中有八种情感——快乐、悲伤、愤怒、恐惧、信任、厌恶、惊讶和预期——被认为是基本的原型情感。"}
{"pid": "N04-1023", "zh_sum": "机器翻译中的判别重排序本文描述了判别重排序技术在机器翻译问题中的应用。对于源语言中的每个句子，我们从基线统计机器翻译系统中获得目标语言中排名n的最佳候选翻译列表。我们介绍了两种新的感知机启发的重排序算法，它们基于BLEU度量的评估，在基线系统的基础上提高了机器翻译的质量。我们提供了NIST 2003中英文大数据跟踪评估的实验结果。我们还对我们的算法进行了理论分析，并通过实验验证了我们的算法在机器翻译中提供了最先进的性能。我们比较了在重新排序框架中调整对数线性权重的不同算法，并获得了与标准最小错误率训练相当的结果。我们提出了使用语法信息对解码器的输出重新排序的方法。"}
{"pid": "J95-4004", "zh_sum": "基于转换的错误驱动学习与自然语言处理：词性标注的案例研究最近，自然语言处理领域的经验主义复兴了。作为一种为自然语言处理系统提供语言知识的方法，基于语料库的自动学习正在挑战人工编码语言信息。尽管基于语料库的方法在自然语言处理的许多不同领域都取得了成功，但这些方法通常会在大型不透明的统计表中间接地捕获它们正在建模的语言信息。这会使分析、理解和提高这些方法对潜在语言行为建模的能力变得困难。在本文中，我们将描述一种简单的基于规则的语言知识自动学习方法。许多任务都显示了这种方法，可以以更清晰、更直接的方式捕获信息，而不会影响性能。我们提出了一个详细的案例研究，这种学习方法应用于词性标注。我们提出了一种基于转换的学习方法，它从预先标记的训练语料库中学习猜测规则。我们提出了基于非序列变换的学习方法。我们介绍了一种符号机器学习方法，一种基于类序列实例转换的学习方法。"}
{"pid": "J96-3004", "zh_sum": "中文随机有限状态分词算法任何自然语言处理任务的文本分析的初始阶段通常涉及到输入到单词的标记化。对于像英语这样的语言，我们可以假设，一级近似情况下，单词边界是由空格或标点符号给出的。另一方面，在包括汉语在内的各种亚洲语言中，从未使用空格来界定单词，因此必须借助词汇信息来“重建”单词边界信息。本文提出了一种随机有限状态模型，其中基本工作是加权有限状态传感器。该模型将中文文本分割为字典条目和通过各种生产性词汇过程派生的单词，并且由于该模型的主要预期应用是文本到语音的合成，因此提供了这些单词的发音。我们通过将其分割“判断”与一组人类分割者的判断进行比较来评估系统的性能，结果表明该系统的性能相当好。我们使用维特比再估计建立了一个单词单图模型，维特比再估计的初始估计来自词典中每个单词的字符串在语料库中的频率。我们提出了一种在不分割语料库的情况下估计一组初始词频的方法。"}
{"pid": "D10-1044", "zh_sum": "统计机器翻译中领域自适应的区分性实例加权我们描述了一种新的SMT自适应方法，该方法根据领域短语对与目标领域的相关性对其进行加权，这取决于它们与目标领域的相似程度，以及它们是否属于通用语言。这扩展了之前关于区分权重的工作，使用了更精细的粒度，关注实例的属性而不是语料库组件，并使用了更简单的训练过程。我们将实例权重纳入混合模型框架，并发现它在广泛的基线范围内产生一致的改进。我们根据句子的复杂度得分对通用领域语料库中的句子对进行排序，这些分数是根据领域内语言模型计算的。我们应用线性插值将实例加权的域外模型与域内模型相结合。我们提出了一种机器翻译方法，该方法使用特征来捕获通用性程度。"}
{"pid": "J08-1001", "zh_sum": "建模局部一致性：一种基于实体的方法本文提出了一种表示和度量局部一致性的新框架。这种方法的核心是话语的实体网格表示，它捕获文本中实体分布的模式。本文介绍的算法自动将文本抽象为一组实体转换序列，并记录有关话语实体的分布、句法和引用信息。我们将连贯性评估重新定义为一项学习任务，并表明我们基于实体的表示非常适合基于排名的生成和文本分类任务。使用该表示，我们在文本排序、摘要连贯性评估和可读性评估方面取得了良好的性能。为每个文档构建实体网格，并将其表示为矩阵，其中每行表示一个句子，每列表示一个实体。我们在两个数据集上进行实验：关于地震（地震）主题的新闻文章和关于航空事故（事故）主题的叙述。"}
{"pid": "W08-0509", "zh_sum": "单词对齐工具的并行实现在大型语料库上训练单词对齐模型是一个非常耗时的过程。本文描述了两种并行实现的GIZA++，它们加速了这个单词对齐过程。其中一个实现运行在计算机集群上，另一个运行在使用多线程技术的多处理器系统上。结果表明，根据使用的CPU数量，速度接近线性，并且保持了对齐质量。我们使用多线程版本的GIZA++工具。这将加快进程并更正GIZA++中可能出现的罕见单词错误。"}
{"pid": "P84-1075", "zh_sum": "语言信息的计算机语言设计关于向计算机传递信息的语言设计的大量积累知识来自编程语言设计和语义学的子领域。SRI零件组的目标是利用这些知识的相关部分实施工具，以促进语言信息与计算机的通信。PATR-II形式主义是我们目前用于编码语言信息的计算机语言。本文简要概述了这种形式主义，试图用一组有效的计算机语言应该包含的属性来解释我们的设计决策。PATR-II是一种基于最小约束的形式主义，它扩展了上下文无关语法。"}
{"pid": "C04-1010", "zh_sum": "英语文本的确定性依赖分析本文提出了一种基于记忆学习的确定性依赖分析方法，它可以在线性时间内解析英语文本。当在宾州树状银行的《华尔街日报》栏目进行培训和评估时，解析器获得了87.1%的最大附件分数。与以前的大多数系统不同，解析器生成带标签的依赖关系图，使用括号标签和语法角色标签的组合作为弧标签，这些标签取自Penn Treebank II注释方案。当仅限于语法角色标签（7个标签）和最大集合（50个标签）时，识别正确头部和正确弧线标签的最佳总体准确率分别为86.0%和84.4%。我们提出了山田和松本模型的一个变体，该模型降低了复杂性，从最坏情况下的二次型到线性。由于采用了贪婪的搜索策略，我们基于确定性移位/约简分类器的依赖解析方法提供了最先进的精度和高效性。"}
{"pid": "P06-2006", "zh_sum": "评估PARC DepBank上未出射化统计解析器的准确性我们评估了未出射化统计解析器的准确性，该解析器使用来自平衡数据的4K树库句子进行训练，并在PARC DepBank上进行测试。我们证明，在精度上具有竞争力（不牺牲处理速度）的解析器可以快速调整，而不依赖于域内手动构建的大型树库。这使得在需要访问谓词参数结构方面的应用程序中使用统计解析器更加实用。使用DepBank对系统进行比较并不简单，因此我们扩展并验证了DepBank，并强调了关系评估方案的一些表示和评分问题。我们表明，当考虑到原始德班克黄金标准中的形态句法特征时，该系统具有与PARC-XLE解析器同等的精度。我们为内部NP结构提供注释。我们建议按依赖类型查看准确度数据，以了解解析器擅长什么。我们使用GRs方案重新注释了DepBank，并使用它来评估RASP解析器。"}
{"pid": "C08-1018", "zh_sum": "在本文中，我们概括了句子压缩任务。不像以前的工作那样，我们只是通过删除单词或成分来缩短句子，而是使用替换、重新排序和插入等附加操作来重写它。我们提出了一个适合我们任务的新语料库和一个能够自然解释结构和词汇不匹配的区分树到树转换模型。该模型采用了一种新的语法提取方法，使用语言模型进行相干输出，并且可以很容易地调整到各种特定于压缩的损失函数。与之前的研究不同，我们基于一个统计模型，通过几个操作的组合来实现句子压缩，包括单词删除、替换、插入和重新排序，这与我们的释义生成过程类似。我们提出了一个模型，该模型可以压缩和解释单个句子，而不需要生成文档级摘要。我们的抽象方法更清楚地揭示了人们是如何压缩句子的，但并不总是能够超越抽取方法。我们将任务扩展到包括从平行文本中自动学习的替换、插入和重新排序。我们提出了第一种抽象压缩方法。"}
{"pid": "W06-2933", "zh_sum": "使用支持向量机进行标记伪投影依赖分析我们使用SVM分类器预测确定性解析器的下一个动作，该解析器以增量方式构建标记投影依赖图。通过对分类器的训练数据进行投影，并对解析器的输出应用逆变换，间接捕获非投影依赖关系。我们目前的评估结果和误差分析侧重于瑞典和土耳其。我们的伪射影方法将非射影训练树转换为射影训练树，但对在DEPREL中进行逆变换所需的信息进行编码，这样也可以在测试树上进行逆变换（Nivre et al，2006）。"}
{"pid": "P09-1039", "zh_sum": "简明的整数线性规划依赖解析公式我们将非投影依赖解析问题描述为一个多项式大小的整数线性规划。我们的公式能够有效地处理非局部输出特征；它不仅与编码为硬约束的先验知识兼容，还可以从数据中学习软约束。特别是，我们的模型能够学习相邻弧（兄弟姐妹和祖父母）之间的相关性、单词的配价以及接近投影分析的趋势。采用线性规划松弛法，在最大裕度框架下学习模型参数。我们评估了解析器在几种自然语言数据上的性能，从而实现了对现有最先进方法的改进。我们介绍了多物质流公式。"}
{"pid": "W99-0611", "zh_sum": "本文介绍了一种新的、无监督的名词短语共指消解算法。它与现有方法的不同之处在于，它将共指消解视为一项聚类任务。在对MUC-6共指消解语料库的评估中，该算法实现了53.6%的F-度量，将其牢牢地置于MUC-6评估中最差（40%）和最佳（65%）系统之间。更重要的是，聚类方法优于唯一将共指消解视为学习问题的MUC-6系统。聚类算法似乎提供了一种灵活的机制，用于协调上下文无关和上下文相关约束和偏好的应用，以便将名词短语准确划分为共指等价类。我们将WordNet的使用与专有名称地名录相结合，以获得聚类算法中相关NPs兼容性的信息。仅依赖于聚类的共指消解方法可以很容易地增强传递性。我们使用成对NP距离对文档提及进行聚类。我们的系统使用WordNet中的节点距离（上限为4）作为距离度量的一个组成部分，用于指导其聚类算法。共指消解分为两个阶段：二元分类阶段，评估每对名词短语共指的可能性；以及一个划分阶段，在该阶段中，相互共转运的核动力源形成集群，最大化某些全局标准。"}
{"pid": "P99-1068", "zh_sum": "为双语文本串挖掘Web（Resnik，1998）是一个独立于语言的系统，用于在万维网上自动发现并行翻译中的文本。本文通过添加自动语言识别、按数量级放大和正式评估性能，扩展了初步的串结果。最新的最终产品是一个自动获取的平行语料库，包含2491个英法文档对，每种语言约150万个单词。我们使用来自页面的结构标记信息，而不查看页面内容来尝试对齐页面。"}
{"pid": "D07-1109", "zh_sum": "词义消歧的主题模型我们使用WORDNET（LDAWN）开发了潜在Dirichlet分配，这是一种将词义作为隐藏变量的无监督概率主题模型。我们开发了一种概率后验推理算法，用于同时消除语料库的歧义和学习考虑每个单词的域。使用WORDNET层次结构，我们将Abney和Light（1999）的结构嵌入到主题模型中，并表明与其他上下文相比，自动学习的域提高了WSD的准确性。我们使用潜在Dirichlet分配（LDA）提取的文档级主题作为词义消歧的指标。我们描述了一个相关的主题模型LDAWN，用于词义消歧，该模型添加了一个潜在变量Z的中间层，马尔可夫模型参数以该层为条件。我们将语义集成到主题模型框架中。"}
{"pid": "W07-1401", "zh_sum": "第三次PASCAL识别文本蕴涵挑战本文介绍了第三次PASCAL识别文本蕴涵挑战（RTE-3），概述了数据集创建方法和提交的系统。在创建今年的数据集时，引入了一些较长的文本，使挑战更加面向现实场景。此外，还提供了一个资源库，以便参与者可以共享通用工具。还设立了一项试点任务，旨在区分未知的必然性和已识别的矛盾，并为总体系统决策提供理由。26名参与者提交了44次跑步，使用了不同的方法，通常提出了新的包含模型，并取得了比之前挑战更高的分数。识别文本蕴涵的任务是确定假设句是否可以由前提句蕴涵（Giampiccolo et al，2007）。文本蕴涵（TE）已经成为一种重要的语义推理建模范式，满足了广泛文本理解应用的需求。"}
{"pid": "E87-1002", "zh_sum": "非接触的有限状态形态学，而不是沿着更传统的有限状态传感器建模形态学，我们建议使用n带自动机对其进行建模，其中磁带将精确地承载闪语交指中所要求的这种交错。我们提出了一个框架，在多磁带有限状态机中为每个自动分段层分配一个磁带，并为曲面形式分配一个额外的磁带。"}
{"pid": "H92-1026", "zh_sum": "面向基于历史的语法：使用更丰富的概率分析模型，我们描述了自然语言的生成概率模型，我们称之为HBG，它利用详细的语言信息来解决歧义。HBG以一种新颖的方式将语法树中的词汇、句法、语义和结构信息整合到消歧过程中。我们使用一个被称为树库的括号内句子语料库，结合决策树构建，梳理出解析树的相关方面，这些方面将决定句子的正确解析。这与通常通过语言内省进一步裁剪语法的方法形成对比，希望生成正确的语法分析。在针对现有最好的鲁棒概率解析模型之一（我们称之为P-CFG）进行的头对头测试中，HBG模型的性能明显优于P-CFG，解析准确率从60%提高到75%，错误减少了37%。我们提出了基于历史的解析，使用解析历史的特征来预测下一个解析器操作。"}
{"pid": "M95-1012", "zh_sum": "MITRE：MUC-6使用的Alembic系统的描述与其他几位资深MUC参与者一样，MITRE的Alembic系统在过去两年中经历了重大变革。这一转变的起源发生在上一届Muc会议Muc-5的一次晚宴对话中。当时，我们中的一些人不情愿地承认，提高性能的主要障碍是依赖于当时的标准语法语言模型。我们知道，我们需要一种替代传统语言语法的方法，甚至是我们当时使用的非传统范畴伪解析器。问题是，哪种选择？答案是以规则序列的形式出现的，埃里克·布里尔（EricBrill）最初在他的词性标注工作中提出了这一方法【5，7】。规则序列现在是Alembic中所有主要处理步骤的基础：词性标记、句法分析、推理，甚至是模板元素任务（TE）中的一些集合填充处理。我们发现，这种方法几乎令人尴尬，因为它的优势、速度和准确性是最明显的外部优势。此外，我们的大多数规则序列处理器都是可训练的，通常来自小样本。以这种方式获得的规则还有一个特点，即它们允许人们随时混合手工制作和机器学习的元素。我们利用这个机会广泛应用机器学习和手工制作的规则，在某些情况下选择运行主要由机器学习的序列，在其他情况下选择运行完全由手工制作的序列。我们针对英语NE的典型机器学习方法是基于转换的学习。"}
{"pid": "W99-0629", "zh_sum": "级联语法关系分配本文讨论了基于级联记忆的语法关系分配。在级联的第一阶段，我们发现了几种类型的语块（NP、VP、ADJP、ADVP、PP），并用它们的状语功能（如局部、时间）来标记它们。在最后一个阶段，我们将语法关系分配给成对的语块。我们研究了在这个级联分类器中添加几个级别的效果，发现即使性能较差的组块也可以提高关系查找器的性能。在对《华尔街日报》约40000个句子进行训练后，我们在自动标记和分块文本的语法关系分配上取得了71.2的F分数。"}
{"pid": "J99-4005", "zh_sum": "词替换翻译模型中的解码复杂性统计机器翻译是解决计算机翻译人类语言这一长期问题的一种相对较新的方法。当前的统计技术从双语培训文本中发现翻译规则，并使用这些规则翻译新文本。一般的架构是源-通道模型：统计生成一个英语字符串（源），然后统计转换为法语（通道）。为了翻译（或“解码”）法语字符串，我们寻找最可能的英语来源。我们证明，对于最简单形式的统计模型，这个问题是NP完全的，即观察到的句子长度可能是指数的。我们将这种复杂性追溯到其他解码问题中不存在的因素。证明了当语言模型为二元模型时，精确解码问题是NP难问题。我们证明了SMT的解码问题以及一些双语拼接问题是NP完全的，因此在一般情况下不存在有效的算法。"}
{"pid": "J80-3003", "zh_sum": "基于计划的间接言语行为分析基于这样一个假设，即语言使用者可以识别他人所做的行为，推断所寻求的目标，并在实现目标的过程中进行合作，我们提出了对请求和告知的间接言语行为形式的描述。这种合作行为是有独立动机的，说话者可能有意也可能无意。如果听者认为这是有意的，他或她可以将言语行为视为间接的；否则直接解释。建议采用启发式方法来决定各种解释。"}
{"pid": "W06-3812", "zh_sum": "Chinese Whispers-一种高效的图聚类算法及其在自然语言处理问题中的应用我们介绍了Chinese Whispers，一种随机图聚类算法，其边缘数呈时间线性。在详细定义了该算法并讨论了其优缺点后，对汉语耳语在语言分离、句法词类习得和词义消歧等自然语言处理问题上的表现进行了测试。在这一点上，事实是，小世界性质适用于NLP中的许多图。介绍了基于共现的图聚类框架。"}
{"pid": "P05-1036", "zh_sum": "基于统计的摘要中句子压缩的监督和非监督学习-第一步：句子压缩，Knight和Marcu（Knight和Marcu，2000）（K&M）提出了一种用于句子压缩的噪声信道模型。使用这种方法的主要困难是缺乏数据；奈特和马库使用了1035个训练句子的语料库。更多的数据不容易获得，因此除了改进原始K&M噪声信道模型外，我们还创建了任务的无监督和半监督模型。最后，我们指出了以这种方式对任务建模的问题。他们提出了未来研究的领域。我们通过考虑上下文无关的语法派生和匹配扩展，来近似非平行语料库（例如，Penn Treebank）的压缩规则。我们认为，噪声信道模型不是一个合适的压缩模型，因为它使用的源模型是在未压缩句子上训练的，因此倾向于考虑压缩句子，而不是未压缩句子。我们表明，应用手工规则来剪裁句子可以提高内容和语言质量。"}
{"pid": "C94-1079", "zh_sum": "PRINCIPAR-一种高效的基于广泛覆盖原则的语法分析器我们为英语提供了一种高效的、广泛覆盖的、基于原则的语法分析器。该解析器已在C++中实现，并在带有X-windows的SUN Sparcstations上运行。它包含一个包含90000多个词条的词典，通过对机器可读词典中的词条应用一组提取和转换规则自动构建。我们发布了MiniPar，一个快速而健壮的语法依赖关系解析器。"}
{"pid": "E99-1001", "zh_sum": "没有地名录的命名实体识别通常认为，命名实体识别系统需要广泛的地名录-人员、组织、地点和其他命名实体的名称列表。事实上，这类地名录的编制有时被称为命名实体识别系统设计中的一个瓶颈。我们报告了一个命名实体识别系统，该系统将基于规则的语法与统计（最大熵）模型相结合。我们使用来自MUC-7竞赛的测试材料，对不同类型和不同大小的地名录的系统性能进行了报告。我们表明，就本次比赛的文本类型和任务而言，使用相对较小的知名地名词典，而不是使用频率较低的大型地名词典，就足够了。最后，我们对竞争和实验的领域独立性进行了观察。我们利用语篇层次对非预测性语境中的项目进行消歧。我们使用相对特殊的多阶段标记过程来利用文档中的标签一致性信息。"}
{"pid": "H05-1091", "zh_sum": "用于关系提取的最短路径依赖核我们提出了一种新的关系提取方法，该方法基于以下观察结果：在同一句子中，断言两个命名实体之间的关系所需的信息通常由依赖图中两个实体之间的最短路径捕获。从ACE（自动内容提取）报纸语料库中提取顶级关系的实验表明，新的最短路径依赖核优于最近基于依赖树核的方法。这项关于关系提取的工作表明，任何两个实体之间的最短依赖路径都可以捕获维护它们之间关系所需的信息。"}
{"pid": "N04-1015", "zh_sum": "抓住要点：概率内容模型及其在生成和摘要中的应用我们考虑在特定领域内，根据文本所涉及的主题以及这些主题出现的顺序，对文本的内容结构进行建模的问题。我们首先提出了一种有效的知识精益方法，用于从未注文档中学习内容模型，该方法利用了一种新的隐马尔可夫模型自适应算法。然后，我们将我们的方法应用于两个互补的任务：信息排序和提取摘要。我们的实验表明，与以前提出的方法相比，在这些应用程序中合并内容模型可以产生显著的改进。我们提出了一个域相关HMM模型来捕获文本中的主题转移，其中主题由隐藏状态表示，句子是观察值。"}
{"pid": "N04-1041", "zh_sum": "自动标记语义类自动发现语义类的系统已经出现，部分是为了解决WordNet和Cyc等广泛覆盖的词汇资源的局限性。目前最先进的技术发现了许多语义类，但未能标记它们的概念。我们提出了一种标记语义类的算法，并使用自顶向下的方法利用它们来提取is-a关系。在我们的系统中自动学习的关系包括同位语、名词性主题（如关系）和相似关系。我们的句法共现方法具有最坏的时间复杂度O（n2k），其中n是语料库中的单词数，k是特征空间。给定一个比Web文档集合更干净、更小的新闻文章集合，将对文档句子应用语法解析器，以识别和利用语法依赖性来选择候选类标签。"}
{"pid": "P99-1004", "zh_sum": "分布相似性测度我们研究分布相似性测度，目的是改进对不可见共现的概率估计。我们在主动语态和被动语态结构中都使用动宾关系。我们发现，我们的非对称偏斜散度（Kullback-Leibler散度的推广）最适合改进看不见单词共现的概率估计。"}
{"pid": "P06-1072", "zh_sum": "多语言加权语法归纳中的退火结构偏差我们首先展示了结构局部性偏差如何提高EM从未注实例训练的最新依赖语法归纳模型的准确性（Klein和Manning，2004）。接下来，通过退火控制该偏差的自由参数，我们实现了进一步的改进。然后，我们描述了另一种结构偏向，倾向于“破碎”的假设，即在分句上的部分结构，并显示了类似的改进模式。我们将这种方法与对比评估（Smith和Eisner，2005a）联系起来，将后者应用于六种语言的语法归纳，并表明我们的新方法比CE（绝对）提高了1-17%（比EM提高了8-30%），就我们所知，在这项任务上取得了迄今为止的最佳结果。我们的方法，结构退火，是一种广泛适用于隐藏结构发现问题的通用技术。我们在自然语言语法归纳任务中惩罚近似后验过度依赖结构，以避免单词之间的长距离依赖。我们提出了结构退火（SA），在这种退火中，对局部依赖附件的强烈偏见在学习的早期被强制执行，然后逐渐放松。我们的退火方法试图随着时间的推移改变E步中隐藏输出的空间，以促进M步中的学习。"}
{"pid": "N10-1115", "zh_sum": "一种简单的非定向依赖解析的有效算法我们提出了一种新的确定性依赖解析算法，该算法试图首先以非定向的方式在依赖结构中创建最简单的弧。传统的确定性语法分析算法基于一个shift-reduce框架：它们从左到右遍历句子，并在每一步执行一组可能的操作中的一个，直到构建一个完整的树。这种方法的一个缺点是它非常局部：虽然决策可以基于左边的复杂结构，但他们只能看右边的几个字。相反，我们的算法通过在每个解析步骤中迭代选择要连接的最佳邻居对来构建依赖树。这允许在附着点的左侧和右侧合并已构建结构的特征。解析器学习附件首选项和执行它们的顺序。结果是一个确定性的、最佳优先的O（nlogn）解析器，它比基于最佳优先转换的解析器精确得多，接近全局优化的解析模型的性能。我们观察到，解析时间主要由特征提取和分数计算决定。"}
{"pid": "P10-1044", "zh_sum": "选择偏好的潜在Dirichlet分配方法选择偏好的计算，关系的可容许参数值，是一个众所周知的NLP任务，具有广泛的适用性。我们提出了LDA-SP，它利用LinkLDA（Erosheva et al.，2004）来模拟选择偏好。LDA-SP通过同时推断潜在主题和主题在关系上的分布，结合了以前方法的优点：与传统的基于类的方法一样，它产生了描述每个关系偏好的人类可解释类，但在预测能力方面与非基于类的方法相竞争。我们将LDA-SP与几种最先进的方法进行比较，在0.9精度下，召回率比互信息提高了85%（Erk，2007）。我们还评估了LDA-SP在过滤推理规则的不当应用方面的有效性，其中我们显示了对Pantel等人的系统的重大改进（Pantel等人，2007）。我们专注于推断潜在主题及其在多个参数和关系上的分布（例如，动词的主语和直接宾语）。"}
{"pid": "D09-1030", "zh_sum": "快速廉价且富有创意：使用亚马逊评估翻译质量；人们普遍认为，机械式土耳其语翻译质量评估过于耗时和昂贵。我们探索了一种快速且廉价的方法，使用亚马逊的Mechanical Turk向大量非专家注释者支付小额款项。花10美元，我们就可以从WMT08翻译任务中重复创建判断。我们发现，当组合的非专家判断与现有的机器翻译质量黄金标准判断高度一致时，与专家判断的相关性比Bleu更强。我们进一步证明，Mechanical Turk可以用来计算人工翻译编辑率（HTER），用机器翻译进行阅读理解实验，并创建高质量的参考译文。我们发现懒惰的注释者倾向于停留更长的时间，做更多的注释。我们将评估视为一个加权投票问题，其中每个注释者的贡献通过与金标准或其他注释者的协议进行加权。我们展示了众包作为完成劳动密集型自然语言处理任务的一种方法的有效性。"}
{"pid": "N07-1051", "zh_sum": "改进的非exicalized解析推理我们对使用分层状态分割PCFGs的非exicalized解析进行了一些改进。首先，我们提出了一种新的从粗到精的方法，其中使用语法自身的层次投影进行增量修剪，包括一种在没有树库的情况下高效计算语法投影的方法。在我们的实验中，层次剪枝极大地加快了解析速度，而不会损失经验准确性。其次，我们从风险最小化的角度比较了状态分割PCFG的各种推理程序，特别注意它们的实际权衡。最后，我们进行了多语言实验，结果表明，在多语言和多领域中，即使没有任何特定于语言的调整，使用层次状态分裂的解析也可以快速而准确地进行。一种有效的从粗到精的语法分析方法是使用基于似然的分层EM训练。"}
{"pid": "P01-1067", "zh_sum": "基于语法的统计翻译模型我们提出了一个基于语法的统计翻译模型。我们的模型通过在每个节点应用随机操作，将源语言解析树转换为目标语言字符串。这些操作捕获了语序和大小写标记等语言差异。使用EM算法在多项式时间内估计模型参数。该模型产生的单词对齐比IBM model 5产生的更好。我们使用目标语言中的解析器来训练一组609个操作的概率，这些操作将目标解析树转换为源字符串。我们提出了一种算法，用于估计模型的概率参数，该模型将翻译表示为语法树中节点子节点上的一系列重新排序操作，使用初始树结构的自动解析器输出。"}
{"pid": "P95-1034", "zh_sum": "两级多路径生成大规模自然语言生成需要整合大量知识：词汇、语法和概念。一个健壮的生成器必须能够很好地运行，即使缺少一些知识。它还必须对不完整或不准确的输入具有鲁棒性。为了解决这些问题，我们构建了一个混合生成器，其中符号知识的缺口由统计方法填补。我们描述了算法并给出了实验结果。我们还讨论了如何使用混合发电模型简化电流发生器并增强其可移植性，即使原则上可以获得完美的知识。我们使用了一种抽样技术，从基础语法创建的数据结构中抽取一组句子，并根据它们满足交际目标的程度进行排序。"}
{"pid": "W07-2016", "zh_sum": "SemEval-2007任务17：英语词汇样本SRL和所有单词本文描述了我们在准备数据和评估SemEval-2007任务17的三个子任务（词汇样本、语义角色标记（SRL）和所有单词）的结果方面的经验。我们将参与系统的结果制成表格并进行分析。粗粒度感知组的使用（Palmer et al，2007）已经在WSD性能方面取得了相当大的进步，准确率约为90%。"}
{"pid": "P05-1065", "zh_sum": "使用支持向量机和统计语言模型评估阅读水平阅读能力是语言能力的基本组成部分。然而，为外语和第二语言学习者找到合适阅读水平的主题文本对教师来说是一个挑战。这项任务可以通过自然语言处理技术来评估阅读水平。现有的阅读水平测量方法并不适合这项任务，但之前的工作和我们自己的试点实验表明了使用统计语言模型的好处。在本文中，我们还使用支持向量机将来自传统阅读水平度量、统计语言模型和其他语言处理工具的特征结合起来，以产生一种更好的评估阅读水平的方法。我们开发了一个SVM分类器，该分类器基于三元语言模型（每种难度一个）、一些解析特征（如平均树高）和传统上用于可读性的变量。我们使用句法特征，如解析树的高度或被动句的数量来预测阅读成绩。"}
{"pid": "P88-1020", "zh_sum": "规划连贯的多中心文本尽管大多数文本生成器能够简单地将多个句子串在一起，但它们无法确定哪个顺序可以确保段落连贯。当连续句子中的信息遵循听者熟悉的推理模式或知识模式时，段落是连贯的。为了表示这种推论，说话者通常使用以固定方式连接连续句子的关系。曼恩和汤普森的修辞结构理论提出了一组20种关系，它们跨越了人们通常在英语中所说的大部分内容。本文描述了这些关系的形式化及其在原型文本规划器中的使用，该原型文本规划器将输入元素组织成连贯的段落。我们使用平面操作符来创建连贯的文本延伸。"}
{"pid": "P07-1107", "zh_sum": "无监督的共指消解在非参数贝叶斯模型中，我们提出了一种无监督的非参数贝叶斯共指消解方法，该方法对语料库中的全局实体身份以及每个文档中的顺序回指结构进行建模。虽然大多数现有的共指工作是由成对决策驱动的，但我们的模型是完全生成的，从全局实体属性和局部注意状态的组合中产生每一个提及。尽管没有监督，但我们的系统在MUC-6测试集上实现了70.3 MUC F1测量，大致在一些最近监督的结果范围内。在我们的模型中，我们使用代词、名词和专有名词之间的区别。我们通过回指消解来评估DPMMs的聚类特性，取得了良好的结果。"}
{"pid": "P96-1024", "zh_sum": "解析算法和度量存在许多不同的度量来评估解析结果，包括维特比、交叉括号率、零交叉括号率以及其他一些度量。然而，大多数解析算法，包括维特比算法，都试图优化相同的度量，即获得正确标记树的概率。通过选择适合评估指标的解析算法，可以获得更好的性能。我们提出了两种新算法：“带标签的召回算法”，它最大化了预期的带标签召回率，和“带括号的召回算法”，它最大化了带括号的召回率。实验结果表明，这两种新算法在许多准则上都优于维特比算法，尤其是在优化的准则上。我们观察到，维特比分析通常不是基于分析中正确成分数量的评估指标（如f分数）的最佳分析。"}
{"pid": "P11-1138", "zh_sum": "维基百科的局部和全局消歧算法以上下文敏感的方式消除概念和实体的歧义是自然语言处理中的一个基本问题。维基百科的全面性使在线百科全书成为越来越受欢迎的消歧目标。对Wikipedia的消歧类似于传统的词义消歧任务，但不同之处在于Wikipedia链接结构提供了有关哪些消歧兼容的其他信息。在这项工作中，我们分析了利用这些信息对给定文档进行一致消歧的方法（我们称之为“全局”方法），并将其与更传统的（局部）方法进行比较。我们表明，以前的全局消歧方法可以改进，但即使如此，局部消歧也提供了一个很难超越的基线。"}
{"pid": "P02-1046", "zh_sum": "Bootstrapping本文对协同训练的分析进行了细化，定义并评估了一种新的具有理论合理性的协同训练算法，给出了Yarowsky算法的理论合理性，并表明协同训练和Yarowsky算法基于不同的独立性假设。我们表明，独立性假设可以放宽，在较弱的独立性假设下，联合训练仍然有效。我们通过使用一个新的约束放松视图独立性假设来改进Dasgupta等人的结果。我们提出了贪婪协商算法，该算法基于数据的两个独立视图，从一组手工输入的种子规则中学习两个二进制分类器。我们证明，如果满足分类器规则之间的某些独立条件，并且每个规则的精度大于阈值T，则最终分类器的精度大于T。我们认为，条件独立假设非常强，在实际数据集中很少满足，这表明较弱的独立假设就足够了。"}
{"pid": "P05-1053", "zh_sum": "在关系抽取中探索各种知识提取实体之间的语义关系是一项挑战。本文研究了支持向量机在基于特征的关系提取中融合不同的词汇、句法和语义知识。我们的研究表明，基本短语组块信息对于关系提取非常有效，并有助于从句法方面提高大部分性能，而来自完整语法分析的附加信息只能提供有限的进一步增强。这表明，用于关系提取的完整解析树中的大多数有用信息都很浅，可以通过分块来捕获。我们还演示了如何将语义信息（如WordNet和名称列表）用于基于特征的关系提取，以进一步提高性能。对ACE语料库的评估表明，有效地结合不同的特征可以使我们的系统在24种ACE关系子类型上优于以前最好的报告系统，并且在5种ACE关系类型上显著优于基于树核的系统，在F-测度上超过20。我们发现，很难提取新的有效特征来进一步提高提取精度。我们使用一组平面特征（即单词、实体类型、提及级别、重叠、基本短语分块、依赖树、解析树和语义信息）。"}
{"pid": "P92-1017", "zh_sum": "部分括号语料库的内外重估计将推断随机上下文无关语法参数的内外算法进行了扩展，以利用部分解析语料库中的成分信息（成分括号）。在形式化和自然语言解析语料库上的实验表明，新算法比原算法具有更快的收敛速度和更好的层次结构建模能力。特别是，对于通过我们的算法从航空旅行信息系统口语语料库中句子的手动解析词性字符串训练集推断出的语法，测试集括号的准确率达到90%以上。最后，当提供足够的括号时，新算法比原算法具有更好的时间复杂度。我们采用内-外算法来应用于从Penn树库中提取的半监督数据。"}
{"pid": "H05-2018", "zh_sum": "OpinionFinder：主观性分析系统我们提供了主观性词典。我们提供了一个预先训练好的分类器，用于用上下文极性值注释句子中的短语。"}
{"pid": "C04-1072", "zh_sum": "ORANGE：一种评估机器翻译自动评估指标的方法机器翻译自动评估指标的比较通常在语料库级别上进行，使用相关统计数据，如Pearson的产品矩相关系数或Spearman的人类得分与自动得分之间的秩序相关系数。然而，这种比较依赖于人类对翻译质量的判断，如充分性和流利性。不幸的是，这些判断往往前后矛盾，获取成本也很高。在本文中，我们介绍了一种新的评估方法ORANGE，用于自动评估自动机器翻译评估指标，除了使用一组参考翻译之外，无需额外的人工参与。我们还显示了使用ORANGE比较几个现有自动度量和三个新的自动度量的结果。BLEU是平滑的（Lin和Och，2004b），它只考虑匹配到bi-gram，因为这与人类判断的相关性比包含高阶n-gram时更高。使用“每句平滑”BLEU作为相似性度量。"}
{"pid": "P06-1043", "zh_sum": "解析器自适应的重新排序和自我训练在《宾夕法尼亚华尔街日报》（WSJ）树库上训练和测试的统计解析器在过去10年中取得了巨大的进步。然而，这种改进很大程度上是基于（通常）WSJ树库数据上需要训练的特征数量不断增加。这导致了人们的担忧，即这些解析器可能会对该语料库进行过精细的调整，而牺牲了对其他体裁的可移植性。这种担忧是有道理的。标准的“Charniak解析器”在宾州华尔街日报的测试集上以89.7%的标记精度召回f-度量进行了检验，但在Brown treebank语料库的测试集上仅为82.9%。这篇论文应该可以消除这些担忧。特别是，我们展示了Charniak和Johnson（2005）中描述的重新排序解析器将Brown上的解析器的性能提高到85.2%。此外，使用（McClosky et al.，2006）中描述的自我训练技术，在不使用任何标记棕色数据的情况下，再次将其提高到87.8%（误差减少28%）。我们利用现有的未标记数据成功地将自训练应用于句法分析，并将相同的技术应用于句法分析自适应，取得了显著的效果。"}
{"pid": "D09-1001", "zh_sum": "无监督语义分析我们提出了第一种使用马尔可夫逻辑的无监督方法来学习语义分析器。我们的USP系统将依赖树转换为准逻辑形式，递归地从这些树中归纳出lambda形式，并对它们进行聚类以抽象出相同含义的语法变体。通过递归地将句子的各个部分分配到lambda形式的聚类中并进行组合，得到句子的映射语义分析。我们通过使用它从生物医学摘要中提取知识库并回答问题来评估我们的方法。USP在这项任务的精确度和召回率方面都大大优于TextRunner、DIRT和知情基线。我们考虑一种语义分析设置，其目标是（1）将句子的句法依赖树分解为片段，（2）将每个片段分配给语义等价的句法结构簇，以及（3）预测片段之间的谓词-参数关系。我们使用马尔可夫逻辑网络（MLN）（Richardson和Domingos，2006）对依赖树及其潜在语义表示的联合概率进行建模，选择参数（一阶子句的权重）以最大化观察到的依赖结构的概率。我们将参数分组，并在每个组内施加局部规范化约束。"}
{"pid": "E06-1015", "zh_sum": "使树核实用于自然语言学习近年来，树核被提出用于自然语言应用的自动学习。不幸的是，它们显示出（a）固有的超线性复杂性和（b）比传统的属性/值方法精度更低。在本文中，我们证明了树核在自然语言处理中非常有用，因为（a）我们提供了一种计算线性平均运行时间中树核的简单算法，（b）我们对不同树核的分类特性的研究表明，核组合总是改进了传统方法。支持向量机在谓词论元分类任务上的实验为本文提供了实证支持。我们介绍了树核的一种快速实现，其中首先为与相同产生式规则相关联的树核构造节点对集。"}
{"pid": "J93-2006", "zh_sum": "从1990年春季到1991年秋季，我们通过概率模型处理歧义和未知单词，进行了一系列小型实验，以测试用概率模型补充基于知识的技术的有效性。本文报告了我们在预测高度歧义词的词性、当一个以上的解释满足所有已知的句法和语义约束时预测话语的预期解释以及从示例使用中学习动词的格框信息方面的实验。通过这些实验，我们确信基于标注语料库的概率模型可以有效地减少文本处理过程中的歧义，并可以通过补充基于知识的技术从语料库中获取词汇信息。基于这些实验的结果，我们构建了一个新的自然语言系统（PLUM），用于从文本（例如新闻专线文本）中提取数据。我们的模型在概率模型中结合了对未知单词的处理。"}
{"pid": "W04-3111", "zh_sum": "生物医学信息提取的综合注释我们描述了生物医学信息提取的两个领域，药物开发和癌症基因组学的方法。我们开发了一个包含多层次语料库注释的框架：一个包含句法结构的树库，一个包含谓词论元结构的Propbank，以及实体和实体之间关系的注释。这种方法的关键是将实体恰当地描述为关系组件，这允许实体注释与语法结构集成，同时保留注释和提取更复杂事件的能力。我们正在培训使用此注释进行此类提取的统计标记者，并使用它们改进注释过程。"}
{"pid": "P99-1032", "zh_sum": "主观性分类金标准数据集的开发和使用本文介绍了一个使用统计技术分析和提高语篇标注中的语码间可靠性的案例研究。Biascorrected标签被制定出来，并成功地用于指导编码手册的修订和开发自动分类器。我们使用一个句子级的朴素贝叶斯分类器，将特定句法类别（代词、形容词、基数、情态动词、副词）、标点和句子位置的存在与否作为特征。我们把主观性句子定义为表达评价、观点、情感和推测的句子。"}
{"pid": "C04-1081", "zh_sum": "中文分词和条件随机场新词检测中文分词是一个困难的、重要的、被广泛研究的序列建模问题。本文通过提供一个易于支持多个字词词典形式的领域知识集成的原则性框架，证明了线性链条件随机场（CRF）能够执行稳健而准确的中文分词。我们还提出了一种概率新词检测方法，进一步提高了性能。我们的系统在最近一次汉语分词比赛中使用的四个数据集上进行了评估。获得了最先进的性能。CRFs在中文信息处理方面的优势也体现在分词上）。CRF是Lafferty等人（2001）提出的一种统计序列建模框架，我们将其用于中文分词任务，将分词视为一个二元决策任务。我们首先将此框架用于中文分词，将其视为一个二元决策任务，这样每个字符都被标记为一个单词的开头或一个单词的延续。我们将分词问题定义为将每个字符标记为当前字符的前一个字符边界是否为单词边界。"}
{"pid": "J01-4004", "zh_sum": "名词短语共指消解的机器学习方法本文提出了一种无限制文本中名词短语共指消解的学习方法。该方法从一个小的带注释的语料库中学习，任务不仅包括解析某种类型的名词短语（例如代词），还包括解析一般的名词短语。它也不限制名词短语的实体类型；也就是说，无论是“组织”、“个人”还是其他类型，都会分配共同引用。我们在常用数据集（即MUC-6和MUC-7共指语料库）上评估了我们的方法，并获得了令人鼓舞的结果，表明在一般名词短语共指任务中，学习方法有希望，并且达到了与非学习方法相当的准确性。我们的系统是第一个基于学习的系统，在这些数据集上提供与最先进的非学习系统相当的性能。我们包括所有由NP标识符返回的名词短语，并报告MUC-6数据的F测度为62.6%，MUC-7数据的F测度为60.4%。我们通过学习确定每个提及之前的提及（如果有的话）属于同一等价类来构建这个实体提及图；这种方法通常被称为成对共指模型。"}
{"pid": "W96-0208", "zh_sum": "词义消歧对比实验：偏倚在机器学习中的作用本文描述了七种不同学习算法在学习从上下文中消除词义歧义问题上的实验比较。测试的算法包括统计、神经网络、决策树、基于规则和基于案例的分类技术。测试的具体问题包括使用当前句子和进行中句子中的单词作为上下文，消除单词“line”的六种含义的歧义。统计和神经网络方法在这一特定问题上表现最好，我们讨论了造成这种差异的潜在原因。我们还讨论了偏差在机器学习中的作用及其在解释特定问题上观察到的性能差异方面的重要性。我们认为，朴素贝叶斯分类和感知器分类器特别适合词汇样本词义消歧问题，因为它们结合了所有特征的加权证据，而不是选择特征子集进行早期识别。由Unigram组成的单词袋特征集在文本分类和词义消歧方面有着悠久的成功历史（Mooney，1996），我们相信，尽管产生了相当多的噪声，但仍然可以为识别提供有用的信息。"}
{"pid": "W07-2018", "zh_sum": "SemEval-2007任务19：框架语义结构提取此任务包括识别唤起FrameNet项目中定义的语义框架的单词和短语(http://framenet.icsi.berkeley.edu)，以及它们的语义依赖项，这些语义依赖项通常是，但并不总是，它们的句法依赖项（包括主语）。训练数据为FN注释句。在测试中，参与者自动标注三个以前看不见的文本，以匹配金标准（人类）标注，包括预测以前看不见的框架和角色。在框架和FEs标签匹配以及基于注释的语义依赖树匹配方面，都测量了精确度和召回率。我们的共享任务表明，运行文本的框架语义SRL是一个难题，部分原因是运行文本必然包含许多框架，而这些框架没有或几乎没有可用的带注释的训练数据。"}
{"pid": "J97-2003", "zh_sum": "语言和语音处理中的有限状态传感器有限状态机已应用于自然语言处理的各个领域。这里我们考虑使用一种支持非常有效程序的传感器：顺序传感器。我们回顾了经典定理，并给出了表征连续串-串变换器的新定理。输出权重的传感器在语言和语音处理中也起着重要作用。我们对弦-权变换器进行了具体研究，包括非常有效地确定和最小化这些变换器的算法，以及允许确定的变换器的特征和相应的算法。描述并举例说明了这些算法在语音识别中的一些应用。本文对加权串传感器（WST）级联的应用进行了深入研究。"}
{"pid": "W05-1506", "zh_sum": "更好的K-Best解析我们讨论了K-Best解析与自然语言处理中最新应用的相关性，并在超图解析框架下开发了有效的K-Best树算法。为了证明这些算法的效率、可扩展性和准确性，我们对比克尔实现柯林斯的词汇化PCFG模型和蒋的基于CFG的分层短语翻译解码器进行了实验。我们特别展示了我们算法的改进输出如何有可能改进解析重排序系统和其他应用程序的结果。"}
{"pid": "P06-4020", "zh_sum": "RASP系统的第二个版本我们描述了RASP（鲁棒精确统计分析）系统的新版本，该系统设计用于自由文本的语法注释。新版本包括一个修改后的和更具语义动机的输出表示，一个增强的语法和词性标记词典，以及一个更灵活和半监督的结构化语法排序模型训练方法。我们使用关系评估方案评估《华尔街日报》上发布的版本，并描述新版本如何允许用户使用（域内）词汇信息提高性能。"}
{"pid": "J99-1003", "zh_sum": "无论是在私人数据仓库还是在万维网上公开访问的网站上，通过两种语言（bitexts）提供的模式识别文本进行的Bitext映射和对齐正变得越来越丰富。与其他类型的数据一样，bitext的价值在很大程度上取决于可用数据挖掘工具的效率。从bitext中提取有用信息的第一步是在其两半中找到相应的单词和/或文本段边界（bitext映射）。本文通过从模式识别的角度来描述这个问题，从而提高了双文本映射的技术水平。从这个角度来看，双文本映射算法的成功取决于它执行三项任务的能力：信号生成、噪声过滤和搜索。这里介绍的平滑内射地图识别器（SIMR）算法将创新方法集成到这些任务中。客观评估表明，对于法语/英语和韩语/英语等多种语言对，SIMR的准确率始终很高。如有必要，可以使用几何线段对齐（GSA）算法将S IMR的位图有效地转换为线段对齐，本文也介绍了该算法。SIMR已经为超过200兆字节的法文-英文比特制作了比特地图。GSA已将这些地图转换为路线。地图和路线均可从语言数据协会获得。我们通过将最长公共子序列的长度除以较长字符串的长度来规范化LCS，并将其称为最长公共子序列比（LCSR）。"}
{"pid": "P04-1054", "zh_sum": "关系抽取中的依赖树核我们扩展了以前对树核的研究，以估计句子依赖树之间的相似度。在支持向量机中使用该核，我们可以检测和分类自动内容提取（ACE）新闻文章语料库中实体之间的关系。我们研究了Wordnet超词、词类和实体类型等不同功能的实用性，发现依赖树内核比一包单词内核实现了20%的F1改进。为了比较两个实例句子中的关系，我们建议比较由关系参数诱导的子树，即计算依赖树中两个最低共同祖先（lca）之间的节点核。我们还使用依赖树内核来检测自然语言文本中的命名实体类。"}
{"pid": "C90-3052", "zh_sum": "类型化统一语法我们引入TFS，这是逻辑形式主义中的一种计算机形式主义，它集成了一个强大的类型系统。其基本数据结构是类型化特征结构。类型系统通过提供多重继承机制和推理机制来鼓励面向对象的语言描述方法，从而允许指定定义为对象类的语言描述级别之间的关系。我们从一个非常简单的DCG开始说明这种方法，并说明如何利用类型化系统来实施一般约束和模块化语言描述，以及进一步的抽象如何导致类似HPSG的语法。所提出的方法不可避免地导致数据结构变得稍微复杂。"}
{"pid": "N06-1058", "zh_sum": "自动评估中的释义本文研究了释义对自动评估准确性的影响。给定一个参考句子和一个机器生成的句子，我们试图找到一个在措辞上比原始参考更接近机器输出的参考句子的释义。我们将我们的释义方法应用于机器翻译评估中。我们的实验表明，使用复述合成参考语可以提高自动评估的准确性。我们还发现，人类判断的自动释义的质量与其对自动评估的贡献之间存在着密切的联系。我们表明，通过将Wordnet中的同义词替换为原始参考句来创建合成参考句，可以增加机器翻译系统输出的精确单词匹配数，并显著改善BLEU（Papineni et al.，2002）分数与人类翻译充分性判断的相关性。"}
{"pid": "P06-1109", "zh_sum": "无监督句法分析的全子树方法我们研究了无监督句法分析的全子树“DOP”方法的推广。无监督DOP模型将所有可能的二叉树分配给一组句子，然后使用这些二叉树中的所有子树（大型随机子集）计算最可能的解析树。我们将测试无监督DOP的相对频率估计器和已知统计一致的最大似然估计器。我们报告了英语（WSJ）、德语（NEGRA）和汉语（CTB）数据的最新结果。据我们所知，这是第一篇在《华尔街日报》上测试DOP最大似然估计量的论文，得出了一个令人惊讶的结果，即无监督解析模型优于广泛使用的监督模型（树库PCFG）。我们发现，一个非二进制的树库语法在《华尔街日报》的句子上平均达到72.3%的f分数？40个单词，而二进制版本的f分数仅为64.6%。"}
{"pid": "P08-1064", "zh_sum": "基于树序列比对的树到树翻译模型本文提出了一种基于树序列比对的翻译模型，其中树序列是指覆盖短语的单个子树序列。该模型利用了基于短语和基于语言语法的方法的优点。它自动从单词对齐的双叉平行文本中学习具有映射概率的对齐树序列对。与以往的模型相比，该模型不仅能捕捉到具有语言结构特征的非句法短语和不连续短语，而且支持更大跨度的树型结构的多级结构重组。这使得我们的模型比其他报道的模型具有更强的表达能力。NIST MT-2005汉英翻译任务的实验结果表明，我们的方法在统计上显著优于基线系统。我们的方法生成源解析树或林中每个节点根的所有可能的树片段，然后将所有生成的树片段与翻译规则的源部分（左侧）匹配，以提取有用的规则。"}
{"pid": "J96-2004", "zh_sum": "评估分类任务的一致性：Kappa统计目前，在话语和对话领域工作的计算语言学家和认知科学家认为，使用几种不同的统计数据，他们的主观判断是可靠的，这些统计数据都不容易解释或相互比较。与此同时，内容分析的研究人员已经经历了同样的困难，并在kappa统计中找到了解决方案。我们讨论了目前在计算语言学和认知科学中用于话语和对话工作的可靠性指标的错误，并认为，如果我们在外地采用内容分析技术，我们会更好。我们的方法，卡帕统计，被广泛用于话语的实证研究（Carletta，1996）。"}
{"pid": "N10-1056", "zh_sum": "为了简单起见：从维基百科中无监督地提取词汇简化我们报告了在提取词汇简化方面正在进行的工作（例如，“协作”→ “协同工作”），重点是利用简单英文维基百科中的编辑历史来完成此任务。我们考虑两种主要方法：（1）通过解释不同操作混合的编辑模型来推导简化概率，（2）使用元数据来关注更可能是简化操作的编辑。我们发现我们的方法优于合理的基线，并产生许多高质量的词汇简化，这些简化没有包含在独立创建的手动准备的列表中。我们学习词汇简化，而不考虑句法语境。"}
{"pid": "P00-1056", "zh_sum": "改进的统计对齐模型在本文中，我们提出并比较了用于统计机器翻译的各种基于单字的对齐模型。我们讨论了五种IBM对齐模型、隐马尔可夫对齐模型、平滑技术和各种修改。我们提供了不同的方法来组合路线。作为评估标准，我们使用产生的维特比比对的质量，与手动生成的参考比对进行比较。我们表明，与简单模型IBM-1或IBM-2相比，具有一阶依赖性和生育率模型的模型可以获得更好的结果，而简单模型IBM-1或IBM-2不能超越零阶依赖性。校准错误率（AER）测量自动校准与参考校准不同的链接比例。"}
{"pid": "J00-4003", "zh_sum": "基于经验的确定描述处理系统我们提出了一个在任意域中处理确定描述的实现系统。该系统的设计基于先前报道的语料库分析结果，该分析突出了报纸语料库中话语新描述的普遍性。注释语料库被用来广泛评估所提出的将明确描述与其先行词匹配、话语分割、识别新的话语描述以及为衔接描述建议锚定的技术。在解析具有完整词头的明确名词短语时，一个主要障碍是其中只有一小部分是回指的（约30%）。在我们的系统中，通过查阅WordNet来获取同义、超义和半义关系，以解决确定回指问题。我们将每个明确的描述分为直接回指、话语新描述或衔接描述。我们通过在逗号之间添加修饰语来区分限制性和非限制性后修饰，这不应归类为链起始。对于语篇新分类任务，该模型最重要的特征是要分类的NP的首词是否已经出现过。"}
{"pid": "C04-1100", "zh_sum": "基于语义结构的问答回答自然语言中提出的复杂问题的能力取决于（1）可用语义表示的深度和（2）它们支持的推理机制。在本文中，我们描述了一个QA体系结构，其中通过1）从输入中识别谓词参数结构和语义框架，以及2）在域和场景模型的上下文中使用提取的关系执行结构化概率推理，来分析问题和生成候选答案。我们系统的一个新方面是基于协调概率关系模型（CPRM）的动作和事件的可伸缩和表达表示。在本文中，我们报告了所实现的系统执行多种形式的概率和时间推理以提取复杂问题答案的能力。结果表明，与目前最先进的Q/A系统相比，提高了精度。我们探讨了语义结构在问答中的作用。我们证明了问答可以从广泛的语义处理中受益。我们的问答系统以PropBank/FrameNet注释作为输入，使用PropBank目标指示哪些动作是用哪些参数描述的，并使用动作的概率模型作为推理工具生成答案。"}
{"pid": "P98-1035", "zh_sum": "利用句法结构进行语言建模本文提出了一种语言模型，该模型开发句法结构，并使用它从单词历史中提取有意义的信息，从而支持使用长距离依赖。该模型将概率分配给每一个单词的联合序列二进制解析结构和词头注释，并以从左到右的方式进行操作，因此可用于自动语音识别。介绍了该模型及其概率参数化和一组旨在评估其预测能力的实验；实现了对标准三角图建模的改进。我们选择由移位-约简解析器确定的前两个成分的词法头，发现这比三元模型更有效。我们通过将部分短语结构分配给历史词和渗透中心词来确定语言相关词。"}
{"pid": "P08-1090", "zh_sum": "在1970-80年代，叙事事件链的无监督学习-手工编码脚本被用作知识支柱，支持推理和其他需要深层语义知识的NLP任务。我们建议从原始新闻专线文本中无监督地归纳出类似的图式，称为叙事事件链。叙事事件链是由一个共同的主人公关联的一组部分有序的事件。我们描述了学习叙事事件链的三步过程。第一种方法使用无监督的分配方法来学习共享共同传递论点的事件之间的叙事关系。第二种方法应用时间分类器对连接的事件进行部分排序。最后，第三个从事件空间修剪和聚类自包含的链。我们介绍了两种评价方法：一种是用叙述完形填空来评价事件关联性，另一种是用顺序连贯任务来评价叙述顺序。我们发现，叙事预测比基线提高了36%，时间连贯性提高了25%。我们使用句法位置之间的点式互信息（PMI）研究叙事事件序列的无监督学习。"}
{"pid": "P09-1116", "zh_sum": "用于区分学习的短语聚类我们提出了一种简单且可扩展的算法，用于对数千万个短语进行聚类，并将得到的聚类作为区分分类器的特征。为了证明该方法的强大性和通用性，我们将该方法应用于两个截然不同的应用：命名实体识别和查询分类。我们的结果表明，短语聚类比单词聚类有显著的改进。我们的NER系统在广泛使用的CoNLL基准上实现了最佳的当前结果。我们的查询分类器与KDDCUP 2005中的最佳系统不相上下，无需借助劳动密集型知识工程。我们探索了一种基于两阶段聚类的方法：首先对短语进行聚类，然后依靠有监督的学习者识别有用的聚类，并为聚类特征分配适当的权重。我们提出了一种基于MapReduce的类K-means非层次短语聚类算法。"}
{"pid": "P86-1031", "zh_sum": "在语篇代词消解的中心化方法中，提出了以约束为中心的属性共享约束。这种“属性共享”约束要求在相邻话语中保留相同Cb的两个代词表达式共享某个共同的语法属性。这一性质在日语和英语语篇中都沿着语法功能主语的维度表达，其中不同的代词形式主要用于实现Cb。它在日语中是零代词，在英语中是（非重音）显性代词。由此产生的约束补充了原来的中心，解释了其明显的违反，并为多代词话语的解释提供了解决方案。它还提供了另一种解释回指的解释，似乎是由于结构的平行性。这种对中/对焦和平行度的协调是一个主要优势。然后，我将在约束中添加另一个维度，称为“说话人识别”，以处理日语话语中的一组特殊情况。它表明中心和说话者的观点之间有着密切的联系，并揭示了感知报告对代词分辨率的影响。这些结果，通过利用两种截然不同语言的事实，证明了中心框架的跨语言适用性。"}
{"pid": "P94-1002", "zh_sum": "解释性文本的多段落分割本文描述了文本拼接，一种将解释性文本划分为连贯的多段落话语单元的算法，它反映了文本的副标题结构。该算法使用与领域无关的词频和分布信息来识别多个同时出现的主题之间的交互。描述并显示了该算法的两个完全实现版本，它们生成的分割结果与人类对十三篇长文本的主要副标题边界的判断很好地对应。我们根据词空间向量的相似度计算文本单元之间的相似度。TextTiling能够将消息划分为多段落段，总体精确度为83%，召回率为78%。"}
{"pid": "P04-1053", "zh_sum": "从大型语料库中发现命名实体之间的关系发现文档中嵌入的重要关系不仅对于信息检索，而且对于问答和摘要都非常有用。然而，以前的关系发现方法需要大量带注释的语料库，这需要花费大量的时间和精力。我们提出了一种从大型语料库中发现关系的无监督方法。其核心思想是根据命名实体之间上下文词的相似性对命名实体对进行聚类。我们使用一年的报纸进行的实验表明，不仅能够以较高的召回率和精确度检测命名实体之间的关系，而且能够自动为这些关系提供适当的标签。我们介绍了一个完全无监督的开放IE系统，它基于实体对上下文的聚类来生成共享相同关系的实体对集群。我们使用大型语料库和扩展命名实体标记器来发现新的关系及其参与者。"}
{"pid": "W03-1719", "zh_sum": "第一届国际汉语分词Bakeoff本文介绍了2003年由ACL SIGHAN赞助的第一届国际汉语分词Bakeoff会议的结果，该会议与日本札幌第二届SIGHAN汉语处理研讨会一起报告。我们给出了举办国际细分比赛的动机（考虑到迄今为止中国国内已经有两次比赛），并报告了第一次国际比赛的结果，分析了这些结果，并对未来提出了一些建议。"}
{"pid": "W06-1651", "zh_sum": "意见识别中实体和关系的联合提取我们提出了一种在意见识别和分析的背景下联合提取实体和关系的方法。我们确定了两种与意见相关的实体——意见的表达和意见的来源——以及它们之间存在的链接关系。受Roth和Yih（2004）的启发，我们采用整数线性规划方法来解决联合意见识别任务，并表明基于全局约束的推理可以显著提高关系提取和意见相关实体提取的性能。加入语义角色标记系统后，性能进一步提高。生成的系统在实体和关系提取方面分别实现了79和69的F-度量，大大改善了该领域的先前结果。我们提出了一种ILP方法来联合识别意见持有人、意见表达及其IS-FROM链接关系，并证明了联合推理的有效性。其他人则扩展了标记级方法，以共同识别意见持有人（Choi等人2006年），并确定意见表达的极性和强度（Choi和Cardie，2010年）。"}
{"pid": "N06-1056", "zh_sum": "基于统计机器翻译的语义分析学习我们提出了一种新的语义分析统计方法WASP，用于构建句子完整的形式化语义表示。语义分析器是在给定一组句子并用其正确的意义表示进行注释的情况下学习的。WASP的主要创新在于它使用了最先进的统计机器翻译技术。单词对齐模型用于词汇获取，而解析模型本身可以看作是基于语法的翻译模型。我们表明，与需要类似数量监督的现有学习方法相比，WASP在准确性和覆盖率方面表现良好，并且对任务复杂性和词序的变化表现出更好的鲁棒性。我们使用最大熵模型，该模型定义了给定一个观察到的NL语句的派生上的条件概率分布。"}
{"pid": "W03-0404", "zh_sum": "使用提取模式引导学习主观名词我们探索了创建一个主观分类器的想法，该分类器使用通过引导算法学习的主观名词列表。我们的研究目标是开发一个能够区分主客观句的系统。首先，我们使用两种利用提取模式学习主观名词集的自举算法。然后，我们使用先前研究中确定的主观名词、话语特征和主观线索来训练朴素贝叶斯分类器。bootstrapping算法学习了1000多个主观名词，主观分类器表现良好，实现了77%的召回率和81%的准确率。我们使用手动派生的模式模板通过引导提取主观名词。我们使用两种自举算法从未注文本中挖掘主观名词，这两种算法利用词汇句法提取模式和手动选择的主观种子。"}
{"pid": "W05-0904", "zh_sum": "机器翻译评价的句法特征机器翻译的自动评价基于计算系统输出和人类参考译文之间的n-gram相似度，彻底改变了机器翻译系统的发展。我们探讨了在计算输出和引用之间的相似度时，如何使用句法信息，包括成分标签和头部修改器依赖关系。我们的结果表明，在评价指标中添加句法信息可以提高句子和语料库水平与人类判断的相关性。我们测量机器翻译输出和参考翻译之间的句法相似性。我们使用语法结构和依赖信息来超越表层匹配。"}
{"pid": "C96-1005", "zh_sum": "基于概念密度的词义消歧本文提出了一种基于布朗语料库的名词词义歧义消解方法及其自动评价方法。该方法依赖于use oil的WordNet的广覆盖名词分类法和概念之间概念距离的概念，通过为此目的开发的概念密度公式获得。这种全自动的方法不需要手动编码词条，手动标记文本，也不需要任何培训过程。实验结果已根据SemCor自动评估，SemCor是棕色语料库的感官标记版本。我们的概念密度（CD）是一种灵活的语义相似度，它依赖于词义的概括，而不是指任何固定的层次。我们使用了一个概念距离公式，该公式对连接相关概念的最短路径的长度、层次的深度和层次中概念的密度敏感。为了克服链接距离变化的问题，我们提出了一种对i）路径长度敏感的语义相似性度量（指概念密度），ii）层次中节点的深度（更深的节点排列得更紧密）和iii）子层次中节点的密度（密集子层次中涉及的概念排列得比更稀疏区域中的概念更紧密）。"}
{"pid": "A00-2004", "zh_sum": "与领域无关的线性文本分割的进展本文描述了一种线性文本分割方法，其精度是最新技术的两倍，速度是最新技术的七倍（Reynar，1998）。在局部上下文中，句间相似度被秩代替。通过分裂聚类发现边界位置。我们设计了一个人工数据集，通过连接从Brown语料库中提取的短文本片段来构建。"}
{"pid": "C90-3063", "zh_sum": "自动处理大型语料库以解决回指引用，手动获取广泛领域中的语义约束非常昂贵。本文提出了一种在大型语料库中自动收集共现模式统计信息的方案。这些统计数据在很大程度上反映了语义约束，因此被用来消除回指引用和句法歧义。该计划是通过收集其他语言工具输出的统计数据来实施的。在从语料库中随机选取的句子中，进行了一项实验来解析代词“it”的指称。实验结果表明，在大多数情况下，共现统计确实反映了语义约束，从而为有用的消歧工具提供了基础。我们使用代词上下文的分布来确定哪些候选先行词可以匹配上下文。我们提出了在代词解析中使用谓词参数频率的最早方法之一。"}
{"pid": "W06-1203", "zh_sum": "基于潜在语义分析的非合成多词表达的自动识别利用潜在语义分析，我们探索了一个假设，即本地语言上下文可以用来识别具有非合成意义的多词表达。我们提出，与MWE整体相关的分布向量与其组成部分相关的分布向量之间的向量相似度可以很好地衡量MWE的组成程度。我们目前的实验表明，低（余弦）相似性实际上与非成分性相关。我们设计了一种监督方法，在该方法中，他们计算训练数据中给定表达式的文字和非文字用法的意义向量。我们使用监督学习方法，通过使用潜在语义分析（LSA）向量形式的上下文信息来区分表达式（在德语文本中）的合成和非合成使用。"}
{"pid": "W06-3108", "zh_sum": "统计机器翻译的判别重排序模型我们提出了基于短语的统计机器翻译的判别重排序模型。利用最大熵原理对模型进行训练。我们使用了几种类型的特征：基于单词、基于单词类、基于本地上下文。我们评估了重新排序模型的总体性能以及各个特征类型对单词对齐语料库的贡献。此外，与最先进的基线系统相比，我们使用这些重新排序模型提高了翻译性能。尽管重新排序的LMs非常复杂，但当集成到已经包含区分短语方向模型的PSMT基线时，它们会产生一些改进。为了将重新排序词汇化，使用了一种区分性的重新排序模型（Zens和Ney，2006a）。我们在区分重排序模型中使用了聚类词类，并表明它们降低了分类错误率。"}
{"pid": "P97-1023", "zh_sum": "预测形容词的语义方向我们从大量语料库中识别并验证了连词对连词的正负语义方向的限制。对数线性回归模型使用这些约束来预测连词的方向是相同的还是不同的，当单独考虑每个连词时，在这项任务中达到82%的准确率。结合多个形容词之间的约束，聚类算法将形容词分成不同方向的组，最后将形容词标记为正或负。对真实数据和模拟实验的评估表明，该方法具有很高的性能：对于语料库中数量适中的连词，形容词的分类精度超过90%。我们根据连词结构、加权相似图、最小割集、监督学习和聚类将形容词聚类为+和-集。"}
{"pid": "W03-1728", "zh_sum": "中文分词作为LMR标记本文提出了基于LMR标记的中文分词算法。我们的LMR标记器是用最大熵马尔可夫模型实现的，然后我们使用基于变换的学习来组合两个以相反方向扫描输入的LMR标记器的结果。我们的系统在中央研究院语料库和香港城市大学语料库上的F分数分别为95.9%和91.6%。我们首次描述了用于中文分词的字符分类方法，其中每个字符都有一个边界标记，表示其在单词中的相对位置。"}
{"pid": "C94-2178", "zh_sum": "K-Vec：一种对齐平行文本的新方法已经提出了多种对齐两种或两种以上语言文本的方法，例如加拿大议会辩论（Hansards）。其中一些方法会产生一个双语词典作为副产品。我们提出了一种称为K-vec的替代对齐策略，该策略从估计词典开始。例如，它发现英语单词fishery与法语peches相似，注意到英语文本中渔业的分布与法语中peches的分布相似。K-vec不依赖于句子边界。"}
{"pid": "P00-1058", "zh_sum": "使用自动提取的树邻接语法进行统计分析我们讨论了词汇化树邻接语法作为统计分析的词汇化PCFG的替代方法的优势，描述了从Penn Treebank归纳出的概率LTAG模型，并评估了其解析性能。我们发现，这种归纳方法是对（Hwa，1998）基于EM的方法的改进，并且归纳模型产生的结果与词汇化PCFG相当。我们从《华尔街日报》中提取了一个随机树插入语法或STIG，对于40个单词的句子，获得了86.6%的LP和86.9%的LR。"}
{"pid": "C92-1038", "zh_sum": "一种快速生成指称表达式的算法我们简化了先前开发生成指称表达式算法的工作，同时考虑了心理语言学的发现和成绩单数据。结果是一个简单的算法，它在计算上易于处理，对人类用户的偏好敏感，并且合理地与域无关。我们提供了主机系统必须提供的资源规范，以便使用该算法，并描述了IDAS系统中使用的实现。我们应用关于对象属性显著性的一般化和关于单词构成基本属性的约定，以增量方式选择要包含在描述中的单词。"}
{"pid": "C90-3030", "zh_sum": "约束语法作为分析解析器中使用的运行文本语法的框架，通常直接从自主语法理论和描述性实践中引入，而不是为了明确的分析目的。语法分析器是根据政府和绑定理论、广义短语结构语法和词汇功能语法为英语设计的。我们提出了一种形式主义，用于语法语句更接近真实文本句子的语法分析，并更直接地解决一些臭名昭著的语法分析问题，尤其是歧义。形式主义是一种语言主义。它间接地依赖于过渡概率。概率不是描述的一部分。描述性陈述、约束并没有定义“L中的正确句子”这一概念的一般任务。它们本质上不太明确，与形态特征联系更紧密，更直接地面向解析的基本任务。我们将此任务视为以基本上自下而上的模式从一系列具体标记中推断表面结构的任务之一。约束是在大量语料库研究的基础上制定的。它们可能反映出绝对的、类似规则的事实或概率趋势，其中某些风险被认为是应该承担的。前一种规则类型的约束当然更可取。语言L的约束集合构成了L的约束语法（CG）。CG用于约束语法解析器CGP，作为Lisp解释器实现。我们对CGP的输入标记是经过形态学分析的单词形式。一个中心思想是最大限度地利用形态学信息进行语法分析。所有相关结构都是通过词汇、词法和从词法到语法的简单映射直接指定的。约束的任务基本上是丢弃尽可能多的备选方案，最佳方案是完全消除歧义的句子，只进行一次语法阅读。第二个中心思想是通过相同的机制来处理形态消歧和句法标记，即丢弃不适当的替代。一个好的解析形式主义应该满足许多要求：约束应该是声明性的而不是过程性的，它们应该能够处理任何真实世界的文本句子（即运行文本，而不仅仅是语言学家的实验室句子），它们应该与执行它们的程序代码明确分开，形式主义应该是独立于语言的，它应该很容易实现（最好是作为有限状态自动机），而且运行起来也应该很高效。CG形式主义遵循这些要求。我们提出了约束语法框架。"}
{"pid": "P04-1036", "zh_sum": "在词义消歧（WSD）中发现未标记文本中的主要词义，选择最常见词义的启发式方法非常有效，因为一个词的词义分布往往是倾斜的。除了不考虑周围环境之外，使用主要的或第一感觉启发式的问题在于，它假设了一些手标记的数据。虽然有一些手语语料库可用于某些语言，但人们预计词义的频率分布，尤其是主题词，取决于所考虑文本的类型和领域。我们目前的工作是使用从原始文本语料库和WordNet相似性软件包中获取的同义词表自动查找主要的名词词义。获得的主导意义对SENSEVAL-2英语全词任务中的名词给出了64%的准确率。这是一个非常有希望的结果，因为我们的方法不需要任何手动标记的文本，例如SemCor。此外，我们还证明了我们的方法可以从两个特定领域的语料库中发现合适的主要词义。第一感觉启发式算法通常用作受监督WSD系统的基线，其性能优于许多考虑周围环境的系统。我们将词义消歧任务限制为确定给定域中的主要词义。"}
{"pid": "P07-1019", "zh_sum": "森林重建：使用集成语言模型进行快速解码高效解码一直是机器翻译中的一个基本问题，尤其是使用集成语言模型，这对于获得良好的翻译质量至关重要。我们基于k-best解析算法开发了更快的方法来解决这个问题，并在基于短语和基于语法的机器翻译系统上证明了它们的有效性。在这两种情况下，与传统的波束搜索方法相比，在相同的搜索误差和平移精度水平下，我们的方法实现了显著的速度改进，通常提高了10倍以上。我们对语言模型可能触发的重新排序量进行假设，以限制探索。我们介绍了立方体修剪及其变体，立方体生长。我们使用森林的概念来描述搜索空间的解码与综合语言模型。"}
{"pid": "P03-1023", "zh_sum": "基于竞争学习方法的共指消解在本文中，我们提出了一种用于共指消解的竞争学习方法。传统上，有监督机器学习方法采用单一候选模型。然而，在这个模型中，先前候选人之间的偏好关系无法准确确定。相比之下，我们的方法采用了双候选学习模型。这样的模型可以可靠地给出先前候选人的竞争标准，并确保选择最喜欢的候选人。此外，我们的方法采用了一个候选滤波器，以减少训练和分辨率期间的计算成本和数据噪声。在MUC-6和MUC-7数据集上的实验结果表明，我们的方法优于基于单一候选模型的方法。我们利用非回指在双候选模型（Yang et al 2003）中创建了一类特殊的训练实例，并在MUC-6和MUC-7语料库的F1测量中分别将性能提高了2.9和1.6至67.3和67.2。"}
{"pid": "P11-2008", "zh_sum": "推特词性标注：标注功能和实验我们解决了流行微博服务推特的英语数据词性标注问题。我们开发了一个标记集，对数据进行注释，开发特征，并报告接近90%精度的标记结果。这些数据和工具已提供给研究社区，目的是对推特和相关社交媒体数据集进行更丰富的文本分析。认识到现有系统的局限性，我们开发了一个专门用于推特的POS标记器，方法是创建一个训练语料库，并设计一个标记集，其中包括在线语言中唯一存在的词类，如表情符号（笑脸）。我们发布的推特POS数据集包含1827条推特中约26000个单词。推特数据集使用一个由25个标记组成的域相关标记集。"}
{"pid": "P08-2026", "zh_sum": "生物医学句法分析的自我训练解析器自我训练是一种技术，它使用现有的解析器，解析额外的数据，然后通过将额外的数据作为进一步的训练数据来创建第二个解析器。这里，我们将此技术应用于解析器自适应。特别是，我们使用未标记的生物医学摘要对标准的Charniak/Johnson-Penn树库解析器进行自我训练。在Genia语料库的生物医学摘要标准测试集上，该分数达到84.3%。这比之前生物医学数据的最佳结果（在同一测试集上为80.2%）减少了20%的误差。"}
{"pid": "I08-1059", "zh_sum": "使用上下文拼写器技术和语言建模进行ESL错误更正，我们提出了一个模块化系统，用于检测和更正非母语（英语作为第二语言=ESL）作者的错误。我们关注两种错误类型：限定词的错误使用和介词的选择。我们使用受上下文拼写系统启发的决策树方法来检测和更正建议，并使用在Gigaword语料库上训练的大型语言模型来提供额外信息来过滤虚假建议。我们展示了该系统在非母语英语文本语料库上的表现，并讨论了未来增强的策略。我们除了使用分类器外，还使用了一个语言模型，并将分类器输出和语言模型分数组合在一个元分类器中。我们使用单一语言模型对假设错误和潜在纠正进行评分，以筛选出不太可能纠正的候选人。"}
{"pid": "W00-1308", "zh_sum": "丰富最大熵词性标注器中使用的知识源本文给出了基于最大熵的词性标注器的结果，该标记器主要通过丰富用于标注的信息源来实现优异的性能。特别是，通过结合这些特征，我们得到了改进的结果：（i）对未知词进行更广泛的大写处理；（二）动词时态消歧的特点；（iii）从介词和副词中消除语气词歧义的特征。佩恩树库上的标记器的最佳结果准确率为96.86%，而之前未看到的单词的准确率为86.91%。使用MEMM，我们看到的单词和看不见的单词分别达到96.9%和86.9%。"}
{"pid": "W95-0105", "zh_sum": "随着同义词库的联机出现以及分布式词聚类技术的改进，与Wordnet感官相关的名词分组消歧功能对语言处理任务有用的词分组越来越可用。然而，对于许多任务，人们感兴趣的是词义之间的关系，而不是单词。本文提出了一种对出现在相关名词集合中的名词进行自动语义消歧的方法——在线同义词库中的数据类型，或作为分布式聚类算法的输出。消歧是针对WordNet词义进行的，这些词义获得的效果相当好；然而，该方法还允许分配更高级别的WordNet类别，而不是感官标签。该方法主要通过示例进行说明，但也给出了更严格的评估结果。在这项工作中，使用字典数据库作为知识源来评估语义相似度被认为是为单词聚类提供了重要线索。我们将两个词之间的语义相似度定义为一个层次结构的同义词库中包含这两个词的信息量最大的概念的熵值。我们试图结合范例和组合相似策略。"}
{"pid": "C02-2025", "zh_sum": "行话红杉树银行：动机和初步应用行话红杉倡议是设计和开发新型树银行的一项种子活动。虽然英语（和其他主要语言）有几种中大型树状图库，但现有的公共资源显示出以下局限性：（i）注释是单层的，编码拓扑（短语结构）或结构语法（依赖）信息，（ii）记录的语言信息深度相对较浅，（iii）树库中语言表示的设计和格式将从树库中提取信息的一小部分预定义方式连接起来，（iv）现有树库中的表示是静态的，在大规模树库的发展过程中（通常是一年或十年），往往落后于该领域的发展。行话Redwoods旨在开发一种新颖的树库方法，该方法在不同粒度的树库中检索语言数据的方式以及树库自身的不断进化和定期更新方面都具有丰富的性质和动态性。自2001年10月以来，该项目一直致力于为这种新型树库奠定基础，开发一套基本的树库构建和维护工具，并初步构建一套10000棵带注释的树，与开放源码许可证下的工具一起分发。我们开发了HPSG行话红杉树银行。红杉树库旨在提供带注释的培训材料，以便将模糊度解决的统计模型与ERG产生的精确解释相结合。动态的、基于鉴别的树库是英语红杉树库的先驱。"}
{"pid": "D07-1043", "zh_sum": "V-测度：一种基于条件熵的外部聚类评价测度我们提出了一种基于外部熵的聚类评价测度V-测度。V-measure为影响先前定义的聚类评估指标的许多问题提供了一个优雅的解决方案，包括1）对聚类算法或数据集的依赖性，2）匹配问题，其中仅评估一部分数据点的聚类，以及3）准确评估和组合聚类的两个理想方面，同质性和完整性。我们将V-measure与一些流行的聚类评估方法进行了比较，并使用模拟的聚类结果证明了V-measure满足聚类解决方案的一些期望属性。最后，我们使用V-measure来评估两个聚类任务：文档聚类和基音重音类型聚类。F得分不适合比较不同聚类数的结果。V度量（VM）是一种信息论度量，它报告同质性的调和平均值（每个集群应仅包含单个类的实例）和完整性（一个类的所有实例应为同一集群的成员）。F-Score的一个重要限制是，它不评估大多数类别以外的集群组成。"}
{"pid": "N09-1025", "zh_sum": "11001统计机器翻译的新特征我们使用Crammer等人的边距注入松弛算法为两个机器翻译系统添加大量新特征：Hiero分级短语翻译系统和我们的语法翻译系统。在一项大规模的汉英翻译任务中，我们分别获得了+1.5 Bleu和+1.1 Bleu的统计显著改善。我们分析了新特性和学习算法性能的影响。我们只使用100个最频繁的单词作为单词上下文特征。我们介绍了用于中文/英文翻译的SCFG模型的两种类型的功能：第一种类型显式地对抗规则计数的高估，或者具有不好的重叠点、不好的重写或不需要的目标端插入的规则。"}
{"pid": "I05-3025", "zh_sum": "中文分词的最大熵方法我们参加了第二届国际中文分词大赛。具体而言，我们在所有四个语料库（即中央研究院（AS）、香港城市大学（CITYU）、微软研究院（MSR）和北京大学（PKU））上对我们的中文分词器进行了开放式评估。基于最大熵方法，我们的分词器在AS、CITYU和PKU中获得了最高的F度量，在MSR中获得了第二高的F度量。我们发现，使用外部词典和不同分词标准的额外训练语料库有助于进一步提高分词准确性。我们提出了一种增强未知分词的后处理方法。我们使用表示数字、日期、字母等的模板。"}
{"pid": "J94-4003", "zh_sum": "利用第二语言单语语料库进行词义消歧本文提出了一种利用另一种语言单语语料库的统计数据解决一种语言词汇歧义的新方法。这种方法利用了不同语言中单词到感觉的映射之间的差异。本文主要研究机器翻译中的目标词选择问题，该方法直接适用于该问题。该算法使用源语言解析器识别单词之间的句法关系，并使用双语词典将这些关系的替代解释映射到目标语言。然后根据目标语言中词汇关系的统计数据来选择首选词义。选择基于统计模型和约束传播算法，该算法可同时处理句子中的所有歧义。使用三组希伯来语和德语示例对该方法进行了评估，发现该方法对于消除歧义非常有用。本文对统计意义上的消歧方法进行了详细的比较分析。我们提出了一种使用单语语料库、双语词典和源语言解析器的WSD方法。"}
{"pid": "P03-1012", "zh_sum": "在统计机器翻译中，提高词对齐的概率模型在统计机器翻译中起着至关重要的作用。词对齐语料库是翻译相关知识的优秀来源。我们提出了一个统计模型来计算给定句子对的对齐概率。此模型允许轻松集成特定于上下文的功能。我们的实验表明，该模型可以有效地改善现有的单词对齐。我们提出了一种直接对齐公式，并认为在给定监督对齐语料库的情况下，估计参数是很简单的。我们使用依赖树的短语内聚作为波束搜索对齐器的约束。"}
{"pid": "W08-2121", "zh_sum": "CoNLL 2008年关于句法和语义依赖联合解析的共享任务计算自然语言学习会议每年都会有一个共享任务，其目的是促进自然语言处理应用程序，并在标准设置中对其进行评估。2008年，共享任务致力于句法和语义依赖的联合解析。这项共享任务不仅将前四年的共享任务统一在一种独特的基于依赖关系的形式下，而且还显著扩展了它们：今年的语法依赖关系包括更多信息，如命名实体边界；语义依赖为动词谓词和名词谓词的角色建模。在本文中，我们定义了共享任务，并描述了如何创建数据集。此外，我们报告和分析了结果，并描述了参与系统的方法。我们首先介绍了谓词分类任务，这可以看作是谓词意义消歧。本文详细介绍了完整的合并过程以及从组成表示到依赖关系的转换。"}
{"pid": "P07-1028", "zh_sum": "一个简单的基于相似度的选择偏好模型我们使用基于语料库的语义相似度度量，提出了一个新的、简单的选择偏好自动归纳模型。针对语义角色标注的任务，我们计算了语义角色的选择偏好。在评估中，基于相似度的模型显示出比Resnik的基于WordNet的模型和基于EM的聚类模型更低的错误率，但存在覆盖问题。我们通过语义角色标注从语料库中提取出一组视觉中心词，并且只使用单一的向量空间表示。我们将一个单词的上下文建模为与之同时出现的单词的分布。我们在FrameNet中选择一个子集（Baker等人，1998）来测试并使用该子集中所有标记的实例。"}
{"pid": "P08-1030", "zh_sum": "通过跨文档推理细化事件提取我们将“每个话语一个意义”（Yarowsky，1995）的假设应用于信息提取（IE），并将“话语”的范围从单个文档扩展到一组主题相关的文档。我们采用类似的方法在句子和文档中传播一致的事件参数。将来自相关文档的全局证据与局部决策相结合，我们设计了一个简单的跨文档推理方案，以改进ACE事件提取任务。在不使用任何额外标记数据的情况下，与为每个句子独立提取事件的先进IE系统相比，这种新方法在触发器标记中获得了7.6%的高F-测度，在参数标记中获得了6%的高F-测度。我们采用基于规则的方法在主题相关文档中传播一致的触发器和参数。"}
{"pid": "D09-1127", "zh_sum": "双语约束（单语）Shift-Reduce分析联合分析两种语言已被证明可以提高任一方或双方的准确性。然而，其搜索空间比单语情况大得多，迫使现有方法采用复杂的建模和粗略的近似。在这里，我们提出了一种更简单的替代方法，即双语约束的单语语法分析，源语言语法分析器学习如何利用重排序作为额外的观察，但不必费心构建目标侧树。我们具体展示了如何使用对齐特性增强shift-reduce依赖关系解析器，以解决shift-reduce冲突。对中文树库双语部分的实验表明，仅使用3个双语特征，我们就可以将英语和汉语的句法分析准确率在最先进的基线上提高0.6%（绝对值），但可以忽略不计(∼6%）的效率开销，因此比双火花快得多。我们保持自然规则的概率不变，并将虚拟规则的概率设置为1。我们使用一个未分析的汉语句子的特征来改进英语介词短语连接。"}
{"pid": "D07-1072", "zh_sum": "基于层次Dirichlet过程的无限PCFG我们提出了一种基于层次Dirichlet过程（HDP）的树结构非参数贝叶斯模型。我们的HDP-PCFG模型允许语法的复杂性随着更多的训练数据可用而增长。除了为PCFG提供一个完全贝叶斯模型外，我们还开发了一个有效的变分推理程序。在合成数据上，我们恢复正确的语法，而无需事先指定其复杂性。我们还展示了我们的技术在学习状态分裂语法方面的有效性，从而表明我们的技术可以应用于全面的语法分析应用程序。我们发现，由于潜变量文法没有显式正则化，EM不断拟合训练数据，最终开始过度拟合。"}
{"pid": "N01-1021", "zh_sum": "概率早期语法分析器作为人类句子处理的心理语言学模型，认知负荷可以通过多种方式定义。本报告考虑了认知负荷的定义，即在句子中某个时候被否定的结构选项的总概率：单词wi的前缀w0的出其不意。。。我−1关于短语结构语言模型。使用概率Earley解析器（Stolcke，1995）可以有效地计算这些负载，该解析器被解释为在逐字的基础上生成关于阅读时间的预测。在由微粒频率数据支持的语法假设下，Stolcke的概率Earley解析器的操作可以正确预测与花园路径结构歧义和主语/宾语相对不对称相关的加工现象。自从引入基于解析器的惊奇计算以来，统计技术已经成为阅读难度和语言复杂性的常用模型。"}
{"pid": "C96-1058", "zh_sum": "三种新的依赖语法概率分析模型：在提出了一种新的依赖语法O（n3）分析算法之后，我们开发了三种不同的随机化方法。我们提出了（a）一个词汇亲和力模型，其中单词努力相互修改，（b）一个词义标记模型，其中单词在其选择偏好中随机波动，以及（e）一个生成模型，其中说话人充实每个单词的句法和概念结构，而不考虑对听话人的影响。我们还通过评估这三个模型在《华尔街日报》培训文本（来源于宾夕法尼亚州Treebank）上的解析性能，给出了初步的实证结果。在这些结果中，生成模型的性能明显优于其他模型，并且在分配词性标记方面也同样出色。所提出的解析算法足以在O（n3）时间内搜索所有投影树。"}
{"pid": "N06-2013", "zh_sum": "统计机器翻译中的阿拉伯语预处理方案本文研究了阿拉伯语不同单词级预处理决策对SMT质量的影响。我们的结果表明，在给定大量训练数据的情况下，仅分离过程的性能最好。然而，对于少量的训练数据，最好使用词性标记、复杂的形态分析和消歧来应用类似英语的标记化。此外，如果训练和测试数据之间的类型发生变化，选择适当的预处理会显著提高BLEU分数。我们发现，随着平行语料库大小的增加，各种分割方案的效果都有所改善。"}
{"pid": "P11-1019", "zh_sum": "一种新的ESOL文本自动评分数据集和方法我们演示了如何使用有监督的区别性机器学习技术来自动评估“英语作为第二语言或其他语言”（ESOL）考试脚本。特别是，我们使用等级偏好学习来明确建模脚本之间的等级关系。提取了许多不同的特征，并使用烧蚀测试来研究它们对整体性能的贡献。回归模型和等级偏好模型之间的比较进一步支持了我们的方法。在第一个公开数据集上的实验结果表明，我们的系统可以达到接近任务上限的性能水平，正如人类考官在同一语料库上达成的协议所定义的那样。最后，使用一组“异常值”文本，我们测试了我们模型的有效性，并确定了模型分数与人类考官分数不同的情况。我们公开发布了一组1244个FCE ESOL文本。"}
{"pid": "J02-2003", "zh_sum": "基于类的概率估计使用语义层次这篇文章关注一种特定概率的估计，即名词意义作为谓词的特定参数出现的概率。为了克服伴随而来的稀疏数据问题，这里的建议是从语义层次定义感官的概率，并利用感官可以分组为语义相似的感官组成的类这一事实。人们特别关注的问题是如何为给定的意义确定一个合适的类，或者，如何在层次结构中确定一个合适的泛化级别。开发了一个程序，使用卡方检验来确定适当的泛化水平。为了测试估计方法的性能，使用了一个伪消歧任务，以及两种可选的估计方法。每种方法使用不同的泛化过程；第一种选择使用最小描述长度原则，第二种选择使用Resnik的选择偏好度量。此外，我们还使用标准Pearson-chisquare统计量和对数似然卡方统计量研究了我们方法的性能。简而言之，我们根据语料库频率（动词/槽对的所有名词的频率）填充WordNet层次结构，然后通过使用卡方来确定是否将估计推广到层次结构中的父节点，从而确定层次结构中每个节点处的适当概率估计。"}
{"pid": "J94-4004", "zh_sum": "机器翻译分歧：形式化描述和拟议解决方案在许多情况下，将一种语言自然翻译成另一种语言会导致与原始语言截然不同的形式。翻译差异（即跨语言差异）的存在使得从源语结构到目标语结构的直接转换不切实际。许多现有的翻译系统都有处理发散结构的机制，但没有提供一个利用词汇语义结构和句法结构之间系统关系的通用程序。本文论证了对两类信息的形式化可以得到一个系统的分歧问题的解决方案：（1）词汇语义分歧所基于的语言基础类；解决词汇语义分歧的技巧。这种形式化的优势在于，它促进了系统的设计和实现，允许对系统的状态进行评估，并为证明系统的某些重要属性提供了基础。我们对语言间句法差异的来源进行了分类。"}
{"pid": "N01-1026", "zh_sum": "通过对对齐语料库的稳健投影，归纳出多语词性标记和名词性括号。本文研究了通过自动对齐的平行语料库，将语言注释（包括词性标记和基本名词短语括号）从一种语言投影到另一种语言的可能性。首先，实验评估了标记和括号从源语言英语到目标语言法语和汉语的未经修改的直接转换的准确性，无论是对于吵闹的机器对齐句子还是干净的手动对齐句子。然后，通过使用针对高噪声数据进行优化的训练技术，在这两条基线上大幅提高了性能，为独立单语工具提供了94-96%的核心法语词性标记准确率和90%的法语括号F-度量，而无需使用给定语言中的任何人工注释数据。我们通过英语资源的迁移，为法语和汉语引入词性标记器和基本名词短语检测器。我们是第一个提出使用平行文本来引导标记器创建的人。"}
{"pid": "N06-1033", "zh_sum": "基于同步语法和树变换器的机器翻译系统的同步二值化有望提高统计机器翻译输出的质量，但通常计算量很大。由于两种语言之间的任意重新排序，单个语法规则的复杂度是指数级的，从并行语料库中提取的规则可能相当大。我们设计了一种线性时间算法，在可能的情况下通过对同步规则进行二值化来分解句法重序，并表明生成的规则集显著提高了基于最先进语法的机器翻译系统的速度和准确性。同步二值化同时对同步规则的源端和目标端进行二值化，确保尽可能在两侧都有连续的跨距。"}
{"pid": "P97-1041", "zh_sum": "一种可训练的基于规则的分词算法本文提出了一种可训练的基于规则的分词算法。该算法为需要大量知识工程的大规模基于词典的分词器提供了一种简单、独立于语言的替代方法。作为一个独立的分词器，我们展示了我们的算法来生成高性能的中文分词。此外，我们还证明了基于转换的算法能够有效地提高现有几种分词算法在三种不同语言中的输出。我们的中文分词器只使用手动分词的语料库，没有参考任何词汇。"}
{"pid": "D09-1120", "zh_sum": "简单的共指消解具有丰富的句法和语义特征。共指系统由句法、语义和语篇约束驱动。我们提出了一种将这三个方面完全模块化的简单方法。与当前许多侧重于学习和话语成分的工作相比，我们的系统是确定性的，完全由从大型未标记语料库中学习到的语法和语义兼容性驱动。尽管我们的系统简单且话语幼稚，但它的性能远远优于所有无监督系统和大多数监督系统。主要贡献包括：（1）提出了一个简单易复制、高性能的基线；（2）证明大多数剩余错误可归因于共指现象外部的句法和语义因素（可能最好由非共指系统解决）。我们发现，在最先进的系统中，共指错误常常是由于语义兼容性差的模型造成的。在我们的SYN-CONSTR设置中，每一个指代都与任何具有相同词头或确定性句法结构（同位语或谓语主格结构）的过去提及相关。在搜索mk的先行项时，其候选先行项将按照它们在关联解析树中的位置确定的顺序进行访问。"}
{"pid": "W04-3205", "zh_sum": "VerbOcean：在Web上挖掘细粒度语义动词关系动词之间语义关系的广泛覆盖存储库可以使许多NLP任务受益。我们提出了一种半自动提取动词间细粒度语义关系的方法。我们使用网络上的词汇语法模式来检测强关联动词对之间的关系之前发生的相似性、强度、反义词、启用和时间。在一组29165个强关联动词对上，我们的提取算法的准确率为65.5%。误差类型分析表明，在关系强度上，我们达到了75%的精度。我们提供了名为VERBOCEAN的资源，可在以下位置下载：http://semantics.isi.edu/ocean/.我们引入了一个5类集合，专门用于描述动词-动词语义关系。我们使用模式来提取动词之间的一组关系，例如相似性、强度和反义词。"}
{"pid": "J97-3003", "zh_sum": "对于依赖形态句法信息（如词性标记器或语法分析器）的NLP模块来说，自动归纳未知单词猜测规则是一个很大的问题。在本文中，我们提出了一种完全自动获取规则的技术，该规则可以使用未知单词的起始和结束片段猜测可能的词性标记。学习是从一个通用词汇和从原始语料库收集的词频中进行的。统计归纳出三组互补的单词猜测规则：前缀词法规则、后缀词法规则和结尾猜测规则。利用该技术，将未知词猜测规则集归纳并集成到随机标记器和基于规则的标记器中，然后将其应用于含有未知词的文本。我们的模型LTPOS同时执行句子识别和词性标注。我们的ltpos是一个统计组合词性标记和句子边界消歧模块。我们的ltpos是一个统计组合词性标记和句子边界消歧模块。"}
{"pid": "P06-1101", "zh_sum": "基于异构证据的语义分类归纳我们提出了一种新的语义分类归纳算法。以前的分类归纳算法通常侧重于独立分类器，用于根据手工构建或自动发现的文本模式发现新的单一关系。相比之下，我们的算法灵活地结合了来自异构关系的多个分类器的证据，以优化分类法的整个结构，使用单词坐标词的知识来帮助确定其超词，反之亦然。我们将我们的算法应用于语义消歧的名词下义词获取问题，将超词和协调词分类器的预测与现有语义分类法（WordNet 2.1）中的知识相结合。我们在WordNet 2.1中添加了10000个新的语法集，精确度为84%，与使用相同组件分类器的非联合算法相比，相对误差减少了70%。最后，我们展示了使用我们的算法构建的分类法，在一个独立的超名称对测试集上，与WordNet 2.1相比，相对F分数提高了23%。我们生成数万个超词模式，并将这些模式与名词集群相结合，以生成高精度的建议，将未知名词插入WordNet。我们使用已知的上下义词对为机器学习系统生成训练数据，然后机器学习系统学习许多词汇句法模式。我们通过贪婪地最大化给定分类的一组关系证据的条件概率来添加新的术语。我们使用句法路径模式作为监督下的上下义和同义分类器的特征，这些分类器的训练示例是从WordNet自动派生的。"}
{"pid": "P07-1073", "zh_sum": "学习使用最小监督从Web提取关系，我们提出了一种新的关系提取方法，只需要少量的训练示例。给定已知的几对具有或不具有特定关系的命名实体，将从web中提取包含这些实体的句子包。我们扩展了现有的关系提取方法来处理这种较弱的监管形式，并给出了实验结果，证明我们的方法可以可靠地从web文档中提取关系。我们提供了一个数据集，其中包含目标语义关系中实体对的多个实现，这与之前工作中的类似数据集不同。"}
{"pid": "D08-1031", "zh_sum": "理解特征对共指消解的价值近年来，在共指消解这一重要问题上进行了大量的研究，其中大部分集中于开发新的模型和算法技术。这些工作通常表明，复杂模型在弱成对基线上有所改善。然而，很少有人关注选择强特征来支持学习共指模型的重要性。本文描述了一个非常简单的共指消解成对分类模型，该模型使用了一组精心设计的特征。我们表明，这产生了一个最先进的系统，其性能优于使用复杂模型构建的系统。我们建议，我们的系统可以用作开发更复杂模型的基线，当使用更强大的功能集时，可能影响较小。本文还介绍了烧蚀研究，并讨论了各种特征的相对贡献。我们的算法在时间上以二次方的次数运行。"}
{"pid": "C08-1107", "zh_sum": "学习一元模板的蕴涵规则大多数关于无监督蕴涵规则获取的工作侧重于具有两个变量的模板之间的规则，而忽略了一元规则-具有单个变量的模板之间的蕴涵规则。本文研究了这类规则的两种无监督学习方法，并将所提出的方法与二进制规则学习方法进行了比较。结果表明，学习的一元规则集优于二元规则集。此外，一种新的学习蕴涵方向相似性度量，称为平衡包含，是性能最好的度量。我们提出了一个一元模板，它被定义为一个由一个参数槽和一个谓词短语组成的模板。我们使用参数的分布相似性来检测一元模板蕴涵。研究了两种无监督学习一元规则的方法（即在具有单个变量的模板之间）。在（Zhao et al，2009）中，提出了一种从双语平行语料库中提取复述模式的枢轴方法，而在（Callison-Burch，2008）中，通过要求短语及其复述具有相同的句法类型，提高了从平行语料库中提取复述的质量。我们的方法与他们的方法在许多方面不同：他们的目标是释义提取，而我们提取的是定向蕴涵规则；作为模式提取的文本资源，他们使用平行语料库（使用另一种语言中的模式作为支点），而我们依赖于单语维基百科的修订（得益于其不断扩大的规模）；他们提取的para短语更类似于DIRT，而我们的方法允许专注于获取在蕴涵对中频繁出现的特定现象的规则，而不是其他资源所涵盖的规则。我们尝试使用WeedsPrec识别词汇句法模板之间的蕴涵关系，但观察到它倾向于促进涉及不频繁模板的不可靠关系。"}
{"pid": "P85-1011", "zh_sum": "树邻接语法的一些计算性质树邻接语法（TAG）是自然语言语法的一种形式主义。TAG的一些基本概念是在【Joshi，Levy和Takakashi 1975】和【Joshi，1983】中介绍的。关于标签的语言相关性的详细调查已经在【Kroch和Joshi，1985年】中进行了。在本文中，我们将描述一些关于标记的新结果，特别是在以下方面：（1）标记的解析复杂性，（2）标记的一些闭包结果，以及（3）与头语法的关系。我们为标记提供了解析算法，可以解析同步标记的基本形式。我们在一个类似CYK的算法中提出了第一个标记解析器。"}
{"pid": "D08-1036", "zh_sum": "无监督隐马尔可夫模型POS-taggers贝叶斯估计量的比较将贝叶斯技术应用于NLP问题的兴趣越来越大。贝叶斯模型有许多不同的估计器，了解每个估计器在哪些类型的任务上表现良好是很有用的。本文比较了不同大小数据集上不同隐状态数的隐马尔可夫模型POS标记器的各种不同贝叶斯估计。最近的论文在将贝叶斯估计与无监督HMM词性标注的期望最大化（EM）进行比较时给出了相互矛盾的结果，我们表明，报告结果的差异很大程度上是由于HMM中训练数据的大小和状态数的差异。我们研究了HMM的各种采样器，包括这些早期论文没有研究的一些。我们发现，所有Gibbs采样器都能很好地处理小数据集和少数状态，而变分Bayes能很好地处理大数据集，并且与Gibbs采样器具有竞争力。就收敛时间而言，我们发现变分Bayes是所有估计量中最快的，尤其是在大数据集上，而显式Gibbs采样器（点式和句子阻塞式）通常比其在大数据集上的折叠式采样器更快。我们考虑了三个评价标准：M-to-1和1-to-1分别是最佳多对一映射和贪婪一对一映射下的标注精度；VI是一种无图信息理论准则。我们从语料库的一半中归纳出状态标识符到PoS标记的多对一映射，并在另一半中进行评估，这称为交叉验证准确性。我们表明，在交叉验证的多对一准确性中，稀疏先验可以获得4%（对于100万单词的语料库，为0.62到0.66）。"}
{"pid": "N10-1061", "zh_sum": "在以实体为中心的模块化模型中，共指消解受句法、语义和语篇约束的制约。我们提出了一种生成的、基于模型的方法，其中这些因素中的每一个都以模块化的方式封装，并以主要无监督的方式学习。我们的语义表示首先假设了一组潜在的实体类型，这些实体类型生成特定的实体，这些实体反过来又呈现个人提及。通过在抽象实体类型级别共享词法统计信息，我们的模型能够显著减少语义兼容性错误，从而在完成端到端的共指任务时获得迄今为止的最佳结果。"}
{"pid": "W03-0405", "zh_sum": "无监督人名消歧本文提出了一套基于少量监督或无监督的算法，用于区分文本中具有多个实指的人名。该方法利用无监督聚类技术对生物事实的丰富特征空间进行聚类，这些特征空间通过独立于语言的引导过程自动提取。然后，通过自动提取的生物数据，对命名实体的诱导聚类进行分区，并将其与真实的参照物链接。性能评估基于手动标记的多参考人名测试集和自动生成的假名。我们提取诸如出生日期或地点、职业、亲戚等传记事实，以帮助解决人名含糊不清的问题。"}
{"pid": "P93-1020", "zh_sum": "基于意图的切分：人类的可靠性和与语言线索的相关性语篇中话语的某些跨度，这里称为切分，被广泛认为是连贯的单位。此外，语篇的分段结构被认为受到许多现象的制约和制约。然而，对于分部的性质以及识别或生成分部的标准，人们的共识很弱。我们使用自发的叙事独白语料库呈现了一项分为两部分的定量研究结果。第一部分评估了语料库中人类切分的统计可靠性，其中说话人意图是切分标准。然后，我们使用信息检索指标，使用受试者的切分来评估话语切分与三种语言线索（指称名词短语、提示词和停顿）的相关性。我们将多个手动分段合并为一个只包含大多数编码人员同意的边界的分段。基于语篇切分目的，我们采用了一种扁平的主题切分模型作为黄金标准。"}
{"pid": "P03-1071", "zh_sum": "本文提出了一种独立于领域的多方会话主题切分算法。我们的基于特征的算法结合了使用基于文本的算法作为特征的内容知识，以及使用语言和声学线索从语音中提取主题转移的形式知识。该分割算法使用自动归纳的决策规则来组合不同的特征。嵌入式基于文本的算法建立在词汇衔接的基础上，其性能与基于词汇信息的最新算法相当。通过将这两种知识源相结合，可以显著减少误差。我们通过考虑至少三种人类注释之间的一致性，为主题分割提供了黄金标准。我们提出了基于词汇链的无监督分词器（LCSeg）和有监督分词器来分割会议记录。我们的LCseg系统是唯一一个基于ICSI会议数据评估的文字分发系统。"}
{"pid": "P98-1034", "zh_sum": "错误驱动的树库语法修剪用于基本名词短语识别查找简单、非递归的基本名词短语是许多自然语言处理应用程序的一个重要子任务。虽然以前的基本NP识别的经验方法相当复杂，但本文提出了一种非常简单的算法，该算法适合于任务的相对简单性。特别是，我们提出了一种基于语料库的方法，通过匹配词性标记序列来找到基本的NPs。该算法的训练阶段基于两种成功的技术：首先从“树库”语料库中读取基本NP语法；然后，通过选择“有益”分数较高的规则来改进语法。使用这个简单的算法和简单的启发式匹配规则，我们在《宾夕法尼亚州Treebank华尔街日报》上的一次评估中获得了惊人的准确率。我们存储构成完整数据块的POS标记序列，并将这些序列用作对不可见数据进行分类的规则。"}
{"pid": "N09-1046", "zh_sum": "使用最大熵模型为机器翻译构建切分格最近的工作表明，翻译切分格（将机器翻译系统的输入分解为单词的替代方法编码的格）而不是任何特定切分中的文本，可以提高正字法不标记语素边界的语言的翻译质量。然而，这项工作的大部分依赖于在相同输入上执行不同操作的多个分段器，以生成足够多样化的源分段格。在这项工作中，我们描述了一个复合词拆分的最大熵模型，该模型依赖于一些通用特征，这些特征可用于生成大多数具有生产性复合的语言的切分格。使用针对德语翻译优化的模型，我们给出的结果显示，与最先进的基线相比，德语-英语、匈牙利英语和土耳其英语翻译的翻译质量有了显著提高。我们发现，在我们的分割模型中，单图权重是一种有效的特征。"}
{"pid": "W02-2024", "zh_sum": "CoNLL-2002共享任务简介：语言无关命名实体识别我们描述了CoNLL-2002共享任务：语言无关命名实体识别。我们提供了有关数据集和评估方法的背景信息，概述了参与任务的系统，并讨论了它们的性能。我们专注于西班牙语和荷兰语的命名实体识别。"}
{"pid": "P97-1013", "zh_sum": "无限制自然语言文本的修辞分析我们通过两种新的、基于表面形式的算法来推导文本的修辞结构：一种是识别提示短语的话语用法并将句子分解为子句，另一种是为无限制自然语言文本生成有效的修辞结构树。这些算法使用来自线索短语语料库分析的信息。我们描述了一种基于层次片段的核性和选择性保留的文本摘要方法。"}
{"pid": "C08-1109", "zh_sum": "英语写作中介词错误检测的起伏本文描述了一种检测非英语母语者写作中介词错误的方法。我们的系统对大量学生作文的准确率为84%，召回率接近19%。此外，我们还通过展示当前仅使用一个评分员的方法如何影响系统评估来解决该领域的注释和评估问题。我们提出了一种抽样方法，以避免使错误检测系统评估复杂化的一些问题。我们使用托福数据。我们发现，两个母语为英语的人在以介词为目标的完形填空测试中的一致性约为76%，这表明有许多上下文允许使用多个介词。我们的模型是用词汇特征训练的。"}
{"pid": "N04-4015", "zh_sum": "统计机器翻译中的形态分析我们提出了一种新的形态分析技术，该技术可以在具有高度不对称形态结构的两种语言之间诱导形态和句法对称，以提高统计机器翻译的质量。该技术预先假设，在形态丰富的语言中，单词的细粒度切分为平行语料库的前缀（es）-词干后缀（es）和词性标记序列。该算法在形态丰富的语言中识别要合并或删除的语素，以诱导所需的形态和句法对称。当应用于IBM Model 1和短语翻译模型时，该技术显著提高了阿拉伯语到英语的翻译质量，这些模型是在3500到330万句句子对的训练语料库上训练的。我们发现，在阿拉伯语到英语的翻译系统中，阿拉伯语句子中的限定词切分和删除可以改善句子对齐，从而提高整体翻译质量。"}
{"pid": "P93-1016", "zh_sum": "在以前的基于原则的解析器中，没有过度生成的基于原则的解析过度生成是计算复杂性的主要来源。本文提出了一种基于原理的语法分析的消息传递算法，避免了过度生成问题。该算法已在C++中实现，并用来自（van Riemsdijk和Williams，1986）的示例语句进行了成功测试。我们的解析器为句子中的组件生成功能关系，包括与动词相关的主语和宾语关系。在我们的依赖树中，节点表示文本表达式，边表示它们之间的语法关系。"}
{"pid": "J03-3005", "zh_sum": "使用Web获取未看到的Bigram的频率本文表明，可以使用Web获取给定语料库中未看到的Bigram的频率。我们描述了一种通过查询搜索引擎从Web检索形容词-名词、名词-名词和动词-宾语二元图计数的方法。我们通过论证来评估这种方法：（a）网络频率和语料库频率之间的高度相关性；（b） Web频率和合理性判断之间的可靠相关性；（c） Web频率和使用基于类的平滑重新创建的频率之间的可靠相关性；（d） 在伪消歧任务中，Web频率表现良好。我们的研究表明，可用于网络统计的大量数据可能会超过噪音问题。"}
{"pid": "W97-0209", "zh_sum": "选择偏好和语义消歧缺乏训练数据是基于语料库的语义消歧方法的一个实际问题，这个问题不太可能很快得到解决。选择偏好传统上与意义歧义有关；本文探讨了一种既不需要人工标注选择限制也不需要监督训练的选择偏好统计模型如何用于语义消歧。我们将选择偏好定义为动词提供的关于其语义参数类的信息量。基于某些语言谓词将潜在单词的语义解释限制在某些类别中的思想，我们提出了一种利用选择偏好获取一组词义概念类的方法。在确定选择偏好时，我们使用在给定单词的所有感官上均匀分布的观察频率。"}
{"pid": "P89-1010", "zh_sum": "词汇联想规范了互信息和词典编纂。词汇联想一词在心理语言学文献中有着非常特殊的意义。（一般来说，如果单词“护士”跟在“医生”等高度相关的单词后面，受试者对该单词的反应会比正常人更快）。我们将扩展该术语，为统计描述各种有趣的语言现象提供基础，从医生/护士类型的语义关系（内容词/内容词）到动词和介词（内容词/虚词）之间的词汇-句法共现约束。本文将基于互信息的信息论概念，提出一种新的客观度量方法，用于从计算机可读语料库中估计词语联想规范。（获取单词联想规范的标准方法是用几百个单词测试几千名受试者，这种方法既昂贵又不可靠）。所提出的关联比率（association ratio）直接从计算机可读语料库中估计单词关联规范，从而可以估计数万个单词的规范。"}
{"pid": "J00-4005", "zh_sum": "本文认为，在MUC社区中执行的“共指”注释远远超出了对共指关系本身的注释。因此，并不总是清楚这些注释编码的是什么语义关系。本文讨论了这些注释存在的一些问题，并得出结论，在扩展任务之前，需要重新思考共指任务。特别是，它提出了一种分工，将共指关系本身的注释与其他任务分离开来，例如对有界回指和主语与谓语NP之间的关系的注释。然而，它面临着许多问题（van Deemter和Kibble，2000），其中最主要的问题是，由scheme表示的一个语义关系ident将语义学家视为不同的一些关系合并在一起：除了正确的共指之外，还有身份回指、绑定回指，甚至谓词。"}
{"pid": "J88-2006", "zh_sum": "作为语篇回指的时态在本文中，我考虑了一系列英语表达，并表明它们的语境依赖性可以从两个方面来表征：1。它们指定了听者正在构建的话语演化模型中的实体；指定的特定实体依赖于听者目前正在关注的不断发展的“话语模式”中的另一个实体。这种表达被称为回指。我展示了时态从句如何具有这些特征，通常只归因于回指名词短语。这不仅使我们能够以一种简单的方式捕捉到时态是回指的这一经常陈述但难以证明的直觉，而且有助于我们了解理解叙事文本所需的知识。我们通过指定话语中事件如何相互关联的规则以及Sing和Sing定义的语义约束来改进上述工作，通过这些语义约束事件可以相互关联（Sing，1997）。"}
{"pid": "W04-3250", "zh_sum": "机器翻译评价的统计显著性检验机器翻译（MT）系统的自动评价指标，如BLEU、METEOR和相关的NIST指标，在机器翻译研究和开发中变得越来越重要。本文提出了一种基于n-gram的机器翻译自动评估度量的显著性测试驱动比较方法。统计显著性检验使用自举方法来估计自动机器翻译评估的可靠性。基于这种可靠性评估，我们研究了不同机器翻译评估指标的特点以及如何构建可靠、高效的评估套件。"}
{"pid": "N09-1041", "zh_sum": "探索多文档摘要的内容模型我们提出了一种用于多文档摘要的生成概率模型。从一个简单的基于词频的模型（Nenkova和Vanderwende，2005）开始，我们构建了一系列模型，每个模型都为文档集内容的表示注入了更多的结构，并显示了一路走来的胭脂收益。我们的最终模型HIERSUM利用分层LDA风格模型（Blei et al.，2004）将内容特定性表示为主题词汇分布的层次结构。在制作通用的DUC风格摘要的任务中，HIERSUM产生了最先进的胭脂效果，在成对的用户评价中，其表现明显优于Toutanova et al.（2007）的最先进的鉴别系统。我们还探讨了HIERSUM制作多个“主题摘要”的能力，以促进内容发现和导航。在TOPICSUM中，每个词都是由单个主题生成的，该主题可以是通用词的整个语料库背景分布、文档特定词的分布或给定集群核心内容的分布。我们构建了一个基于主题模型的摘要系统，在该系统中，既可以学习一般文档级别的主题，也可以学习特定子主题级别的主题。"}
{"pid": "P85-1018", "zh_sum": "使用限制扩展基于复杂特征的语法分析算法基于复杂值特征系统中语法信息编码的语法形式在语言学和自然语言处理研究中具有一定的通用性。通过类比上下文无关语法，可以认为这种形式化将非终结符号的概念从原子元素的有限域推广到某种有向图结构的可能无限域。不幸的是，在移动到无限非终结域时，标准解析方法可能不再适用于形式主义。通常，问题表现为算法的总体效率低下，甚至未终止。在本文中，我们讨论了将解析算法扩展到可能具有无限个非终结域的形式的问题的解决方案，该解决方案基于一种我们称之为限制的通用技术。作为这种扩展的一个特殊示例，我们提供了Earley算法的一个完整、正确、终止的扩展，该扩展使用限制来执行自顶向下的过滤。我们对该算法的实现证明了该技术可以极大地消除图表边缘。最后，我们描述了该技术的进一步用途，包括解析其他语法形式，包括定冠词语法；扩展其他解析算法，包括LR方法和语法偏好建模算法；以及高效的索引。我们提出了一个改进版的Earley解析器，使用受限的自顶向下预测。"}
{"pid": "J03-3002", "zh_sum": "Web作为一种平行语料库，已经成为多语言自然语言处理工作中必不可少的资源。在本文中，我们报告了使用STRAND系统在万维网上挖掘并行文本的工作，首先回顾了原始算法和结果，然后介绍了一组重要的增强。这些改进包括使用基于文档结构特征的监督学习来提高分类性能，一种新的基于内容的翻译等效性度量，以及对系统进行调整，以利用互联网档案从Web大规模挖掘平行文本。最后，通过构建低密度语言对的重要平行语料库，证明了这些技术的价值。我们首先在双语网站中挖掘平行web文档，然后使用句子对齐方法从挖掘出的平行文档中提取双语句子。我们利用URL结构、文档结构和其他线索的相似性来挖掘Web中的并行文档。"}
{"pid": "P05-3026", "zh_sum": "在显式词匹配指导下的多引擎机器翻译我们描述了一种综合组合多个不同机器翻译引擎在相同输入上的输出的新方法。目标是生成一个综合的组合，在翻译质量上超过所有原始系统。我们的方法将单个MT引擎用作“黑匣子”，不需要原始MT系统的任何明确合作。解码算法使用显式单词匹配，结合各种引擎的置信度估计和三元语言模型，以便对一组句子假设进行评分和排序，这些假设是来自各种原始引擎的单词的合成组合。我们选择得分最高的句子假设作为系统的最终输出。使用几个质量相似的阿拉伯语到英语系统进行的实验表明，翻译输出的质量有了显著提高。我们提出了一种基于启发式的匹配算法，该算法允许非单调对齐来对齐假设之间的单词。"}
{"pid": "A00-2034", "zh_sum": "使用语义偏好来识别角色转换交替中的言语参与，我们提出了一种识别素质交替的方法，其中在交替形式中具有不同语法角色的插槽中可以看到特定的参数类型。该方法使用所获得的选择偏好作为WordNet上的概率分布。使用分布相似性度量来比较目标槽的偏好。该方法是根据因果交替进行评估的，但通常是适用的，不需要特定于交替的先验知识。我们使用倾斜差异（Lee，1999年提出的KL差异的一种变体）来比较一个动词的一个变元（例如，不及物的主语位置）与同一个动词的另一个变元（例如，及物的宾语位置）的语义特征，以确定动词是否参与了涉及这两个位置的变元交替。"}
{"pid": "D07-1074", "zh_sum": "基于Wikipedia数据的大规模命名实体消歧本文提出了一个基于从大型百科全书集合和Web搜索结果中提取的信息的大规模命名实体识别和语义消歧系统。它详细描述了所采用的消歧范式和从维基百科中提取信息的过程。通过最大化从Wikipedia提取的上下文信息与文档上下文之间的一致性，以及与候选实体相关的类别标记之间的一致性，实现的系统在新闻故事和Wikipedia文章上都显示出较高的消歧准确性。我们发现，候选实体和上下文中其他实体之间的主题一致性将提高NED的准确性。我们使用由从维基百科中提取的短语和类别组成的上下文向量。我们引入了一个实体消歧数据集。我们利用维基百科中的明确类别信息。为了进行评估，我们使用了来自MSNBC的20篇新闻报道，其中642篇实体提及手动链接到维基百科，另外113篇提及没有任何相应的维基百科链接。"}
{"pid": "J97-3002", "zh_sum": "随机倒装转换语法与平行语料库的双语分析我们介绍了（1）一种新的用于句子对双语建模的随机倒装转换语法形式，以及（2）各种平行语料库分析应用的双语分析概念。除了双语取向外，形式主义与计算语言学中更为传统的有限状态变换器有三个主要区别：它直接跳转到上下文无关而非有限状态基，允许最小程度的额外排序灵活性，它的概率公式支持一种有效的最大似然双语句法分析算法。一个方便的范式被证明是存在的。对形式主义表达能力的分析表明，它特别适合于建模语言之间的顺序转换，平衡所需的灵活性和复杂性约束。我们讨论了一些例子，说明随机倒装转换语法如何将双语约束带到有问题的语料库分析任务上，如切分、括号、短语对齐和句法分析。我们使用一种内外类型的训练算法来学习统计上下文无关的转换。我们的双语括号是为汉英词语对齐研究的双语浅层句法分析方法之一。我们介绍了一种基于同步二叉树的对齐问题的多项式时间解。"}
{"pid": "P08-1036", "zh_sum": "用于情感总结的文本和方面评分联合模型在线评论通常伴随着用户对一组服务或产品方面提供的数字评分。我们提出了一个统计模型，该模型能够发现文本中相应的主题，并从支持每个方面评级的评论中提取文本证据，这是基于方面的情感总结中的一个基本问题（Hu和Liu，2004a）。我们的模型达到了很高的准确性，除了用户提供的意见评分之外，没有任何明确标记的数据。该方法具有通用性，可用于序列数据伴随相关信号的其他应用中的分割。相比之下，MLSLDA利用了将情绪视为基于文档中使用的主题的回归问题的技术，如监督潜在Dirichlet分配（SLDA）（Blei和McAuliffe，2007）或文档的细粒度部分（Titov和McDonald，2008）。我们提出了一个文本和方面评分的联合模型，该模型利用改进的LDA主题模型来构建代表可评分方面的主题，并构建了一组情绪预测因子。"}
{"pid": "N06-2015", "zh_sum": "OntoNotes：90%的解决方案我们描述了OntoNotes方法及其结果，即一个以90%的注释间一致性构建的大型多语言丰富注释语料库。2007年，将向社区提供最初的部分（30万字的英文新闻专线和25万字的中文新闻专线）。Ontonotes包含大量数据源，如广播新闻、新闻专线、杂志、网络文本等。在Ontonotes项目（Hovy等人，2006年）中，注释者使用小规模语料库分析，通过将WordNet词义组合在一起，创建词义目录，其过程仅限于保持90%的注释者之间的一致性。"}
{"pid": "D07-1080", "zh_sum": "统计机器翻译的在线大幅度训练通过在线大幅度训练算法使用大量特征，我们在统计机器翻译方面取得了最先进的性能。数百万个参数只在一个不到1K句的小开发集上进行了调整。阿拉伯语到英语的翻译实验表明，使用稀疏二进制特征训练的模型优于使用少量特征的传统SMT系统。我们在之前翻译的一组句子的上下文中执行蓝色计算。我们发现数据集中存在过度拟合的可能性（阿拉伯语-英语新闻专线翻译），尤其是当存在域差异时。"}
{"pid": "J94-4002", "zh_sum": "代词回指消解算法本文提出了一种识别第三人称代词和词汇回指（反身代词和倒数代词）的名词短语先行词的算法。该算法适用于由McCord的槽语法分析器生成的句法表示，并依赖于从句法结构和简单的注意状态动态模型得出的显著性度量。与解析器一样，该算法也是在Prolog中实现的。作者在计算机手册文本上进行了广泛的测试，并对包含360个代词出现的手册文本进行了盲测试。该算法在86%的代词出现中成功地识别了代词的先行词。在此盲测试中，检查了算法组件对其整体成功率的相对贡献。对该算法进行了改进，将语义和现实世界关系的统计建模信息提供给该算法的决策过程。有趣的是，这种增强只略微提高了算法的性能（2%）。该算法与文献中提出的其他回指消解方法进行了比较。特别是，霍布斯算法的搜索过程在时隙语法框架中实现，并应用于盲测试集中的句子。作者的算法实现了更高的成功率（4%）而不是霍布斯的算法。讨论了该算法与中心法的关系，以及与回指消解模型的关系，回指消解模型调用了各种信息因素对先行候选词进行排序。在基于显著性的启发式代词消解算法中，我们介绍了一种将回指关联NP识别为一个聚类的过程，其中全局显著性值是其元素显著性值之和。我们描述了一种代词回指消解算法，该算法实现了较高的正确分析率（85%）。"}
{"pid": "A97-1004", "zh_sum": "最大熵方法识别句子边界我们提出了一个可训练的模型来识别原始文本中的句子边界。给定一个带有句子边界注释的语料库，我们的模型学习对.，？，的每次出现进行分类？，和/或作为有效或无效的句子边界。培训过程不需要手工编制的规则、词典、词性标记或特定领域的信息。因此，该模型可以很容易地在任何类型的英语上进行训练，并且应该可以在任何其他罗马字母语言上进行训练。性能可与类似系统媲美或优于类似系统的性能，但我们强调新领域再培训的简单性。我们的统计系统mxTerminator使用候选句点左侧和右侧单词的更简单词汇特征。"}
{"pid": "P03-1013", "zh_sum": "基于姐妹头依赖的德语概率句法分析我们提出了一个在Negra树库上训练的德语概率句法分析模型。我们观察到，现有的使用head-head依赖的词汇化句法分析模型虽然在英语中取得了成功，但在德语中却没有优于非词汇化基线模型。学习曲线表明，这种影响不是由于缺乏训练数据造成的。我们提出了一种替代模型，该模型使用姐妹头依赖而不是头头依赖。该模型优于基线，实现了高达74%的标记精度和召回率。这表明姐妹头依赖更适合于具有非常平坦结构（如Negra）的树组。我们表明，对于其他语言、注释或域，完全词汇化不会跨越不同的树库。我们表明，分配正确的语法功能比单纯的基于成分的句法分析更困难。"}
{"pid": "P11-1020", "zh_sum": "为释义评估收集高度平行的数据缺乏标准数据集和评估指标，使得释义领域无法取得过去15年来机器翻译界所享有的那种快速进步。我们通过提出一种新的数据收集框架来解决这两个问题，该框架以相对低廉的成本大规模生成高度并行的文本数据。这些数据的高度平行性使我们能够使用简单的n-gram比较来衡量释义候选词的语义充分性和词汇差异性。实验表明，除了计算简单高效外，这些指标还与人类的判断高度相关。我们的数据集由1500对使用众包收集的短视频描述组成。"}
{"pid": "N01-1020", "zh_sum": "基于桥语言的多路径翻译词典归纳本文提出了一种基于桥语言同源对转换模型的翻译词典归纳方法。使用概率字符串编辑距离模型归纳语系中的双语词汇。然后，通过这些族内翻译模型和一个或多个跨族在线词典的组合，生成任意远程语言对的翻译词典。目标词汇的精确匹配准确率高达95%（30-68%的族间测试对）。因此，对于不存在双语词典或平行语料库的语言，可以准确地生成大部分翻译词典。我们提出了一种基于桥语言同源对的转换模块来归纳翻译词汇的方法。我们提出了一种基于桥语言同源对转换模型的翻译词汇归纳方法。"}
{"pid": "N09-1012", "zh_sum": "通过更丰富的上下文改进无监督依赖分析，并平滑无监督语法归纳模型，与有监督的对应模型相比，往往使用相对简单的语法模型。传统上，由于易处理性和数据稀疏性的考虑，无监督模型一直保持简单。在本文中，我们将基本配价框架和词汇信息引入到无监督依赖语法诱导器中，并展示如何通过平滑利用这些附加信息。我们的模型在无监督语法归纳的任务上产生了最先进的结果，比以前最好的工作提高了近10个百分点。我们使用频率超过100的词汇值，并定义了捆绑概率上下文无关语法（PCFG）和Dirichlet先验，提高了准确性。我们还通过学习儿童概率的退避分布，为E-DMV实现了一种参数绑定。"}
{"pid": "P98-1069", "zh_sum": "从非平行可比文本中翻译新词的IR方法我们证明，一个词与其上下文种子词之间的关联在不同语言的可比文本中得以保留。我们建议用实值向量表示单词或短语的上下文，其中一个元素对应上下文中的一个单词。"}
{"pid": "W11-2107", "zh_sum": "Meteor 1.3：机器翻译系统可靠优化和评估的自动度量本文介绍Meteor 1.3，我们提交给2011年EMNLP统计机器翻译自动评估度量任务研讨会的文件。新的度量特征包括改进的文本规范化、更高精度的释义匹配以及内容和虚词之间的区分。我们包括与人类对翻译质量的判断高度相关的指标的排名和适当性版本，以及在基于短语的乌尔都语英语系统的最低错误率培训中表现出优于BLEU的更平衡的调整版本。"}
{"pid": "W95-0103", "zh_sum": "最近的工作考虑了基于语料库或统计的方法来解决介词短语依附歧义问题。通常，v-rip1-p-rip2形式的歧义动词短语通过一个考虑四个中心词（v、nl、p和77,2）值的模型来解决。本文表明，该问题类似于语音识别中的n-gram语言模型，并且最常用的语言建模方法之一&回退估计是适用的。使用该方法，在《华尔街日报》数据上获得的结果准确率为84.5%。我们使用回退模型，这使他们能够在Ratnaparkhi数据集上考虑低频效应（具有良好的结果）。我们对Ratnaparkhi et al（1994）的数据集进行了修改，旨在消除数据稀疏性，并使用修改后的版本来训练其后退模型。"}
{"pid": "P11-1016", "zh_sum": "基于推特数据的目标相关推特情绪分类情绪分析近年来备受关注。本文主要研究目标相关的推特情感分类；也就是说，给定一个查询，我们根据推文中是否包含对该查询的积极、消极或中性情绪，将推文中的情绪分为积极、消极或中性。在这里，这个问题是情感的目标。解决这一问题的最新方法总是采用与目标无关的策略，这可能会给给定的目标分配不相关的情感。此外，最先进的方法仅在对情绪进行分类时考虑要分类的推特；他们忽略其上下文（即相关推文）。然而，由于推特通常较短且更加模糊，有时仅考虑当前推特进行情绪分类是不够的。在本文中，我们建议通过1）结合目标相关特征改进目标相关的推特情感分类；2）考虑相关推文。实验结果表明，该方法大大提高了目标相关情感分类的性能。我们结合了目标相关的特征，并利用基于图的优化来考虑相关的推文。我们将与目标无关的特征（内容和词汇）和与目标相关的特征（基于依赖性分析结果的规则）结合在一起，对推特进行主观性分类和极性分类。"}
{"pid": "J98-4003", "zh_sum": "机器音译在不同字母和声音目录的语言中翻译姓名和技术术语是一项挑战。这些项目通常是音译的，即用近似的语音等价物代替。例如，英语中的“computer”在日语中被称为“konpyuutaa”。将这些项目从日语翻译回英语更具挑战性，也更具实际意义，因为音译项目构成了双语词典中未发现的大部分文本短语。我们描述并评估了一种通过机器执行向后音译的方法。这种方法使用生成模型，在音译过程中包含几个不同的阶段。我们提出了一种基于英日片假名语音映射概率的日英音译方法。"}
{"pid": "N01-1016", "zh_sum": "转录语音的编辑检测和解析我们提出了一种简单的转录语音解析体系结构，其中，编辑的单词检测器首先从句子字符串中删除这些单词，然后由一个在转录语音上训练的标准统计解析器解析剩下的单词。编辑检测器对已编辑单词的误分类率为2.2%。（将所有内容标记为未编辑的空模型的错误率为5.9%。）为了评估我们的解析结果，我们引入了一个新的评估指标，其目的是使解析树的评估与编辑节点的确切树位置相对无关。通过这个度量，解析器实现了85.3%的准确率和86.5%的召回率。我们在统计分析会话语音方面的工作检查了在前面步骤中删除编辑区域的解析器的性能。"}
{"pid": "C04-1197", "zh_sum": "基于整数线性规划推理的语义角色标注我们提出了一个语义角色标注系统。该系统将机器学习技术与基于整数线性规划的推理过程相结合，支持将语言和结构约束纳入决策过程。该系统在CoNLL-2004语义角色标注共享任务中提供的数据上进行了测试，取得了非常有竞争力的结果。我们将SRL描述为一个逐成分（C-by-C）标记问题。"}
{"pid": "N06-1003", "zh_sum": "使用释义平行语料库改进统计机器翻译对于培训SMT系统至关重要。然而，对于许多语言对来说，它们的数量非常有限。对于这些语言对，在运行时遇到的大量短语将是未知的。我们展示了如何使用意译技术来处理这些未知的源语言短语。我们的研究结果表明，通过释义来增强最先进的SMT系统可以显著提高覆盖率和翻译质量。对于一个包含10000个句子对的训练语料库，我们将唯一测试集单格的覆盖率从48%提高到90%，新覆盖的项目有一半以上被准确翻译，而目前的方法没有。我们提出了一种新的方法，用意译代替输入句子中未知的源词或短语，然后继续使用该意译的翻译生成目标语言结果。"}
{"pid": "P11-2031", "zh_sum": "更好的统计机器翻译假设检验：控制统计机器翻译中优化器的不稳定性，研究人员试图确定与基线系统相比，某些创新（例如，新的特征、模型或推理算法）是否提高了翻译质量。为了回答这个问题，他进行了一个实验，以评估这两个系统在所提供数据上的行为。在本文中，我们考虑如何使这些实验在统计学上更可靠。我们对优化器不稳定性（一个很少控制的无关变量）对实验结果的影响进行了系统分析，并提出了更准确报告结果的建议。我们实现了一个分层近似随机化测试来解释多个调优复制。"}
{"pid": "P03-1022", "zh_sum": "口语对话中代词消解的机器学习方法我们将基于决策树的方法应用于口语对话中的代词消解。我们的系统处理具有NP-和非NP-先行词的代词。我们提出了一组用于口语对话中代词解析的特征，并确定了最有希望的特征。我们在20个交换机对话中评估了该系统，并表明它与Byron（2002）的手动调谐系统相比效果良好。我们的目标是找到可用特征的子集，由此产生的共指分类器可以在保留的数据上产生最佳的聚类级别精度。"}
{"pid": "N09-2004", "zh_sum": "SMT的语义角色：一个混合的双程模型我们给出了一个新的混合语义SMT模型的结果，该模型融合了语义角色标记和基于短语的统计机器翻译的优点。该方法通过双通道架构避免了主要的复杂性限制。第一步是使用传统的基于短语的SMT模型执行的。第二步是由浅层语义解析器引导的重新排序策略执行，浅层语义解析器生成语义框架和角色标签。对《华尔街日报》（Wall Street Journal）新闻专线（newswire）体裁测试集的评估表明，混合模型在BLEU得分方面比基于短语的强SMT基线提高了大约半个百分点，据我们所知，这是语义角色标记首次成功应用于SMT。我们对翻译输出进行语义角色标注，并对参数进行重新排序，以最大限度地实现源语句和目标译文之间语义框架的跨语言匹配。"}
{"pid": "C10-2028", "zh_sum": "使用推特标签和笑脸自动识别不同情绪类型的增强情绪学习对于许多NLP系统，如评论摘要和公共媒体分析，都是有益的。在其中一些系统中，可以选择为单个句子或非常短的文本指定情感值。在本文中，我们提出了一个基于推特（一种流行的微博服务）数据的监督情绪分类框架。通过使用50个推特标签和15个笑脸作为情感标签，该框架避免了劳动密集型手动注释的需要，允许识别和分类不同类型的短文本情感。我们评估了不同特征类型对情感分类的贡献，并表明我们的框架成功地识别了未标记句子的情感类型。人类法官也证实了情感识别的质量。我们还探讨了笑脸和推特标签所代表的不同情绪类型之间的依赖关系和重叠。我们使用50个散列标签和15个表情符号作为噪音标签，创建了一个用于推特情感分类的数据集。"}
{"pid": "J98-4004", "zh_sum": "语言树表示的PCFG模型树库语料库中使用的树表示类型可以对基于该语料库估计的PCFG的解析器的性能产生显著影响，导致树的估计可能性与其在训练语料库中的频率有很大差异。本文指出，Penn II树库表示具有这种效果，并描述了一种简单的节点重新标记转换，该转换可将基于树库PCFG的解析器的平均精度和召回率提高约8%，或约为简单PCFG模型与目前可用的最佳广泛覆盖解析器之间性能差异的一半。产生这种性能差异的原因是，任何PCFG，以及由此产生PCFG的树的语料库，都体现了关于单词和短语分布的独立假设。树表示中隐含的特定独立性假设可以通过树变换/去变换过程进行理论研究和实证研究。我们通过树中的父类别对每个节点进行注释，与Penn Treebank上的原始PCFGs相比，我们取得了显著的改进。"}
{"pid": "N01-1011", "zh_sum": "二元图的决策树是词义的准确预测器。本文提出了一种基于语料库的词义消歧方法，其中决策树根据附近出现的二元图为歧义词指定词义。这种方法是使用1998年SENSEVAL词义消歧练习中的词义标记语料库进行评估的。它比36个单词中30个的平均结果更准确，比36个单词中19个的最佳结果更准确。我们比较了决策树、决策树桩和朴素贝叶斯分类器，以表明bigrams在识别单词的意图方面非常有用。"}
{"pid": "W95-0104", "zh_sum": "上下文相关拼写纠正的贝叶斯混合方法两类方法已被证明对解决词汇歧义非常有用。第一种依赖于特定单词在歧义目标词一定距离内的存在；第二种是在目标单词周围使用单词模式和词性标记。这些方法具有互补性：前者捕捉词汇“气氛”（话语主题、时态等），而后者捕捉局部句法。亚洛夫斯基利用决策列表将这两种方法结合起来，利用了这种互补性。其思想是汇集组件方法提供的证据，然后通过应用单个最强的证据来解决目标问题，不管它是什么类型的。本文以Yarowsky的工作为出发点，将决策表应用于上下文相关拼写更正问题。人们发现，决策列表总体上优于任何一种组件方法。然而，我们发现，通过不仅考虑最有力的证据，而且考虑所有可用证据，可以获得进一步的改进。为此，提出了一种基于贝叶斯分类器的混合方法，并对其性能进行了改进。我们基于一组丰富的上下文特征构建了一个分类器。"}
{"pid": "D08-1065", "zh_sum": "统计机器翻译中的格最小贝叶斯风险解码我们提出了压缩编码大量翻译假设的翻译格上的最小贝叶斯风险（MBR）解码。我们描述了损失函数的条件，这将使MBR解码器能够在晶格上高效实现。我们介绍了满足这些条件的BLEU分数近似值（Papineni et al.，2001）。该近似BLEU下的MBR译码是用加权有限状态自动机实现的。我们的实验表明，在阿拉伯语到英语、汉语到英语和英语到汉语的翻译任务中，Lattice MBR解码器在翻译性能上比N-best MBR解码器有适度的、一致的增益。我们进行了一系列实验，以了解为什么Lattice MBR改进了N-best MBR，并研究了各种参数对MBR性能的影响。我们考虑BLEU对数的泰勒近似。我们将MBR扩展到word晶格，这比k-best list MBR提高了性能。log-BLEU函数必须稍加修改，以产生线性泰勒近似：我们用一个n-gram计数和一个n-gram指示符函数的乘积来替换裁剪的n-gram计数。我们通过将平移格与每个n-gram t的格相交来计算期望的特征值。"}
{"pid": "P00-1010", "zh_sum": "本文介绍了一种时态表达式的注释方案，并描述了一种在印刷和广播新闻中解析时态表达式的方法。该系统基于手工和机器学习的规则，对手工标注的数据达到83.2%的准确率（F-measure）。还描述了标记事件年表的一些初始步骤。该系统的主要部分是一个时间表达式标记器，它使用基于手写规则的有限状态传感器。我们致力于新闻，并介绍了时态表达式的注释方案，以及使用显式时态表达式为整个文章分配活动时间的方法。我们将基线方法一半以上的错误归因于错误事件时间向相邻事件的传播。"}
{"pid": "C04-1146", "zh_sum": "词汇分布相似度的表征这项工作研究了单词的分布近邻相对于所用相似度的变化。我们确定了一种变异类型，即相邻单词相对于目标单词频率的相对频率。然后，我们论证了相似词的相对频率、分布单义性概念和上下义性语义关系之间的三方面联系。最后，我们考虑这对分布相似性方法的一个应用（判断搭配的组成性）的影响。从具体测试集的结果中抽象出来，我们试图根据相似性度量的性能来确定统计和语言特性。我们还发现，频率在决定蕴涵的方向方面起着很大的作用，更一般的术语往往出现得更频繁。我们分析了一个单词在不同相似度下的分布变化。我们试图完善分布相似性目标，以预测一个术语是否是另一个术语的泛化/规范。"}
{"pid": "D10-1124", "zh_sum": "地理词汇变异的潜在变量模型地理标记社交媒体的迅速发展为研究地理语言变异带来了新的计算可能性。在本文中，我们提出了一个关于潜在主题和地理区域的多层次生成模型。“体育”或“娱乐”等高层次主题在每个地理区域的呈现方式不同，揭示了特定主题的区域差异。应用于一个新的地理标记微博数据集，我们的模型恢复了连贯的主题及其区域变体，同时确定了语言一致性的地理区域。该模型还可以根据原始文本预测作者的地理位置，优于文本回归和监督主题模型。我们在推特网站上收集了9250名微博用户的文字和地理位置。com来构造数据集。我们从推特的官方API收集了大约38万条推特。作为层次贝叶斯模型的一部分，我们根据地球表面的高斯分布预测位置。我们将用户的所有推文视为一个单独的文档，并使用最早收集的GPS指定位置作为黄金位置。"}
{"pid": "P07-1005", "zh_sum": "词义消歧改善统计机器翻译最近的研究表明，词义消歧（WSD）系统是否有助于提高统计机器翻译（MT）系统的性能。在本文中，我们成功地将最先进的WSD系统集成到最先进的基于短语的分层机器翻译系统Hiero中。我们首次表明，集成WSD系统可以提高最先进的统计机器翻译系统在实际翻译任务中的性能。此外，这种改善在统计上是显著的。我们使用单词的局部和跨句单格搭配来训练WSD的判别模型，以便通过合并WSD分类器的分数来动态优化短语对选择。我们使用基于SVM的分类器来消除词义的歧义，这些词义通过作为对数线性组合模型一部分的附加特征直接合并到解码器中。"}
{"pid": "P04-1015", "zh_sum": "感知器算法的增量解析本文描述了一种增量解析方法，其中使用感知器算法的一种变体来估计参数。在该方法的训练和解码阶段均使用波束搜索算法。感知器方法使用与现有生成模型相同的特征集实现（Roark，2001a），实验结果表明，感知器方法在解析Penn树库方面具有与生成模型相当的性能。我们证明，在搜索过程中训练感知器模型与生成模型相结合，与仅生成模型相比，F-度量提高了2.1%，达到88.8%。我们提出了一种早期更新机制，每当单个gold动作落在波束之外时，停止解码以更新模型权重。当解析器利用有关最后标点和前瞻的信息时，我们的解析器可以获得最佳结果（LR=88.4%、LP=89.1%和F=88.8%）。采用早期更新策略，以提高准确性和加快训练速度。"}
{"pid": "W11-1901", "zh_sum": "CoNLL-2011共享任务：在OntoNotes中建模无限制的共指。CoNLL-2011共享任务涉及使用OntoNotes数据预测共指。该领域的资源往往局限于名词短语的共指，通常是在一组有限的实体上，如ACE实体。OntoNotes提供了大规模的一般回指共指语料库，不仅限于名词短语或一组特定的实体类型。OntoNotes还提供了额外的集成注释层，捕获额外的浅层语义结构。本文简要描述了ontonnotes注释（coreference和其他层），然后描述了共享任务的参数，包括格式、预处理信息和评估标准，并介绍和讨论了参与系统所取得的结果。拥有一个标准的测试集和评估参数，所有这些都基于一个新的资源，该资源提供了多个集成的注释层（语法分析、语义角色、词义、命名实体和协同引用），可以支持联合模型，这应有助于激发实体和事件协同引用任务中正在进行的研究。此处概述了参与CONLL-2011共享任务的所有系统及其结果。"}
{"pid": "P10-1146", "zh_sum": "在过去几年中，人们广泛采用了试图捕捉语言递归结构的源语法和目标语法统计翻译模型。这些模型利用了语言学理论中不同数量的信息：一些完全不使用，一些使用目标语言语法的信息，一些使用源语言语法的信息。但是，在能够了解源语言和目标语言语法之间关系的翻译模型方面，进展较慢。我们讨论了这一挑战的原因，回顾了现有的应对挑战的尝试，并展示了如何将一些新旧思想结合到一个简单的方法中，使用源语法和目标语法来显著提高翻译准确性。通过在源端和目标端使用语法分析树来归纳模糊（不精确）树到树规则，以及允许语法不匹配的替换，我们比他的层次基线有了显著的改进。我们发现，由于系统的限制性太强，双方句法信息的整合往往会降低翻译质量。"}
{"pid": "J01-2001", "zh_sum": "自然语言形态的无监督学习本研究报告了使用最小描述长度（MDL）分析来模拟欧洲语言形态切分的无监督学习的结果，使用的语料库大小从5000个单词到500000个单词不等。我们开发了一组启发式算法，可以快速开发概率形态语法，并使用MDL作为主要工具来确定是否采用启发式算法提出的修改。由此产生的语法与人类形态学家的分析结果非常吻合。在最后一节中，我们讨论了这种类型的MDL语法分析与早期生成语法中的评价度量概念之间的关系。我们提出了一种递归结构，使得茎可以由一个子茎和一个后缀组成。我们使用基于签名的形态学表示，签名是一组表示具有屈折或派生形态学的单词族的词缀。我们观察到频率较低和词缀较短的词缀更容易出错。"}
{"pid": "C00-2163", "zh_sum": "统计机器翻译中对齐模型的比较本文提出并比较了统计机器翻译中的各种对齐模型。我们建议使用维特比对齐的质量来衡量对齐模型的平铺质量，与手动生成的对齐进行比较，并描述一种改进的注释方案，以生成合适的参考对齐。我们还比较了不同对齐模型对统计机器翻译系统平铺翻译质量的影响。为了改进基于HMM的对齐中的转换模型，我们将转换模型扩展为与词类相关。"}
{"pid": "W01-0511", "zh_sum": "通过特定领域的词汇层次对名词复合词中的语义关系进行分类我们正在开发基于语料库的技术，用于在中间描述层次上识别语义关系（比case框架中使用的更具体，但比传统知识表示系统中使用的更一般）。在本文中，我们描述了一种用于识别两个词-名词复合词之间关系的分类算法。我们发现，使用机器学习算法和特定领域词汇层次结构的一种非常简单的方法可以成功地从训练实例中概括出来，与对单词本身进行训练的基线相比，它在以前看不见的单词上表现更好。我们使用13个类别对医学领域的名词复合词进行分类，这些类别描述了给定名词复合词中头名词和修饰语之间的语义关系。我们使用判别分类器对生物医学文本中的18个名词复合词关系进行分类，准确率达到60%。"}
{"pid": "W01-1605", "zh_sum": "在修辞结构理论的框架下构建话语标注语料库我们描述了我们在开发一个供社区广泛使用的话语标注语料库方面的经验。在修辞结构理论的框架下，我们能够使用定义良好的方法和协议创建一个具有高度一致性的大型注释资源。该资源通过语言数据联盟公开提供，以使研究人员能够开发基于经验的、特定于话语的应用程序。在我们的语篇树库中，只有26%的对比关系由线索短语表示，而在NTC-7中，大约70%的对比关系由线索短语表示。我们的语料库包含了385篇《华尔街日报》的文章，这些文章是按照修辞结构理论注释的。"}
{"pid": "J94-3001", "zh_sum": "语音规则系统的正则模型本文提出了一套数学和计算工具，用于处理和推理正则语言和正则关系，并认为它们为计算语音学提供了坚实的基础。它详细说明了该框架如何应用于上下文相关重写规则的有序集，以及Koskenniemi的两层形式主义中的语法。这种分析提供了语音约束的通用表示，支持单个简单口译员高效地生成和识别。我们提供了一种编译为传感器的算法。我们描述了一种将置换过程表示为有限状态转换的一般方法。"}
{"pid": "D08-1076", "zh_sum": "统计机器翻译中基于格点的最小错误率训练最小错误率训练（MERT）是估计线性模型特征函数权重的有效手段，可以在训练中直接优化衡量系统性能的自动评估标准。为了实现这一点，训练过程为每个特征函数确定其在给定候选翻译集上的精确误差面。然后，通过遍历所有句子的组合误差面，并选取结果误差计数达到最小值的值，来调整特征函数权重。通常，MERT中的候选项表示为N个最佳列表，其中包含解码器产生的N个最可能的翻译假设。在本文中，我们提出了一种新的算法，该算法可以有效地构造和表示在短语格中编码的所有翻译的精确错误面。与N-best MERT相比，因此考虑的候选翻译数量增加了几个数量级。该方法用于训练基于短语的统计机器翻译系统的特征函数权重。在NIST 2008翻译任务上进行的实验表明，与N-best MERT相比，运行时有了显著的改进，BLEU分数有了适度的提高。我们发现，调整过程的第一次迭代会产生非常糟糕的权重（甚至接近0）；这种异常的性能下降归因于对候选存储库的过度拟合。我们提出了一种直接在编码Cs中假设的字格上进行线优化的程序。我们将扫描线算法应用于并集，以丢弃冗余线性函数及其相关假设。我们的理论是，在最终状态下，上包络中的线性函数数的上界等于晶格中的边数。在我们的MERT算法中，我们计算来自翻译搜索空间（由压缩林表示）的目标句子的错误统计信息，这些句子正是通过沿特征空间维度中的单个向量改变特征权重而具有最小可辨别性的句子。我们扩展了MERT算法，以使用解码器生成的搜索格中紧凑表示的整个候选翻译集，而不是仅从中提取N个最佳候选列表。我们发现，当维数增加10倍以上时，下坡单纯形算法失去了其鲁棒性。"}
{"pid": "C92-2066", "zh_sum": "随机词汇化树邻接文法正式定义了随机词汇化树邻接文法（SLTAG）的概念。SLTAG的参数对应于组合两个结构的概率，每个结构都与一个单词相关。SLTAG的特点是独特和新颖的，因为它在词汇上是敏感的（如N-gram模型或隐马尔可夫模型），但在层次上（如随机上下文无关语法）。然后，介绍了SLTAG的两种基本算法：计算SLTAG生成句子的概率的算法和在给定训练语料库的情况下估计SLTAG参数的类内-外迭代算法。最后，我们应该了解SLTAG如何定义随机上下文无关语法的词汇化版本，并报告了初步实验，展示了SLTAG相对于随机上下文无关语法的一些优势。在随机树邻接语法中，通过将概率分配给较大的结构单元，克服了上下文敏感性的不足。"}
{"pid": "P07-1034", "zh_sum": "在自然语言处理（NLP）领域自适应中，由于新领域中缺乏标记数据，领域自适应中的实例加权是自然语言处理（NLP）中的一个重要问题。本文从实例加权的角度研究了领域自适应问题。我们从分布的角度对域自适应问题进行了形式化的分析和表征，并表明存在两种不同的自适应需求，对应于源域和目标域中实例和分类函数的不同分布。然后，我们提出了一个通用的领域自适应实例加权框架。我们在三个NLP任务上的实证结果表明，通过实例权重合并和利用来自目标域的更多信息是有效的。我们根据训练实例与未标记目标域数据的相似性来衡量训练实例。我们发现，平衡自举比标准自举在域自适应方面更有效。在我们的实例权重中，我们为可转移实例分配更大的权重，以便在源域上训练的模型能够更有效地适应目标域。"}
{"pid": "D10-1115", "zh_sum": "名词是向量形容词是矩阵：在语义空间中表示形容词-名词结构我们提出了一种基于语料库的分布语义的形容词-名词组合（an）方法，该方法基于理论语言学的见解，将名词表示为向量，将形容词表示为名词向量上的数据诱导（线性）函数（编码为矩阵）。我们的模型在重建训练中未发现的向量的任务上明显优于竞争对手。一项小型事后分析进一步表明，当模型生成的向量与观察到的向量不相似时，这是由于后者的异常。此外，我们还表明，我们的方法提供了两种新的形容词含义表示方法，替代了基于语料库的共现向量表示方法，在形容词聚类任务中均优于后者。我们发现，mult方法在原始的、未缩减的语义空间中可以表现得更好，因为SVD维度可能有负值，导致组件式乘法的反直觉结果（将较大的相反符号值相乘会产生较大的负值，而不是被抵消）。形容词特定线性映射（alm）模型在近似未知ANs的正确向量方面的表现远远好于add和mult，而在这方面（在某种意义上，更元语言）任务add和mult工作得更好，而alm仅在更复杂的邻居密度度量方面取得了成功。"}
{"pid": "D08-1082", "zh_sum": "将自然语言解析为意义表示的生成模型本文提出了一种学习自然语言句子生成模型及其具有层次结构的形式意义表示的算法。该模型被应用于将句子映射到其潜在意义的层次表示的任务。我们引入动态规划技术来实现高效的训练和解码。在实验中，我们证明，当该模型与一种有区别的重新排序技术相结合时，在两个公开的语料库上进行测试时，该模型达到了最先进的性能。当呈现与训练中看到的实例不同的实例时，生成模型会显著退化。与以前的车型相比，这可以显著提高召回率。我们的混合树模型使用基于树转换的方法。我们提出了一个联合生成过程，该过程生成一个包含单词、句法结构和意义表示的混合树结构，其中意义表示是一种无变量的树结构形式。我们提出了三种生成语义分析模型：unigram、bigram和mix gram（二者之间的插值）。"}
{"pid": "A94-1009", "zh_sum": "鲍姆·韦尔奇（Baum Welch）的重新评估是否有助于标记者？在隐马尔可夫模型词性标注中，使用统计模型为文本中的单词指定语法类别。该领域的早期工作依赖于由人类注释员标记的语料库来训练模型。最近，Cutting等人（1992）提出，通过使用Baum-Welch重新估计来自动优化模型，可以用最少的词典和有限的概率先验信息来实现训练。在本文中，我报告了两个旨在确定需要多少手动培训信息的实验。第一个实验表明，词汇或过渡概率的初始偏差对于获得良好的准确性至关重要。第二个实验表明，Baum-Welch重估计有三种不同的模式。在其中两种模式中，重新估计最终会降低标记的准确性，而不是提高它。可以根据初始模型的质量和被标记训练语料库（如果有）与待标记语料库之间的相似度来预测适用的模式。给出了决定如何有效使用重新估计的启发式方法。这些结论与梅里亚尔多（1994）的结论大体一致，但对模型不同部分的贡献给出了更详细的说明。我们报告了在LOB语料库的一部分上训练的基于单词的无监督HMM的准确率分别为75.49%、80.87%和79.12%，标记集为134个标记。"}
{"pid": "W08-2102", "zh_sum": "标记动态规划和感知器用于高效的特征丰富解析我们描述了一种解析方法，该方法利用感知器算法，结合动态规划方法，来恢复基于完整成分的解析树。形式主义允许一组丰富的解析树特性，包括基于PCFG的特性、二元和三元依赖特性以及曲面特性。将这种方法应用于完整语法分析的一个严峻挑战是所涉及的分析算法的效率。我们使用基于树邻接语法（TAG）的解析形式证明了有效的训练是可行的。使用低阶依赖解析模型来限制整个模型的搜索空间，从而提高效率。在宾州华尔街日报树库（PennWSJ treebank）上的实验表明，该模型在成分准确性和依赖性准确性方面都达到了最先进的性能。根据从更简单的解析模型计算出的边缘，可以预先排除许多边缘（Carreras et al 2008）。"}
{"pid": "P03-1009", "zh_sum": "聚类多义子范畴化框架分布语义上先前的研究表明，聚类在从未经匹配的语料库数据中归纳语义动词类方面有很大的作用。我们描述了一种新的方法，该方法涉及使用信息瓶颈和最近邻方法对子类别化框架（SCF）分布进行聚类。与之前的工作相比，我们特别关注多义动词的聚类。提出了一种新的评估方案，该方案考虑了多义对聚类的影响，使我们能够更好地了解对未进行语义分类的SCF数据进行语义分类的潜力和局限性。我们基于黄金标准评估硬聚类，每个动词有多个类。我们根据Levin的分类和LCS数据库（Dorr，1997），通过考虑动词多义来创建一个测试集。将介词短语（pp）参数化为两个频繁的子范畴化框架（NP和NP pp），并将子范畴化框架的未过滤原始频率用作表示动词的特征。"}
{"pid": "N09-1037", "zh_sum": "联合解析和命名实体识别对于许多语言技术应用程序，例如问答，整个系统在数据上运行几个独立的处理器（例如命名实体识别器、共同引用系统和解析器）。这很容易导致注释不一致，这对聚合系统的性能有害。我们开始在基于区分特征的选区解析器的基础上，通过解析和命名实体识别的联合模型来解决这个问题。我们的模型生成一致的输出，其中命名的实体跨度与解析树的短语跨度不冲突。联合表示还允许来自每种类型注释的信息提高另一种类型注释的性能，并且在使用OntoNotes语料库的实验中，我们发现解析的F1绝对值提高了1.36%，命名实体识别的F1绝对值提高了9.0%。虽然将命名实体识别与选区分析结合起来执行这两项任务可以提高性能，但NER组件利用的sytnax的唯一方面是名词短语的位置。"}
{"pid": "E89-1037", "zh_sum": "通过结构对应翻译，我们勾勒并说明了一种机器翻译方法，该方法利用了不同语言表达水平之间同时对应的潜力，如LFG的代码描述概念所形式化的那样。该方法以英语、德语和法语为例进行了说明，其中源语和目标语句子在语言分析方面存在显著差异。该体系结构可以提供一个正式的基础，以声明的方式指定复杂的源-目标翻译关系，该关系建立在独立动机和理论上合理的单语语法和词汇的基础上。"}
{"pid": "N10-1019", "zh_sum": "使用大部分本地数据纠正学习者的错误；本文介绍了一系列针对非英语母语者的冠词和介词纠错实验的结果。我们首先比较了语言模型和特定错误分类器（都是在大型英语语料库上训练的）在错误检测和纠正方面的表现。然后，我们通过将来自分类器和语言模型的证据作为元分类器的输入特征，在元分类方法中结合语言模型和分类器。元分类器依次在错误注释的学习者数据上进行训练，优化该领域的错误检测和纠正性能。元分类方法比仅使用分类器和仅使用语言模型的方案有很大的优势。由于元分类器需要错误注释数据进行训练，我们研究了在不使用元分类器的基础上，需要多少训练数据来改善结果。所有的评估都是在一个大型的错误注释英语学习者语料库上进行的。我们从数据中删除一些其他错误紧接着介词或限定词错误出现的句子。"}
{"pid": "W04-3219", "zh_sum": "单语机器翻译用于释义生成我们使用统计机器翻译（SMT）工具生成同一语言中输入句子的新释义。该系统是针对从万维网上可获得的集群新闻文章中自动提取的大量句子对进行训练的。对齐错误率（AER）是用来衡量结果语料库的质量的。单调短语解码器生成上下文替换。人工评估表明，该系统优于基线释义生成技术，并且与之前的工作不同，与当前同类最佳的释义方法相比，该系统提供了更好的覆盖范围和可扩展性。我们基于统计机器翻译框架，从单语可比语料库中构建了一个释义生成模型，其中语言模型评估翻译的语法性，即生成的表达式。受基于短语的机器翻译的启发，我们提出了一个端到端的释义系统，该系统既可以获取释义，也可以使用它们生成新的字符串。"}
{"pid": "P97-1009", "zh_sum": "使用句法依赖作为局部上下文来解决词义歧义，以前大多数基于语料库的算法都使用从同一单词的先前用法中训练出来的分类器来消除单词的歧义。必须针对不同的单词训练不同的分类器。我们提出了一种使用相同的知识源来消除不同单词歧义的算法。该算法不需要语义标记语料库，并且利用了两个不同的单词如果出现在相同的本地上下文中可能具有相似的含义这一事实。我们将两个对象之间的相似性定义为对象之间的公共性所包含的信息量除以对象描述中的信息量。"}
{"pid": "A00-1031", "zh_sum": "TnT-统计词性标记器Trigrams'n'Tags（TnT）是一种有效的统计词性标记器。与文献中其他地方的说法相反，我们认为基于马尔可夫模型的标记器至少与当前的其他方法（包括最大熵框架）表现一样好。最近的一项比较甚至表明，TnT在测试语料库中的表现要好得多。我们描述了TnT的基本模型，用于平滑和处理未知单词的技术。此外，我们还对两个语料库进行了评价。我们实现了一组基于句法结构的语法功能标记的自动标注，这些语法功能标记包括短语组块和在监督模式下从德语树库中训练的句法角色修饰语。"}
{"pid": "P04-1083", "zh_sum": "统计机器翻译在普通语法分析器中，输入是字符串，语法范围是字符串。本文探讨了允许输入由字符串元组组成和/或语法覆盖字符串元组的普通解析算法的一般化。这种算法可以推断出隐藏在并行文本中的同步结构。事实证明，这些通用解析器可以完成训练和应用语法感知统计机器翻译系统所需的大部分工作。当解析器的语法的维数小于解析器的输入时，我们称之为同步器。我们将机器翻译问题形式化为基于多文本语法的同步解析。"}
{"pid": "P04-1077", "zh_sum": "基于最长公共子序列和跳过二元统计的机器翻译质量自动评价本文描述了两种新的机器翻译客观自动评价方法。第一种方法基于候选翻译和一组参考翻译之间的最长公共子序列。最长公共子序列自然地考虑了句子级结构的相似性，并自动识别序列n-gram中最长的共现子序列。第二种方法将严格的n-gram匹配放宽到跳过二元匹配。Skip bigram是句子顺序中的任意一对单词。Skip-bigram共现统计信息测量候选翻译和一组参考翻译之间的Skip-bigram重叠。实证结果表明，这两种方法在充分性和流利性方面都与人类的判断有很好的相关性。我们对一系列指标进行了实验，包括NIST、WER（Niefen et al，2000）、PER（Tillmann et al，1997）以及ROUGE、BLEU和GTM的变体。"}
{"pid": "P98-2204", "zh_sum": "《永不回头：中心的另一种选择》我提出了一个确定听者注意状态的模型，该模型仅依赖于一系列突出的话语实体（s-list）。S-list元素之间的排序还包括居中模型中的后向中心的功能。S-list的排名标准基于听者新旧话语实体之间的区别，并结合了对句间和句内回指的偏好。该模型是逐字递增运算算法的基础。我们认为，先行词的信息状态比它所起的语法作用更重要。我们评估现有的注释数据。我们将算法限制在当前和最后一句话。"}
{"pid": "W98-1118", "zh_sum": "在命名实体识别中利用最大熵来挖掘各种知识源本文描述了一种基于最大实体框架的统计命名实体（即“专有名称”）识别系统。通过在最大熵理论的框架内工作，并利用灵活的基于对象的体系结构，系统能够在做出标记决策时利用极其多样的知识源。这些知识来源包括大写特征、词汇特征、指示文本当前部分的特征（即标题或正文）以及单词或多词术语词典。纯统计系统不包含手工生成的模式，并可获得与最佳统计系统相当的结果。然而，当与其他手工编码系统相结合时，该系统的得分超过了迄今为止公布的最高可比得分。"}
{"pid": "P06-1115", "zh_sum": "使用字符串核学习语义解析器，我们提出了一种新的方法，使用基于字符串核的分类器将自然语言句子映射到其形式意义表示。我们的系统为形式语言语法中的每一个产物学习这些量词。通过使用这些字符串分类器寻找最可能的语义解析，可以获得新的自然语言句子的意义表示。我们在两个真实数据集上的实验表明，与其他现有系统相比，该方法具有更好的性能，并且对噪声特别鲁棒。我们使用单词子序列核来计算两个子串之间的相似度。我们的模型KRISP采用了一种有区别的方法，即从自然语言字符串分层构建意义表示结构。KRISP（基于核的语义分析鲁棒解释）（Kate和Mooney，2006）是一个用于语义分析的监督学习系统，它将NL句子与其MRs配对作为训练数据。"}
{"pid": "D07-1002", "zh_sum": "利用语义角色改进问答浅层语义分析，即句子成分的自动识别和标注，近年来受到了广泛关注。我们的工作考察了语义角色信息是否有利于问答。我们介绍了一个通用的答案提取框架，该框架利用FrameNet范式中的语义角色注释。我们将语义角色分配视为二部图中的一个优化问题，将答案提取视为图匹配的一个实例。TREC数据集上的实验结果表明，与最先进的模型相比有了改进。我们表明，谓词参数结构形式的浅层语义信息（PASs）提高了对目标问题正确答案的自动检测。我们还指出，当前版本的FrameNet的低覆盖率显著限制了预期的性能提升。"}
{"pid": "P94-1019", "zh_sum": "动词语义与词汇选择本文将重点研究动词在计算机系统中的语义表示及其对机器翻译中词汇选择问题的影响。本文考察了两组英语和汉语动词，以表明词汇选择必须基于对句子的解释以及对动词参数的选择限制。提出了一种新的表示方案，并将其与基于转移的机器翻译中使用的具有选择限制的表示进行了比较。我们认为我们的方法与基于知识的机器翻译方法（KBMT）紧密一致，并且是可以并入现有系统的单独组件。实例和实验结果表明，使用该方案，不精确匹配可以实现正确的词汇选择。我们设计了我们的度量，使浅层节点的相似性低于WordNet层次结构中较深的节点。我们的测量是纯粹的分类学；它不需要任何语料库统计。我们的相似性度量衡量WordNet分类法中这两个概念的深度和最不常见的包含词的深度，然后将这些数字组合成一个相似性分数。"}
{"pid": "J88-1003", "zh_sum": "基于统计优化的语法类别消歧过去已经开发了几种算法，它们试图在不依赖句法或语义层信息的情况下解决自然语言文本中的类别歧义。兰卡斯特奥斯陆/卑尔根英国英语语料库的研究人员最近开发了一种创新方法（称为“爪”）。该算法使用基于特定标签共现概率的系统计算。它的精确度很高，但速度很慢，并且通过多种方式手动增强。这种手动增强对准确性的影响尚不清楚。本文提出了一种类似于爪的消歧算法，该算法在线性时间和空间上运行，而不是在指数时间和空间上运行，并且最小化了非系统的增广。使用布朗标准英语语料库的百万单词对该算法进行了测试；总体准确率为96%。该算法可以为任何英语语法分析或自然语言处理系统提供快速、准确的前端。"}
{"pid": "P98-1029", "zh_sum": "用于改进词法消歧的分类器组合机器学习中最近最令人兴奋的方向之一是发现多个分类器的组合通常比使用单个分类器可以获得更好的性能。在这篇文章中，我们首先证明了三种不同的最先进的词性标注者所犯的错误具有很强的互补性。接下来，我们将展示如何将这种互补行为用于我们的优势。通过使用上下文线索来指导标记者的组合，我们能够派生出一个新的标记者，它的性能显著高于任何单个标记者。我们定义了两个学习者之间的互补性，以量化一个系统错误而另一个系统正确的时间百分比，从而提供了组合精度的上限。"}
{"pid": "W03-1508", "zh_sum": "跨语言信息检索中的专有名称音译我们解决了使用中文正字法音译英文名称的问题，以支持跨语言语音和文本处理应用程序。我们展示了统计机器翻译技术的应用，将通过自动文本语音转换系统获得的英文名称的音位表示“翻译”到一系列首字母和韵母，这是汉语常用的发音子词单位。然后，我们使用另一个统计翻译模型将初始/最终序列映射到汉字。我们还对该模块在使用英语文本查询从TDT语料库检索普通话口语文档中的应用进行了评估。我们采用了噪声信道建模框架。"}
{"pid": "C08-1114", "zh_sum": "类比、同义词、反义词和关联的统一方法识别类比、同义词、反义词和关联似乎是四个不同的任务，需要不同的NLP算法。过去，这四项任务都是使用各种算法独立处理的。然而，这四个语义类只是所有语义现象的一个小样本，我们无法为每个语义现象创建特定的算法；我们需要寻求一种统一的方法。我们建议将广泛的现象纳入类比范畴。为了限制本文的范围，我们将注意力限制在同义词、反义词和联想的包含上。我们介绍了一种基于有监督语料库的机器学习算法来对相似词对进行分类，并从认知心理学的角度证明了它可以解决多项选择SAT类比问题、托福同义词问题、ESL同义词反义词问题以及相似关联的两个问题。我们提出了一种简单的基于SVM的类比分类算法PairClass。我们认为，许多NLP任务可以用类比推理来表述，并且我们将我们的PairClass算法应用于许多问题，包括SAT语言类比测试、同义词/反义词分类以及语义相似和语义关联词之间的区分。我们主张需要一种统一的方法来处理基于语料库的语义任务。"}
{"pid": "D08-1022", "zh_sum": "基于林的翻译规则提取翻译规则提取是机器翻译中的一个基本问题，特别是对于需要从bitext的任意一侧或两侧解析树的基于语言语法的系统。当前的主流做法仅使用1-best树，这会由于解析错误而对规则集质量产生不利影响。因此，我们提出了一种新的方法，该方法从压缩林中提取规则，该压缩林对许多解析进行指数级压缩编码。实验表明，该方法在最先进的树到字符串系统上提高了1个BLEU点以上的翻译质量，比在30个最佳语法上提取的翻译质量提高了0.5个点（速度是提取速度的两倍）。当与我们之前基于森林的解码工作相结合时，它比基线提高了2.5个BLEU点，甚至比Hiero的层次系统提高了0.7个点。我们使用维特比算法来修剪森林。"}
{"pid": "W02-1021", "zh_sum": "统计机器翻译中的词图生成本文描述了一个基于自然语言的专家系统路线顾问，用于挪威特隆赫姆的公共巴士交通。该系统可在互联网上使用，自1999年初以来已安装在巴士公司的web服务器上。该系统是双语的，依赖于内部独立于语言的逻辑表示。在问答之间是一个词法分析、语法分析、语义分析、语用推理和数据库查询处理的过程。有人可能会辩称，信息内容可以通过询问来解决，即要求客户提供4个项目：出发站、到达站、最早出发时间和/或最晚到达时间。我们使用IBM约束为自下而上的搜索生成单词图。单词图是一种加权有向无环图，其中每个节点表示一个部分翻译假设，每条边用目标句子的单词标记，并根据模型给出的分数进行加权。"}
{"pid": "D11-1033", "zh_sum": "基于伪域内数据选择的领域自适应我们从与目标领域最相关的大型通用领域并行语料库中提取句子，探索有效的统计机器翻译任务的领域自适应。这些句子可以用简单的基于交叉熵的方法来选择，我们给出了三种方法。由于这些句子本身与域内数据不同，我们将其称为伪域内子集团。然后，这些子语料库（只有原始语料库的1%）可以用来训练小领域自适应统计机器翻译（SMT）系统，其性能优于在整个语料库上训练的系统。当我们将这些领域自适应模型与真正的领域内模型结合使用时，性能会进一步提高。结果表明，训练数据越多并不总是越好，通过适当的域相关数据选择，以及在解码过程中结合域内和一般域系统，可以获得最佳结果。我们改进了基于困惑度的方法，提出了双语交叉熵差作为领域内和一般领域语言模型的排序函数。我们提出了一种双语交叉熵差来从平行语料库中选择数据进行领域适应，该方法捕获的上下文信息略多于单语交叉熵差。"}
{"pid": "P07-1094", "zh_sum": "无监督词性标注的全贝叶斯方法语言结构的无监督学习是一个难题。一种常见的方法是定义生成模型，并在给定观测数据的情况下最大化隐藏结构的概率。通常，这是使用模型参数的最大似然估计（MLE）来完成的。我们使用词性标记表明，完全贝叶斯方法可以极大地提高性能。贝叶斯方法不是估计单个参数集，而是综合所有可能的参数值。这种差异确保了学习的结构在一系列可能的参数上具有很高的概率，并允许使用有利于自然语言典型稀疏分布的先验。我们的模型具有标准三元HMM的结构，但其精度更接近最先进的判别模型（Smith和Eisner，2005），比MLE高出14个百分点。我们发现，无论是单独从数据进行训练，还是使用标记词典，都有改进。在我们的模型中，跃迁、发射和耦合参数由Dirichlet先验控制，并使用令牌级折叠Gibbs采样器进行推断。"}
{"pid": "H93-1051", "zh_sum": "基于语料库的统计意义解析本文研究的三种基于语料库的统计意义解析方法试图利用单词共现模式的知识来推断多义词的正确意义。这些技术基于贝叶斯决策理论、神经网络和信息检索中使用的内容向量。为了更好地理解这些方法，我们提出了一个非常具体的问题：给定一组上下文，每个上下文都包含已知意义上的名词行，构造一个分类器，为新上下文选择正确的行。为了了解一词多义的程度如何影响表现，我们比较了三义和六义任务的结果。结果表明，每种技术都能区分六种线感，准确率超过70%。此外，在大多数情况下，分类器的响应模式在统计上彼此无法区分。对这两项任务的比较表明，解决个人感觉的困难程度比多义程度是一个更大的表现因素。我们构建了2094字词行数据集用于词义消歧。"}
{"pid": "C00-2137", "zh_sum": "对结果差异的统计显著性进行更准确的检验对回忆、精确度和平衡F分数等指标值的差异进行统计显著性检验是经验自然语言处理的必要部分。不幸的是，我们在一组实验中发现，许多常用的测试往往低估了显著性，因此不太可能检测出不同技术之间存在的差异。这种低估来自经常被违背的独立性假设。我们指出了一些不作此假设的有用测试，包括计算密集型随机化测试。F分数的标准差通过自举重采样进行估计。"}
{"pid": "P93-1002", "zh_sum": "利用词汇信息对双语语料库中的句子进行对齐本文描述了一种在双语语料库中对齐句子和译文的快速算法。现有的有效算法忽略了单词身份，只考虑句子长度（Brown el al.，1991b；Gale and Church，1991）。我们的算法在对齐过程中动态构建了一个简单的统计词到词翻译模型。我们找到了最大化生成该翻译模型语料库概率的对齐方式。我们在加拿大Hansard数据上实现了大约0.4%的错误率，这比之前的结果有了显著的改进。该算法与语言无关。我们发现，当要对齐的文本包含小段落，或者所涉及的语言几乎没有同源词时，基于句子长度的方法会受到影响。我们发现，动态规划特别容易受到这两种语言之一中发生的删除的影响。我们使用手动对齐的句子对来训练对齐模型。"}
{"pid": "W02-1028", "zh_sum": "一种利用抽取模式上下文学习语义词典的自举方法本文描述了一种称为Basilisk的自举算法，该算法可以学习多个类别的高质量语义词典。Basilisk从每个语义类别的未注语料库和种子词开始，然后引导它们学习每个类别的新词。Basilisk假设一个单词的语义类别是基于大量抽取模式上下文中的集体信息。我们从六个语义类别来评估Basilisk。Basilisk生成的语义词典比以前的技术生成的语义词典精度更高，有几个类别显示出了实质性的改进。我们同时学习多个语义类别，这依赖于一个单词不能属于多个语义类别的假设。"}
{"pid": "D09-1159", "zh_sum": "观点挖掘中的短语依赖分析本文提出了一种从产品评论中挖掘观点的新方法，该方法将观点挖掘任务转换为识别产品特征、观点表达以及它们之间的关系。利用产品的许多特征都是短语的特点，引入了短语依赖分析的概念，将传统的依赖分析扩展到了短语层次。然后将此概念用于提取产品特征和意见表达之间的关系。实验结果表明，挖掘任务可以从短语依赖分析中获益。我们利用依赖解析器从评论中提取名词短语和动词短语作为方面候选。对于单语任务，我们使用浅层解析器将词汇依赖关系从依赖关系解析器转换为短语依赖关系。"}
{"pid": "P11-1060", "zh_sum": "学习基于依赖关系的组合语义组合问答从将问题映射到逻辑形式开始，但训练语义解析器执行此映射通常需要对目标逻辑形式进行代价高昂的注释。在本文中，我们学习通过潜在的逻辑形式将问题映射到答案，这些逻辑形式是从问答对中自动归纳出来的。在解决这一具有挑战性的学习问题时，我们引入了一种新的语义表示，它强调了依赖语法与有效评估逻辑形式之间的并行性。在两个标准语义分析基准（GEO和JOBS）上，我们的系统获得了最高的发布精度，尽管不需要带注释的逻辑形式。我们支持基于依赖关系的组合语义，它将语义解析表示为一棵树，其中节点表示数据库元素和操作，边表示关系连接。基于依赖关系的组合语义（DC）通过使用简单的依赖关系树，提供了一种直观的方法来建模问题的语义。"}
{"pid": "A00-2018", "zh_sum": "基于最大熵的语法分析器，我们提出了一种新的语法分析器，用于解析Penn tree bank风格的语法树，当在之前建立的华尔街日报树库的[5,9,10,15,17]“标准”部分上进行训练和测试时，长度为40及以下的句子的平均准确率/召回率达到90.1%，长度为100及以下的句子的平均准确率/召回率达到89.5%。这表明，与此语料库上的最佳单个解析器结果相比，错误率降低了13%。主要的技术创新是使用“最大熵启发”的调节和平滑模型，使我们能够成功地测试和组合许多不同的调节事件。我们还提供了一些部分结果，显示了不同条件信息的影响，包括由于在猜测词头之前猜测词头的前末端，令人惊讶的提高了2%。作为硬编码启发式的替代方法，我们建议自动恢复Penn函数标记。我们的解析器也是一个两阶段的ctf模型，其中第一阶段是平滑马尔可夫语法（它使用多达三个前一个成分作为上下文），第二阶段是词汇化马尔可夫语法，带有关于父母和祖父母的额外注释。"}
{"pid": "P07-1056", "zh_sum": "宝莱坞Boom Box and Blenders传记：情感分类的领域自适应自动情感分类近年来得到了广泛的研究和应用。然而，不同领域的情感表达方式不同，为每个可能的兴趣领域注释语料库是不切实际的。我们研究情感分类器的领域适应性，重点关注不同类型产品的在线评论。首先，我们将最近提出的结构对应学习（SCL）算法扩展到情感分类，与原始SCL算法相比，域间自适应导致的相对误差平均减少了30%，与监督基线相比减少了46%。其次，我们确定了一个域相似性度量，该度量与分类器从一个域到另一个域的自适应潜力密切相关。例如，可以使用此度量来选择一小组域来注释其经过训练的分类器将很好地转移到许多其他域。我们引入了一个多域情感数据集。"}
{"pid": "P08-1085", "zh_sum": "EM可以找到非常好的HMM词性标记器（如果有一个良好的开端），我们解决了无监督词性标记的任务。我们证明，当提供良好的初始条件时，即使字典不完整，使用鲁棒EM-HMM学习器也可以获得良好的结果。我们提出了一系列算法来计算有效的初始估计p（t | w）。我们在希伯来语的完全形态消歧任务上测试了该方法，在强均匀分布基线上实现了25%的误差减少。我们还在标准的《华尔街日报》无监督词性标注任务中测试了相同的方法，并获得了与最新最先进的方法相竞争的结果，同时使用了简单有效的学习方法。我们使用语言考虑为EM算法选择一个良好的起点。我们注意到，手工修复有噪音的字典实际上是非常可行的，并建议应将工作重点放在利用人类知识上，而不仅仅是改进算法。"}
{"pid": "D08-1014", "zh_sum": "使用机器翻译的多语言主观性分析虽然其他语言的研究正在增加，但主观性分析中的许多工作已应用于英语数据，这主要是因为该语言有大量可用的电子资源和工具。在本文中，我们提出并评估了可用于跨语言传输主观性资源库的方法。具体而言，我们试图利用英语可用的资源，并通过使用机器翻译生成其他语言的主观性分析资源。通过对两种不同语言（罗马尼亚语和西班牙语）的对比评估，我们表明自动翻译是构建新目标语言主观性分析资源和工具的可行选择。我们证明，将主观性分析扩展到多语环境下，机器翻译可以取得很好的效果。我们假设，由于词汇化、正式和非正式标记等原因，主观性在不同的语言中表现不同。"}
{"pid": "P06-1124", "zh_sum": "基于Pitman-Yor过程的层次贝叶斯语言模型我们提出了一种新的自然语言层次贝叶斯n-gram模型。我们的模型利用了一种被称为Pitman-Yor过程的常用Dirichlet分布的推广，该过程产生的幂律分布与自然语言中的分布更为相似。我们表明，对层次Pitman-Yor语言模型的近似恢复了插值Kneser-Ney的精确公式，这是n-gram语言模型的最佳平滑方法之一。实验证明，我们的模型给出的交叉熵结果优于插值的Kneser-Ney，与修正的Kneser-Ney相当。我们对Kneser-Ney和Witten-Bell退避方案等平滑技术提供了贝叶斯解释。非参数贝叶斯建模能够提供特别适合NLP中任务的先验知识。虽然Dirichlet过程只是d=0的Pitman-Yor过程，但我们发现贴现参数允许对自然语言中常见的长尾分布进行更有效的建模。"}
{"pid": "W06-0301", "zh_sum": "从网络新闻媒体文本中抽取观点持有者和话题本文给出了一种从网络新闻媒体文本中抽取一个句子来识别观点及其持有者和话题的方法。我们介绍了一种利用句子语义结构的方法，该语义结构锚定在一个表达意见的动词或形容词上。该方法使用语义角色标记作为中间步骤，使用来自FrameNet的数据标记意见持有者和主题。我们将任务分解为三个阶段：识别一个意见词，标记与句子中该词相关的语义角色，然后在标记的语义角色中找到意见词的持有者和主题。为了更广泛的覆盖范围，我们还使用聚类技术来预测FrameNet中未定义的单词的最可能框架。我们的实验结果表明，我们的系统性能明显优于基线。我们使用语义角色标记来识别意见持有者和目标。"}
{"pid": "P03-1054", "zh_sum": "准确的非exicalized解析我们证明，非exicalized PCFG可以通过使用简单的、语言动机的状态拆分来解析比之前显示的更精确的内容，这打破了普通树库语法中潜在的虚假独立假设。事实上，其86.36%（LP/LR F1）的性能优于早期词汇化PCFG模型，令人惊讶地接近当前的最新水平。这一结果的潜在用途不仅仅是建立一个关于非出射化模型最大可能精度的强下界：非出射化PCFG比更复杂的词法模型更紧凑、更容易复制和更容易解释，并且解析算法更简单、更广为理解、渐近复杂度更低、更容易优化。我们还提出了一种手动符号细化方法。"}
{"pid": "P93-1005", "zh_sum": "面向基于历史的语法：使用更丰富的概率分析模型，我们描述了自然语言的生成概率模型，我们称之为HBG，它利用详细的语言信息来解决歧义。HBG以一种新颖的方式将语法树中的词汇、句法、语义和结构信息整合到消歧过程中。我们使用一个被称为树库（Treebank）的括号内句子语料库，结合决策树构建，梳理出解析树的相关方面，这些方面将决定句子的正确解析。这与通常通过语言内省进一步裁剪语法的方法形成对比，希望生成正确的语法分析。在针对现有最好的鲁棒概率解析模型之一（我们称之为P-CFG）进行的头对头测试中，HBG模型的性能明显优于P-CFG，解析准确率从60%提高到75%，错误减少了37%。"}
{"pid": "W02-0109", "zh_sum": "NLTK：自然语言工具包NLTK，自然语言工具包，是一套开源程序模块、教程和习题集，提供现成的计算语言学课件。NLTK涵盖符号和统计自然语言处理，并与带注释的语料库连接。学生扩充和替换现有组件，通过示例学习结构化编程，并从一开始就操作复杂的模型。自然语言工具包NLTK是一套Python模块，提供许多NLP数据类型、处理任务、语料库样本和读者，以及动画算法、教程和问题集。"}
{"pid": "W06-3114", "zh_sum": "手动和自动评估欧洲语言之间的机器翻译我们评估了参与共同任务的六对欧洲语言的机器翻译性能：将法语、德语、西班牙语文本翻译成英语和英语。使用BLEU分数自动进行评估，并手动评估流利性和充分性。研讨会的结果进一步表明，Bleu系统地低估了基于规则的机器翻译系统的质量。我们报告并分析了几个人类评估师提供的系统排名与BLEU指标产生的系统排名之间存在严重分歧的案例。"}
{"pid": "C96-1021", "zh_sum": "每个人的回指：无语法分析器的代词回指消解我们提出了一种回指消解算法，该算法是（Lappin和Leass，1994）开发的算法的修改和扩展版本。与此相反，我们的算法不需要对文本进行深入、完整的语法分析。相反，在最小程度上降低输出质量的情况下，这些修改使解析过程能够从词性标记器的平铺输出开始，仅通过输入文本流中词汇项的语法功能注释来丰富。对我们实现结果的评估表明，在自然语言处理框架中可以实现准确的回指解析，而自然语言处理框架没有——不能——使用健壮可靠的解析组件。我们还认为回指消解是语篇指称消解的一部分。"}
{"pid": "P04-1041", "zh_sum": "基于自动获取的宽覆盖PCFG的LFG近似中的长距离依赖解析本文展示了如何对从树库获取的宽覆盖、鲁棒、概率词汇功能语法（LFG）资源自动获取长距离依赖（LDD）解析的有限近似。我们从为Penn II树库树自动生成的f结构中提取LFG子类别帧和链接LDD重入的路径，并将其用于LDD解析算法来解析新文本。与（Collins，1999；Johnson，2002）不同，在我们的方法中，LDD的解析是在f结构（基本谓词参数或依赖结构的属性-值结构表示）上完成的，在CFG解析树中没有空的结果、跟踪和共索引。目前，我们最好的自动归纳语法在分析宾夕法尼亚II树库《华尔街日报》部分第23节的f结构并对照DCU 1051进行评估时，f分数达到80.97%，在PARC 700依赖数据库中，f分数达到80.24%（King等人，2003），表现与最先进的手工制作语法相同或略好（Kaplan等人，2004）。我们的f-structure注释算法用于从Penn II treebank for English中归纳LFG资源，使用配置、分类、功能标记和跟踪信息。我们根据树节点的短语类别、母节点的类别及其相对于局部头的位置，将语法功能分配给树节点，从而自动将c结构映射到f结构。我们的解析器使用c结构树和f结构依赖自动注释输入文本，达到了较高的精度和召回率。"}
{"pid": "P06-1123", "zh_sum": "翻译等值复杂性的经验下界本文研究了各种位文本所表现出的翻译等值模式。研究发现，每个双文本中这些模式的复杂性都高于文献中的建议。这些发现揭示了为什么句法约束不能帮助改进统计翻译模型，包括基于有限状态短语的模型、树到字符串模型和树到树模型。本文还证明，即使在具有严格词序的语法相似的语言中，相对简单的实位文本中，倒装转换语法也不能生成某些翻译等价关系。我们的方法使用同步解析器所需的间隙数量来衡量单词对齐的复杂性，这允许不连续的跨度成功解析。翻译等值是指具有相同意义的语言表达之间的数学关系。我们认为，在规则的秩不超过2的约束下，为了使同步解析覆盖带注释的单词对齐数据，有必要使用不连续的跨度（即，对于同步CFG之外的形式主义）。"}
{"pid": "P08-1076", "zh_sum": "使用千兆字级未标记数据的半监督序列标记和分割本文证明，在半监督学习中使用更多的未标记数据可以提高自然语言处理（NLP）任务的性能，如词性标记、句法组块和命名实体识别。我们首先提出了一个简单而强大的半监督判别模型，适用于处理大规模的未标记数据。然后，我们描述了在广泛使用的测试集合上进行的实验，即PTB III数据、CoNLL'00和'03共享任务数据，分别用于上述三个NLP任务。我们合并了多达1G个字（十亿个标记）的未标记数据，这是这些任务中使用的最大数量的未标记数据，以研究性能改进。此外，我们的结果优于上述所有测试集合的最佳报告结果。我们在未标记的数据上运行一个基线鉴别分类器来生成伪示例，然后使用伪示例来为同一问题训练不同类型的分类器。我们使用自动标注的语料库来训练HMM。"}
{"pid": "E03-1005", "zh_sum": "一种新DOP模型的有效实现文献中存在两种明显相反的DOP模型：一种用于计算包含树库中最频繁子树的解析树，另一种用于计算包含树库中最少子树的解析树。本文提出了两种模型的集成，其性能分别优于它们各自。通过减少DOP的PCFG，我们提高了《华尔街日报》树库的准确性和效率。我们的结果显示，与以前的模型相比，错误率相对减少了11%，平均每个华尔街日报句子的处理时间为3.6秒。我们注意到，需要的是最高级别的解析，而不是派生。我们表明，使用最短派生标准选择测试句子首选语法的DOP模型表现得非常好。我们通过一个简单的修正因子来修正子树概率。"}
{"pid": "P09-1104", "zh_sum": "更好的单词对齐与监督ITG模型这项工作研究了利用倒装转换语法（ITG）约束的监督单词对齐方法。我们考虑最大裕度和条件似然目标，包括为规范化派生提出一种新的范式语法。即使对于非ITG句子对，我们也表明，通过简单地放松结构化区分性学习目标，可以学习ITG对齐模型。为了提高效率，我们描述了一组剪枝技术，这些技术使我们能够比单纯的双文本CKY分析更快地将句子对齐两个数量级。最后，我们引入了多对一块对齐功能，这大大改进了我们的ITG模型。总的来说，我们的方法得到了最好的汉英AER报告数，与GIZA++对齐相比，性能提高了1.1 BLEU。我们描述了一种修剪启发式算法，该算法的平均案例运行时间为O（n3）。"}
{"pid": "J07-4004", "zh_sum": "使用CCG和对数线性模型进行广泛覆盖的高效统计分析本文描述了一些用于自动提取词汇化语法的对数线性分析模型。这些模型是“完整”解析模型，因为概率是为完整解析定义的，而不是为通过分解解析树派生的独立事件定义的。区分性训练用于估计模型，这需要对训练数据中的每个句子进行不正确的语法分析以及正确的语法分析。使用的词汇化语法形式是组合范畴语法（CCG），该语法是从CCGbank（Penn Treebank的CCG版本）自动提取的。区分性训练和自动提取语法的结合导致了显著的内存需求（高达25 GB），通过在Beowulf集群上运行的BFGS优化算法的并行实现可以满足这一需求。打包图上的动态规划与并行实现相结合，使我们能够在三个小时内解决统计解析文献中规模最大的估计问题之一。对于训练和测试而言，解析系统的一个关键组件是最大熵supertagger，它将CCG词汇类别分配给句子中的单词。supertagger使区分性训练成为可能，同时也产生了一种高效的解析器。令人惊讶的是，考虑到CCG的“虚假歧义”，解析速度明显高于文献中报道的可比解析器的解析速度。我们还通过开发一种新的模型和高效的解析算法来扩展CCG的现有解析技术，该算法利用了所有派生，包括CCG的非标准派生。该模型和解析算法与范式约束相结合，为从CCGbank恢复谓词-参数依赖关系提供了最先进的准确性。该解析器也在DepBank上进行了评估，并与RASP解析器进行了比较，在总体和大多数关系类型上都优于RASP。对DepBank的评估提出了许多关于解析器评估的问题。本文为构建一个覆盖范围广泛的CCG解析器提供了一个全面的蓝图。我们证明了使用CCG可以实现准确和高效的解析。从语法分析的角度来看，C&C解析器在各种测试套件上都与最先进的统计解析器具有竞争力，包括语法关系（Clark和Curran，2007）、Penn Treebank短语结构树（Clark和Curran，2009）和无界依赖（Rimell et al，2009）。"}
{"pid": "P07-1032", "zh_sum": "与CCG和DepBank无关的解析器评估解析社区面临的一个关键问题是如何比较使用不同语法形式并产生不同输出的解析器。在用于创建解析器的相同资源上评估解析器可能会导致不可比较的准确度得分和对解析器性能的过于乐观的看法。本文评估了DepBank上的CCG解析器，并说明了将解析器输出转换为DepBank语法关系的困难。此外，我们还提出了一种测量转换有效性的方法，它提供了解析精度的上限。CCG解析器在标记依赖项上获得81.9%的F分数，而上界为84.8%。我们将CCG解析器与RASP解析器进行了比较，总体而言，在大多数依赖类型上都比RASP好5%以上。我们开发了一组从组合范畴语法解析器的输出到语法关系（GR）的映射规则（Carroll et al，1998）。我们演示了如何使用自适应超级标记、并行化和动态规划图表解析算法等技术来实现C＆C解析器，这是一种高效的CCG解析器，可以很好地对抗基于不同形式的解析器（Rimell et al，2009）。"}
{"pid": "N09-1036", "zh_sum": "改进非参数贝叶斯推理：基于适配器语法的无监督分词实验非参数贝叶斯推理之所以在计算语言学中备受关注，是因为它提供了一种学习泛化单元及其概率的原则性方法。适配器语法是定义各种层次非参数贝叶斯模型的框架。本文研究了在制定适配器语法和相关推理过程时出现的一些选择，并表明它们可以对无监督分词任务的性能产生显著影响。通过适当的适配器语法和推理程序，我们在Bernstein-Ratner语料库的标准Brent版本上获得了87%的单词标记f分数，与之前报道的该语料库的最佳结果相比，错误减少了35%以上。我们发现，具有音节结构语音约束和三层搭配结构的衔接语法的分词标记f得分最高。"}
{"pid": "P97-1063", "zh_sum": "翻译对等的词对词模型许多多语言NLP应用程序需要在不同语言之间翻译单词，但无法承担归纳或应用完整翻译模型的计算费用。对于这些应用，我们设计了一种快速算法来估计部分翻译模型，该模型仅在单词级别上考虑翻译对等。该模型的精度/召回权衡可以通过一个阈值参数直接控制。此功能使模型更适合不完全统计的应用程序。模型的隐藏参数可以很容易地根据模型外部的信息进行调整，从而提供一种简单的方法来集成预先存在的知识，如词性、词典、词序等。。我们的模型可以链接平行文本中的单词标记以及文献中的其他翻译模型。与其他翻译模型不同，它可以自动生成字典大小的翻译词典，并且准确率超过99%。我们提出了用于链接词对的竞争链接算法和通过爬山计算词对之间最佳对应级别的方法。单词对单词对齐中出现的一个问题如下：如果e1是f1的翻译，f2与f1有很强的单语关联，e1和f2也会有很强的相关性。"}
{"pid": "J06-1003", "zh_sum": "基于WordNet的词汇语义相关性度量方法的评价词汇语义相关性的量化在自然语言处理中有许多应用，并且提出了许多不同的度量方法。我们通过比较它们在检测和纠正真实单词拼写错误方面的性能，评估了其中五项指标，所有这些指标都使用WordNet作为其核心资源。Jiang和Conrath提出的基于信息内容的措施优于Hirst和St Onge、Leacock和Chodorow、Lin和Resnik提出的措施。此外，我们还解释了为什么分布相似性不能充分代表词汇语义相关性。众所周知，基于WordNet的度量由于其层次、分类结构，更适合于度量相似性而非相关性。"}
{"pid": "N06-1011", "zh_sum": "命名实体识别（NER）是许多自然语言处理任务的重要组成部分。目前大多数方法都采用机器学习技术，需要有监督的数据。然而，许多语言缺乏这种资源。本文提出了一种自动发现无资源语言中命名实体的算法，给出了一个双语语料库，其中命名实体在时间上与资源丰富的语言弱对齐。我们观察到，NEs在这些语料库中具有相似的时间分布，并且它们经常被音译，我们开发了一种算法，可以迭代地利用这两者。该算法利用了一种新的基于频率的时间分布度量和一种无资源的识别性音译方法。我们在一个英俄语料库上对该算法进行了评估，结果表明该算法在俄语中具有很高的NEs发现水平。以字符单元图和双元图为特征学习判别式音译模型，并将时间序列相似性与音译相似性模型相结合。"}
{"pid": "P00-1027", "zh_sum": "基于多模态对齐的最小监督形态分析本文提出了一种基于语料库的算法，能够对规则和高度不规则的形态（如→带来）来自大型单语文本中的分布模式，无直接监督。该算法结合了基于相对语料库频率、上下文相似度、加权字符串相似度和增量重排屈折转换概率的四种原始对齐模型。从没有成对的<屈折变化，根>训练示例开始，也没有事先播种合法的形态转换，3888个英语过去式测试用例的归纳分析准确率超过99.2%，目前在最不规则的形式上超过80%，在显示非连接后缀的形式上超过99.7%。我们在归纳英语过去时方面取得了突出的成果，首先列出了该语言的开放类词根列表、一种语言的屈折词类表以及每个词类的规范后缀。我们提出了一种提取与动词词根和屈折形式相关的形态规则的算法（但该算法可以扩展到其他形态关系）。本文提出的监督形态学学习者将柠檬化建模为一个词尾词干变化加上一个从潜在后缀列表（可能为空）中提取的后缀。"}
{"pid": "E89-1009", "zh_sum": "DATR中的推理DATR是一种声明性语言，用于表示一类受限的继承网络，允许多重继承和默认继承。主要的应用领域是自然语言处理的词法条目表示，我们始终使用该领域的示例。本文介绍了该语言的语法和推理机制。DATR企业的目标是设计一种简单的语言，该语言（i）具有必要的表达能力，能够对统一语法传统中当代作品预设的词汇条目进行编码，（ii）能够表达关于此类条目的所有明显概括，（iii）具有明确的推理理论，（iv）具有计算可处理性，and（v）具有明确的声明语义。本文件主要涉及（iii），尽管所使用的示例可能暗示我们在（i）和（ii）方面的策略。我们介绍了DATR，一种表示词汇知识的形式化语言。"}
{"pid": "N09-1009", "zh_sum": "无监督语法归纳中软参数关联的共享Logistic正态分布我们提出了一系列概率语法权重的先验分布，称为共享Logistic正态分布。该族扩展了分区逻辑正态分布，使得概率语法中不同派生事件的概率之间具有因子协方差，为未知语法的先验知识编码提供了一种新方法。我们描述了一种基于此先验族的变分EM算法来学习概率语法。然后，我们对无监督依赖语法归纳进行了实验，并使用我们的模型对单语学习和非平行多语言语料库的双语学习进行了显著改进。我们看到，我们最大的改进是将各种粗糙词类的参数单独联系在一起，然后在单语共享的基础上允许跨语言影响，只有适度的改进。"}
{"pid": "W01-0521", "zh_sum": "语料库变化和解析器性能统计分析中的大多数工作都集中在单个语料库上：宾夕法尼亚树库的《华尔街日报》部分。虽然这允许对解析技术进行定量比较，但对于其他类型的文本如何影响解析器性能，以及解析模型在语料库中的可移植性等问题仍然存在疑问。我们通过比较Brown语料库和WSJ语料库的结果来检验这些问题，并考虑解析器概率模型的哪些部分特别适合于它所训练的语料库。这就引出了一种修剪参数以减小解析模型大小的技术。我们表明，当应用于不同的类型和领域时，在Penn树库上训练的解析器的准确性会下降。我们报告了所有布朗语料库部分中40个或40个以下单词的句子的结果，对于这些结果，当仅对来自《华尔街日报》语料库的数据进行训练时，我们获得了80.3%/81.0%的查全率/准确率，当对来自《华尔街日报》语料库和布朗语料库所有部分的数据进行训练时，我们获得了83.9%/84.8%的查全率。"}
{"pid": "H94-1020", "zh_sum": "宾夕法尼亚树库：注释谓词参数结构宾夕法尼亚树库最近实施了一个新的语法注释方案，旨在突出谓词参数结构的各个方面。本文讨论了这种新的注释方案的关键方面的实现。它对广泛的语法现象进行了更加一致的处理，为wh移动、被动和无限结构的主语等现象提供了一组共同索引的空元素，提供了一些非上下文无关的注释机制，以便于恢复不连续成分的结构，并允许为某些语义角色提供清晰、简洁的标记系统。我们的总机语料库包含用短语结构树注释的口语、自发对话的转录本。PennTreebank II标记了主语（SBJ）、被动词的逻辑宾语（LGS）、一些简化关系从句（RRC）以及其他语法信息，但没有用语法角色标记每个成分。"}
{"pid": "N10-1020", "zh_sum": "推特对话的无监督建模我们提出了第一种无监督方法来解决在开放域中建模对话行为的问题。我们的方法基于嘈杂的推特对话语料库进行训练，通过对原始话语进行聚类来发现对话行为。因为它解释了这些行为的顺序行为，所以学习的模型可以深入了解新媒体中的沟通形式。我们通过定性可视化和内在对话排序任务来解决评估紧急模型的挑战。这项工作的灵感来自于130万条推特对话的语料库，这些对话将被公开。只有推特模糊了聊天和发布之间的界限，才能获得如此大量的数据，这突出表明需要能够快速适应新媒体。我们提出了一种基于秩相关系数的评价方法，它可以衡量序列数据上任意两个排序之间的相似度。我们通过选择包含3到6条帖子（话语）的推特对话来限制我们的数据集，使其能够列出所有排列。我们提出了一个概率模型来发现推特对话中的对话行为，并根据这些行为对对话中的推特进行分类。在块HMM下，会话中的消息按照马尔可夫过程流动，其中消息的单词根据与隐马尔可夫模型中的状态相关联的语言模型生成。"}
{"pid": "P08-1012", "zh_sum": "非合成短语的贝叶斯学习与同步语法我们将贝叶斯建模和同步语法的优点结合在基本翻译短语对的无监督学习中。同步语法的结构化空间自然适合短语对概率估计，尽管搜索空间可能会非常大。因此，我们探索有效的算法来修剪这个空间，从而得到经验上有效的结果。结合使用变分贝叶斯的稀疏先验，将模型偏向于可推广、简洁的参数集，从而显著改善了单词对齐。这种对稀疏解决方案的偏好，加上有效的修剪方法，形成了一种短语对齐方案，可以产生比标准单词对齐方法更好的端到端翻译。我们建议tic-tac-toe修剪，它使用模型1后验概率来排除计算中的细胞范围。"}
{"pid": "D08-1083", "zh_sum": "使用组合语义作为结构推理进行语义下情感分析，确定情感表达的极性需要的不仅仅是简单的词包方法。特别是，表达式中的单词或成分可以相互作用以产生特定的整体极性。在本文中，我们从组合语义的角度来看待这种子语义交互，并提出了一种新的基于学习的方法，该方法将由组合语义驱动的结构推理融入到学习过程中。我们的实验表明：（1）基于组合语义的简单启发式算法的性能优于不包含组合语义的基于学习的方法（准确率分别为89.7%和89.1%），但（2）将组合语义集成到学习中的方法的性能优于所有其他备选方法（90.7%）。我们还发现，在之前的研究中没有广泛使用的“内容词否定词”在确定表达水平极性方面起着重要作用。最后，与传统观点相比，我们发现，随着上下文的增加，表达级别的分类准确率会一致降低，这可能会消除歧义。内容词否定词是指不是虚词，但在语义上起否定词作用的词。我们通过启发式和机器学习的各种组合语义模型，将不同类型的否定词与词汇极性项相结合，以改进短语情感分析。我们提出了一种基于短语的情感分析算法，该算法在给定短语中单词的先验（即上下文外）极性和（正确的）短语级极性的情况下，学习中间情感分析决策变量的正确分配。我们手工编写组合规则，以模拟组合短语中不同单词的组合效果。我们将极性反转词分为两类：功能词否定词如not和内容词否定词如eliminate。"}
{"pid": "C02-1011", "zh_sum": "基于Web数据和EM算法的基本名词短语翻译我们在这里考虑基本名词短语翻译问题。我们提出了一种新的方法来执行该任务。对于给定的基本NP，我们首先从web上搜索其翻译候选。接下来，我们将使用我们开发的两种方法之一，从候选人中确定可能的翻译。在一种方法中，我们使用EM算法构建的朴素贝叶斯分类器集成。在另一种方法中，我们使用同样用EM算法构造的TF-IDF向量。实验结果表明，该方法的覆盖率和准确性明显优于依赖现有技术的基线方法。在我们的方法中，通过连接术语组成部分的翻译，合成术语的翻译候选，并通过测量与源语言术语的上下文相似性来重新排序。"}
{"pid": "C10-2005", "zh_sum": "从有偏见和嘈杂的数据中检测推特上的情感在本文中，我们提出了一种自动检测推特消息（tweets）上情感的方法，该方法探索了推特如何书写的一些特征以及构成这些消息的单词的元信息。此外，我们利用噪声标签的来源作为我们的训练数据。这些嘈杂的标签是由一些情绪检测网站通过推特数据提供的。在我们的实验中，我们表明，由于我们的特征能够捕获更抽象的推特表示，因此我们的解决方案比以前的解决方案更有效，并且对于有偏差和噪声的数据（这些数据源提供的数据）也更具鲁棒性。我们提出了一种利用具有抽象特征的SVM分类器对推特情感进行分类的两步方法。"}
{"pid": "N13-1039", "zh_sum": "利用词簇改进在线会话文本的词性标注我们考虑了非正式在线会话文本的词性标注问题。我们系统地评估了大规模无监督词聚类和新词汇特征的使用，以提高标记准确性。通过这些功能，我们的系统在Twitter和IRC词性标记任务上都实现了最先进的标记结果；推特标签的准确率从90%提高到93%（绝对准确率超过3%）。对这些词群的定性分析可以深入了解自然语言处理和这一类型的语言现象。此外，我们为此类文本提供了第一个POS注释指南，并发布了一个新的英语推文数据集，使用这些指南进行注释。标签软件、注释指南和大型词组位于：http://www.ark.cs.cmu.edu/TweetNLP本文介绍了CMU Twitter词性标记器的0.3版和带注释的数据。"}
{"pid": "P08-1024", "zh_sum": "统计机器翻译的判别潜变量模型大规模判别式机器翻译有望进一步发展最新技术，但与当前的启发式频率计数系统相比，未能取得令人信服的成果。我们认为，这种失败的一个主要原因是没有处理多个等效的翻译。我们提出了一个翻译模型，该模型将推导作为一个潜在变量，在训练和解码中进行建模，并且具有完全的区分性和全局优化。结果表明，考虑多重衍生确实可以提高性能。此外，我们还证明了正则化对于最大条件似然模型至关重要，以避免退化解。我们表明，在解码过程中边缘化不同的分段可以提高性能。我们提出了一个潜变量模型，可以清楚地描述翻译和派生之间的关系。对于基于层次短语的方法，我们提出了一个判别规则模型，并说明了在训练中仅使用维特比对齐和在所有可能的派生上使用全和之间的区别。"}
{"pid": "P00-1016", "zh_sum": "规则编写或注释：基本名词短语组块的经济高效资源利用本文对开发基本名词短语组块的两种方法进行了全面的实证比较：人工规则编写和使用交互式实时人工注释的主动学习。研究了主动学习的几种新变化，提出并探讨了用于跨模式机器学习比较的潜在成本模型。结果表明，在人力投资水平相当的情况下，使用主动学习注释而不是手工编写规则来训练系统更有效、更成功。我们使用基于bagging和分区的集成来进行基本NP组块的主动学习。"}
{"pid": "P98-1081", "zh_sum": "通过系统组合改进数据驱动的词类标记在本文中，我们研究了如何利用执行相同NLP任务的不同数据驱动系统之间建模的差异，以获得比最佳单个系统更高的精度。我们通过一个涉及形态-句法词类标记任务的实验来实现这一点。在相同的语料库数据上训练四个著名的标记生成器（隐马尔可夫模型、基于记忆、转换规则和最大熵）。在比较之后，使用几种投票策略和第二阶段分类器将它们的输出进行组合。所有组合标记器都优于其最佳组件，最佳组合的错误率比最佳单个标记器低19.1%。我们提出了三种投票策略：相等投票，其中每个分类器的投票权重相等；总体准确度，其中权重取决于分类器的总体准确度；成对投票。"}
{"pid": "N06-1020", "zh_sum": "有效的语法分析自我训练我们提出了一种简单但出人意料的有效的方法，使用现成的未标记数据对两阶段语法分析器重新排序系统进行自我训练。我们表明，当引导解析由一个有区别的重新排序器处理时，这种类型的引导解析是可能的。我们改进后的模型的f得分为92.1%，与之前华尔街日报解析的最佳结果相比，绝对提高了1.1%（错误减少了12%）。最后，我们提供一些分析，以更好地了解这一现象。我们提出了一种非常有效的方法来自训练两阶段句法分析系统，该系统由第一阶段生成词典化句法分析和第二阶段区分性重排组成。自我训练可能会受到过度拟合的影响，原始模型中的错误会在新模型中重复和放大。"}
{"pid": "P07-1003", "zh_sum": "为句法机器翻译系统提取树状变换器规则来调整单词对齐，可能会受到违反句法对应关系的单词对齐错误的阻碍。我们提出了一种新的无监督词对齐模型，该模型明确考虑了目标语言的组成结构，同时保持了HMM对齐模型的鲁棒性和效率。我们的模型预测在不牺牲对准质量的情况下，提高了树型传感器提取系统的产量。我们还讨论了各种基于后验的方法对协调双向对齐的影响。我们改进了HMM对齐器的失真模型，以反映树距离而不是字符串距离。我们使用硬联合竞争阈值。我们在HMM单词对齐模型中使用基于语法的距离来支持语法友好的对齐。"}
{"pid": "W10-0701", "zh_sum": "使用亚马逊创建语音和语言数据；s Mechanical Turk在本文中，我们介绍了如何使用亚马逊的Mechanical Turk众包平台为人类语言技术收集数据。我们调查了NAACL2010研讨会上发表的论文。24名研究人员参与了研讨会的共享任务，为语音和语言应用程序创建数据，费用为100美元。我们尝试使用Amazon Mechanical Turk（AMT）创建和评估人类语言数据（Callison Burch和Dredze，2010）。我们概述了使用MTurk的各种任务，并提供了一组确保高质量数据的最佳实践。"}
{"pid": "A97-1011", "zh_sum": "非投影依赖解析器我们描述了一个用于无限制依赖的实用解析器。解析器创建单词之间的链接，并根据其语法功能命名链接。我们首先描述了旧的约束语法解析器，其中有许多想法。然后，我们继续描述新解析器的核心思想。最后，对解析器进行评估。"}
{"pid": "P04-1013", "zh_sum": "在许多机器学习应用中，与传统的生成方法相比，神经网络统计解析器的鉴别训练鉴别方法有了显著的改进，但在将其扩展到自然语言解析方面存在困难。一个问题是，关于鉴别方法的许多工作将学习方法的更改与问题参数化的更改合并在一起。我们展示了如何在仍然根据生成概率模型对问题进行参数化的情况下，使用区分学习方法来训练解析器。我们提出了三种训练神经网络来估计统计解析器的概率的方法，一种是生成的，一种是判别的，另一种是概率模型是生成的，但训练标准是判别的。后一种模型优于前两种模型，达到了最先进的性能水平（成分的F值为90.1%）。我们讨论了为什么生成模型比参数化模型更好地直接估计后验概率。我们使用神经网络来诱导潜在的左角解析器状态。我们发现，通过区分性地重新排列生成模型的输出，区分性训练速度太慢，报告的准确性高于生成模型。"}
{"pid": "P07-1004", "zh_sum": "统计机器翻译的归纳学习统计机器翻译系统通常在目标语言的大量双语文本和单语文本上进行训练。在这篇论文中，我们探讨了如何使用半监督的翻译方法来有效地利用源语言中的单语数据，以提高翻译质量。为此，我们提出了几种算法，并介绍了每种算法的优缺点。我们对法语-英语EuroParl数据集和NIST中文-英语大数据轨道的数据进行了详细的实验评估。我们在这两项任务中都显示了翻译质量的显著提高。"}
{"pid": "D10-1125", "zh_sum": "基于对偶分解的非投影头自动机句法分析算法介绍了一种基于对偶分解的非投影句法分析算法。我们专注于非投射头自动机的解析算法，这是头自动机模型对非投射结构的推广。对偶分解算法简单高效，依赖于标准的动态规划和最小生成树算法。它们可证明地解决了非投影解析问题的LP松弛。根据经验，LP松弛通常很紧：对于许多语言，98%以上的测试句子都能得到精确解。我们的模型的准确性比以前在广泛数据集上的工作要高。我们考虑三阶特征，如外祖父母和三兄弟姐妹。"}
{"pid": "D07-1061", "zh_sum": "具有随机图的词汇语义关联性行走在许多系统中，如问答、多文档摘要和信息检索等任务需要鲁棒的词汇关联性数值度量。标准的基于同义词库的词对相似性度量仅基于同义词库图中这些词之间的一条路径。相比之下，我们提出了一个新的词汇语义关联模型，该模型包含了连接整个图中两个单词的每个显式或隐式路径的信息。我们的模型使用来自WordNet链接和语料库统计数据的随机遍历节点和边。我们将图视为马尔可夫链，并通过广义PageRank算法计算特定于单词的平稳分布。一个词对的语义相关性是通过一个新的发散度量ZKL来评分的，该度量在某些类别的分布上优于现有度量。在我们的实验中，由此产生的相关性度量是基于WordNet的度量，它与人类相似性判断的相关性最高，排序为？=。90、我们在WordNet上使用随机游动，将meronymy和字典光泽等信息结合在一起。"}
{"pid": "A97-1029", "zh_sum": "Nymble：一种高性能的学习名字查找器本文介绍了一种统计、学习的方法，使用标准隐马尔可夫模型的变体在文本中查找名字和其他非递归实体（根据NE任务的MUC-6定义）。我们提出了问题的理由和我们的方法，详细讨论了模型本身，最后介绍了这种新方法的成功结果。我们开发了Nymble，一个基于HMM的英文和西班牙文姓名标记系统。Nymble使用统计学习获得一个隐马尔可夫模型（HMM），该模型可以识别文本中的NEs。"}
{"pid": "A94-1006", "zh_sum": "Termight：识别和翻译技术术语我们提出了一种半自动工具Termight，它可以帮助专业翻译人员和术语学家识别技术术语及其翻译。该工具利用词性标记和单词对齐程序来提取候选词及其翻译。虽然提取程序还远远不够完善，但用户从谷壳中过滤出小麦并不难。提取算法强调完整性。备选方案可能会遗漏重要但不常见的术语/翻译。为了减少用户在筛选阶段的负担，候选项以方便的顺序呈现，并在一个旨在最大程度减少击键次数的界面中提供一些有用的一致性证据。Termight目前正由at&T商务翻译服务公司（前身为at&T语言专线服务公司）的翻译人员使用。"}
{"pid": "P08-1023", "zh_sum": "基于林的翻译在基于语法的翻译模型中，基于树的方法是一个很有希望的方向，它以源语句的解析树作为输入，比基于字符串的方法更快、更简单。然而，当前基于树的系统有一个主要缺点：它们只使用2-best解析来指导翻译，这可能会由于解析错误而导致翻译错误。我们提出了一种基于林的方法，该方法可以转换一个包含指数级多个解析的压缩林，它编码的备选方案比标准的n-best列表多得多。大规模实验表明，与1-最佳基线相比，BLEU分数绝对提高了1.7分。这一结果也比30次最佳解析的解码高0.8个百分点，而且所需时间更少。在解码时，我们将输入的句子解析为树，并通过规则模式匹配将其转换为翻译林。我们建议首先直接使用填充森林。"}
{"pid": "W04-0807", "zh_sum": "Senseval-3英语词汇样例任务本文介绍了英语词汇样例任务的任务定义、资源、参与系统和比较结果，该任务是作为Senseval-3评估练习的一部分组织的。这项任务吸引了来自世界各地的27个团队的参与，共有47个系统。"}
{"pid": "W06-1616", "zh_sum": "非投影依赖解析的增量整数线性规划整数线性规划最近被用于许多概率模型的解码，以强制全局约束。然而，在某些应用中，如非投影依赖解析和机器翻译，将解码问题完全表述为整数线性规划使得求解变得困难。我们提出了一种增量求解问题的方法，从而避免了创建难以处理的整数线性规划。这种方法被应用于荷兰语依赖项解析，我们展示了添加语言动机约束如何比最新技术产生显著的改进。对于依赖分析，我们研究了一种使用整数线性规划的方法，该方法可以结合全局语言约束。我们在依赖解析方面的工作表明，当以增量方式使用时，可以使用ILP对非常大的程序执行有效的推理。我们表明，如果使用切割平面算法（Dantzig et al，1954），使用ILP解算器甚至可以有效地解决指数级大的解码问题。我们通过一种增量方法来解决依赖关系解析的映射问题，这种方法从放松问题开始，解决它，并仅在违反约束的情况下添加其他约束。"}
{"pid": "W11-2123", "zh_sum": "KenLM：更快、更小的语言模型查询我们提供了KenLM，这是一个实现两种数据结构的库，用于高效的语言模型查询，减少了时间和内存成本。探测数据结构使用线性探测哈希表，是为提高速度而设计的。与广泛使用的SRILM相比，我们的探测模型在使用57%内存的情况下速度是SRILM的2.4倍。TRIE数据结构是一种具有位级打包、排序记录、插值搜索和可选量化的TRIE，旨在降低内存消耗。TRIE同时使用的内存少于最小无损基线，CPU少于最快基线。我们的代码是开源的、线程安全的，并集成到Moses、cdec和Joshua翻译系统中。本文描述了使用的几种性能技术，并针对替代实现提出了基准测试。我们描述了一个语言建模库。"}
{"pid": "P03-2026", "zh_sum": "日本学习者英语口语数据中的自动错误检测本文介绍了一种检测日本学习者英语语法和词汇错误的方法以及其他技术，这些技术可以在有限的训练数据中提高错误检测的准确性。在本文中，我们通过使用包含学习者错误信息的学习者语料库进行实验，证明了所提出的方法在多大程度上有希望。在一项基于访谈的英语口语测试中，我们使用日语使用者的错误注释成绩单来训练最大熵分类器，以识别13种不同类型的语法和词汇错误，包括涉及介词的错误。在日语学习者的英语语料库中（Izumi et al.，2003），与动词相关的错误是最常见的类别。在日本学习者英语语料库中，冠词的使用是最常见的错误类别。"}
{"pid": "P04-1014", "zh_sum": "使用CCG和对数线性模型解析WSJ本文描述并评估了组合范畴语法（CCG）的对数线性解析模型。描述了L-BFGS优化算法的并行实现，该算法在Beowulf集群上运行，允许使用完整的Penn树库进行估计。我们还为CCG开发了一种新的高效解析算法，最大限度地提高了依赖项的预期召回率。我们将使用所有CCG推导（包括非标准推导）的模型与标准形式模型进行比较。这两种模型的性能具有可比性，其结果与现有的宽覆盖CCG解析器具有竞争力。我们的CCG解析器非常准确和高效，在《华尔街日报》文本上以超过84%的总F分数恢复标记的依赖项，并且每秒解析多达50个句子。我们的解析性能依赖于每个单词至少97%的超级标记准确率和至少60%的句子准确率（每个单词1.5个类别）。我们的解析性能表明了超级标记准确度与总体依赖恢复的对应关系。"}
{"pid": "P03-1001", "zh_sum": "在线问答的离线策略：在提问之前回答问题最近在问答方面的工作集中在基于网络的系统上，该系统使用简单的词汇句法模式提取答案。我们提出了一种替代策略，其中使用模式离线提取高度精确的关系信息，创建用于高效回答问题的数据存储库。我们根据最先进的基于网络的问答系统，对具有挑战性的问题子集，即“谁是…”问题，评估我们的策略。结果表明，提取的关系正确回答了25%以上的问题，比现有系统快三个数量级。我们使用词性模式来提取涉及专有名词的下义关系子集。利用机器学习方法滤除噪声，可以显著提高提取信息的精度。"}
{"pid": "W06-1642", "zh_sum": "面向领域情感分析的全自动词库扩展本文提出了一种无监督的词库构建方法，用于检测极性从句，这些极性从句在特定领域中表达积极或消极的方面。要获得的词条被称为极性原子，是人类可以理解的最小句法结构，它规定了从句的极性。作为获得候选极性原子的线索，我们使用上下文相干性，即相同极性在上下文中连续出现的趋势。使用语料库中的整体密度和一致性精度，统计估计可以在候选对象中拾取适当的极性原子，而无需手动调整阈值。实验结果表明，使用自动获取的词典进行极性赋值的平均精度为94%，该方法对不同领域的语料库和初始词典的大小都具有鲁棒性。我们验证了具有相同极性的极性文本单元倾向于一起出现，以使上下文连贯。我们提出了一种基于上下文一致性自动扩展初始意见词典的算法，即相同极性在上下文中连续出现的趋势。我们从大型领域语料库中使用连接规则来解决这个问题。我们采用领域知识，从特定领域的语料库中提取情感词。"}
{"pid": "J04-3002", "zh_sum": "学习主观语言自然语言中的主观语言是指用来表达观点、评价和推测的语言方面。有许多与主观性分析相关的自然语言处理应用，包括信息提取和文本分类。这项工作的目标是从语料库中学习主观语言。生成并测试主观性线索，包括低频词、搭配以及使用分布相似性识别的形容词和动词。这些特性也将协同工作进行检查。使用不同的过程从不同的数据集生成的特征在性能上表现出一致性，因为它们在相同的数据集上表现得更好和更差。此外，本文还表明，周围语境中主观性线索的密度会强烈影响单词的主观性，并提供了一项注释研究的结果，该研究评估了具有高密度特征的句子的主观性。最后，使用这些线索进行意见片识别（一种文本分类和体裁检测），以证明本文所获得知识的实用性。我们发现低频词和一些搭配是一个很好的主观性指标。"}
{"pid": "N04-4038", "zh_sum": "阿拉伯语文本的自动标记：到目前为止，从原始文本到基本短语块，还没有完全自动化的系统来满足社区对阿拉伯语文本基本语言处理工具的需求。在本文中，我们提出了一种基于支持向量机（SVM）的方法来自动标记（切分clitics）、词性（POS）标记和注释阿拉伯语文本中的基本短语（BPs）。我们采用为英语文本开发的高度精确的工具，并将其应用于阿拉伯语文本。Khoja（2001）首先为阿拉伯语引入了一个tagger，它有131个标记，但这项工作已经折叠了标记集以简化标记。我们描述了一种基于支持向量机的词性标记器，该标记器在标记化数据（clitics是单独的标记）上进行训练，标记准确率为95.5%。"}
{"pid": "W02-1011", "zh_sum": "竖起大拇指？情感分类使用机器学习技术，我们考虑的问题不是按主题，而是按总体情感对文档进行分类，例如，确定评论是正面的还是负面的。使用电影评论作为数据，我们发现标准的机器学习技术明显优于人类产生的基线。然而，我们使用的三种机器学习方法（朴素贝叶斯、最大熵分类和支持向量机）在情感分类上的表现不如传统的基于主题的分类。最后，我们分析了使情绪分类问题更具挑战性的因素。我们从电影数据库中收集评论，并根据评论人提供的培训将其分为正面、负面或中性。我们建议，基于术语的模型比基于频率的模型表现更好。"}
{"pid": "P10-4002", "zh_sum": "cdec：有限状态和上下文无关翻译模型的解码器对齐和学习框架我们介绍了cdec，这是一个开源框架，用于解码、对齐和培训许多统计机器翻译模型，包括基于单词的模型、基于短语的模型和基于同步上下文无关语法的模型。解码器使用翻译林的单一统一内部表示，将特定于模型的翻译逻辑与一般的重新排序、修剪和推理算法严格分离。从这种统一表示中，解码器不仅可以提取1或k最佳翻译，还可以提取与参考的对齐，或使用基于梯度或无梯度优化技术驱动区分性训练所需的数量。其高效的C++实现意味着内存使用和运行时性能明显优于同类解码器。我们介绍了cdec，一种基于短语的分层翻译模型的快速实现（Chiang，2005），它代表了一种最先进的翻译系统。我们的cdec解码器以无监督的方式从原始文本中学习分词格。"}
{"pid": "C92-1025", "zh_sum": "两级形态与成分我们认识到，一个级联的组合FST可以实现两级模型。我们观察到，规则集可以有效地与词典传感器组成，并且生成的传感器在大小上与词典传感器本身大致相似。"}
{"pid": "W03-0419", "zh_sum": "CoNLL-2003共享任务简介：与语言无关的命名实体识别我们描述了CoNLL-2003共享任务：与语言无关的命名实体识别。我们提供了有关数据集（英语和德语）和评估方法的背景信息，概述了参与任务的系统，并讨论了它们的性能。"}
{"pid": "P93-1003", "zh_sum": "一种在双语语料库中寻找名词短语对应关系的算法本文描述了一种利用英语和法语文本标记器在对齐的双语语料库中关联名词短语的算法。标记提供词性类别，有限状态识别器使用这些类别提取两种语言的简单名词短语。然后，使用迭代重新估计算法将名词短语相互映射，该算法与用于训练标记者的Baum-Welch算法相似。该算法为寻找单词对应关系提供了一种替代方法，其优点是结合了语言结构。描述了对基本算法的改进，这使得在构造名词短语映射时能够考虑上下文。我们尝试使用词性标注和名词短语识别方法在平行语料库中发现名词短语对应关系。"}
{"pid": "C00-2136", "zh_sum": "信息提取领域知识的自动获取在为一类新的事件或关系开发信息提取（IE）系统时，主要任务之一是确定这些事件或关系在文本中表达的多种方式。这通常涉及手动分析，在某些情况下，还涉及涉及这些事件的大量文本的注释。本文提出了一种基于自动发现过程EXDISCO的替代方法，该方法识别一个集合；从一小组“种子模式”开始，从未注释的文本中提取相关文档和一组事件模式。我们通过比较发现的模式与手动构建的系统在实际提取任务中的性能来评估EXDISCO。我们提出了一种针对少量示例学习抽取模式的算法，大大减轻了应用程序开发人员的负担，减少了知识获取瓶颈。我们选择了一种方法，其动机是假设包含大量已确定为与特定IE场景相关的模式的文档可能包含进一步的相关模式。ExDisco使用一种引导机制，使用未注文本和一些种子模式作为初始输入来发现新的提取模式。"}
{"pid": "A00-2026", "zh_sum": "表面自然语言生成的可训练方法我们提出了三个表面自然语言生成系统，它们可以从带注释的语料库中进行训练。前两个系统（称为NLG1和NLG2）需要一个仅标记有特定领域语义属性的语料库，而最后一个系统（称为NLG3）需要一个同时标记有语义属性和句法依赖信息的语料库。所有系统都试图从特定领域的语义表示中生成语法自然语言短语。NLG1是一个基线系统，使用短语频率一步生成整个短语，而NLG2和NLG3使用最大熵概率模型分别生成短语中的每个单词。NLG2和NLG3系统学习确定短语的单词选择和词序。我们提出的实验中，我们生成短语来描述航空旅行领域中的航班。我们使用最大熵模型来驱动单词二元图或依赖关系表示的生成，同时考虑（未实现的）语义特征。我们使用大量生成模板来实现曲面。我们提出了最大熵模型来学习属性排序和词汇选择，以便从属性-值对的语义表示中生成句子，并限制在航空旅行领域。"}
{"pid": "P89-1002", "zh_sum": "基于统一形式的语义头驱动生成算法我们提出了一种从逻辑形式编码生成字符串的算法，该算法改进了以前的算法，因为它对适用的语法类限制较少。特别是，与Earley演绎生成器（Shieber，1988）不同，它允许使用语义非单调语法，但与自顶向下方法不同，它还允许左递归。该算法的启用设计功能是隐式遍历以语义头驱动方式生成的字符串的分析树。我们指出了左递归规则中的一个终止问题。"}
{"pid": "A00-2019", "zh_sum": "一种无监督的语法错误检测方法我们提出了一种无监督的语法错误检测方法，通过从编辑的文本语料库中推断否定证据来检测语法错误。该系统的开发和测试使用的是对托福（TOEFL）英语考试提示的论文长度反应。错误识别系统ALEK的准确率约为80%，召回率约为20%。我们试图根据上下文来识别错误——更具体地说，是围绕感兴趣的单词的两个单词窗口，我们从中考虑虚词和词性标记。除了n克的原始频率外，我们还使用互信息度量。语法功能包括句子片段、动词形式错误和代词错误等错误。我们利用互信息和卡方统计从一个庞大的结构良好的语料库中识别一小部分目标词的典型上下文。"}
{"pid": "P11-1061", "zh_sum": "基于双语图形投影的无监督词性标注我们描述了一种新方法，用于为没有标记的训练数据，但有资源丰富的语言翻译文本的语言诱导无监督词性标注器。我们的方法不假设任何关于目标语言的知识（特别是假设没有标记词典），因此它适用于各种资源贫乏的语言。我们使用基于图的标签传播进行跨语言知识转移，并使用投影标签作为无监督模型中的特征（Berg-Kirkpatrick et al.，2010）。在八种欧洲语言中，我们的方法比最先进的基线平均绝对改善10.4%，比使用期望最大化算法建立的普通隐马尔可夫模型平均绝对改善16.7%。我们通过在平行文本中使用单词对齐，从资源丰富的语言传输带注释的数据，为特定语言构建词典。"}
{"pid": "P08-1084", "zh_sum": "用于形态分割的无监督多语言学习几个世纪以来，语言之间的深层联系带来了人类交流方面的重大发现。在本文中，我们研究如何利用这一强大的信息源进行无监督的语言学习。特别地，我们研究了多语言的形态切分任务。我们提出了一个非参数贝叶斯模型，该模型可以联合归纳每种语言的语素切分，同时识别跨语言语素模式或抽象语素。我们将我们的模型应用于三种闪族语言：阿拉伯语、希伯来语、亚拉姆语以及英语。我们的结果表明，与单语模型相比，串联学习形态学模型可以减少多达24%的错误。此外，我们提供的证据表明，我们的联合模型在应用于同一语系的语言时取得了更好的性能。我们使用双语信息，但切分是独立于翻译建模学习的。"}
{"pid": "P04-1085", "zh_sum": "识别会话中的同意和不同意：使用贝叶斯网络建模语用依赖我们描述了一种统计方法，用于建模会话交互中的同意和不同意。我们的方法首先基于一组词汇、持续时间和结构特征（在语篇中向前和向后）使用最大熵排序来识别邻接对。然后，我们利用这些邻接对和表征先前同意或不同意对当前话语的各种语用影响的特征，将话语分为同意或不同意。我们的方法实现了86.9%的准确率，比以前的工作提高了4.9%。我们将相邻对定义为由两部分组成，这两部分是有序的、相邻的，并且由不同的扬声器产生。我们还实现了说话人识别率提高8%。我们使用动态贝叶斯网络考虑说话人变化、段持续时间和邻接对序列依赖特征。我们建议，通过增加功能集可以获得进一步的收益。"}
{"pid": "J92-1004", "zh_sum": "TINA：一种用于口语应用的自然语言系统一种新的自然语言系统TINA，是为涉及口语任务的应用程序开发的。TINA整合了上下文无关语法、增强转换网络（ATN）和统一概念的关键思想。TINA提供了语法和语义分析之间的无缝接口，还生成了一个高度约束的概率语言模型，以提高识别性能。首先将手动提供的初始无上下文重写规则集转换为网络结构。网络中所有弧的概率赋值都是从一组例句中自动获得的。解析器使用堆栈解码搜索策略，具有自上而下的控制流，并包括一个特征传递机制来处理长距离移动、一致性和语义约束。TINA提供了一种自动句子生成功能，可以有效地识别过度泛化问题，并为识别器生成单词对语言模型。该解析器目前与麻省理工学院的SUMMIT识别器集成，用于两个应用领域，解析器在句子层次上筛选识别器输出，或在主动搜索过程中过滤部分理论。我们提出了语言理解系统TINA，该系统集成了上下文无关语法、增强的转换网络和统一概念等关键思想。"}
{"pid": "P94-1020", "zh_sum": "使用可分解模型的词义消歧大多数用于词义消歧的概率分类器要么仅基于一个上下文特征，要么使用一个简单假设的模型来描述多个上下文特征之间的相互依赖关系。本文提出了一种不同的方法来建立概率模型，并对以这种方式生成的模型的性能进行了案例研究，以消除名词兴趣的歧义。我们描述了一种建立概率模型的方法，该模型使用多个上下文特征进行词义消歧，而不需要关于模型形式的未经测试的假设。使用这种方法，所有变量的联合分布仅由最系统的变量相互作用来描述，从而限制要估计的参数数量，支持计算效率，并提供对数据的理解。我们手动分配了《朗文当代英语词典》中2476个有意义标记的有趣用法。"}
{"pid": "P93-1035", "zh_sum": "自动语法归纳和解析自由文本：一种基于转换的方法本文描述了一种解析自由文本的新技术：自动学习转换语法I，它能够将文本精确地解析为二元分支句法树，并且不标记非终结符。该算法的工作原理是从了解短语结构的非常幼稚的状态开始。通过反复比较当前状态下的括号结果与训练语料库中提供的适当括号，系统学习一组可用于减少错误的简单结构转换。在描述了算法之后，我们给出了结果，并将这些结果与自动语法归纳中的其他最新结果进行了比较。我们基于转换的标记需要手动标记文本进行培训。"}
{"pid": "N13-1090", "zh_sum": "连续空间词表征中的语言规律连续空间语言模型最近在各种任务中取得了显著的结果。在本文中，我们研究了通过输入层权重隐式学习的向量空间单词表示。我们发现，这些表示法在捕捉语言的句法和语义规律方面出人意料地出色，并且每个关系都有一个特定于关系的向量偏移量。这允许基于单词之间的偏移量进行面向向量的推理。例如，男性/女性关系是自动学习的，使用诱导向量表示，“国王-男人+女人”会导致向量非常接近“女王”。我们证明，词向量通过句法类比问题（本文提供）捕捉句法规律，并且能够正确回答几乎40%的问题。我们证明，通过使用向量偏移方法回答SemEval-2012任务2的问题，单词向量能够捕捉语义规律。值得注意的是，这种方法优于以前最好的系统。通过CBOW预测模型，我们在句法子集（syn）上达到了最高精度。"}
{"pid": "J96-1001", "zh_sum": "双语词汇搭配的翻译：众所周知，非母语人士很难翻译搭配，这主要是因为搭配不透明，无法逐字翻译。我们描述了一个名为Champollion的程序，该程序给定两种不同语言的一对平行语料库和其中一种语言的搭配列表，自动生成它们的翻译。我们的目标是提供一种工具，用于编译不同领域的多语言单词级以上的双语词汇信息。我们使用的算法基于统计方法，生成n个单词搭配的p单词翻译，其中n和p不必相同。例如，Champollion翻译make。。。决策、就业公平和股票市场进入就业市场。。。分别为decision、equite en matiere d'emploi和交易所。在汉莎语料库中对Champollion进行为期三年的测试，得出每年300个搭配的法语译文，平均准确率为73%。在本文中，我们描述了使用的统计度量、算法和Champollion的实现，给出了我们的结果和评估。本文讨论了逐点互信息与骰子系数之间的关系。我们提出了一种基于语料库的双语词汇提取方法。我们提出了一种骰子系数的统计关联度量来处理搭配翻译问题。"}
{"pid": "H93-1061", "zh_sum": "语义一致性语义一致性是一个文本语料库和一个词汇的组合，使得文本中的每个实体词都与其在词汇中的适当意义相关联。因此，它既可以被视为一个语料库，其中的单词在句法和语义上都被标记，也可以被视为一个词典，在其中可以找到许多定义的例句。目前正在构建语义一致性，用于研究语境中的语义消解（语义消歧）。Brown语料库是文本，WordNet是词典。语义标记（指向WordNet语法集的指针）使用一个接口（上下文）手动插入到文本中，该接口是为方便任务而设计的。另一个界面支持搜索标记的文本。提出了语义一致性的一些实际应用。我们提供了SemCor，一个平衡的语义标注数据集，所有内容词都由训练有素的词典编纂者手动标记。"}
{"pid": "P07-1059", "zh_sum": "答案检索中查询扩展的统计机器翻译我们提出了一种答案检索中查询扩展的方法，该方法使用统计机器翻译（SMT）技术来弥合问题和答案之间的词汇差距。基于SMT的查询扩展是通过i）使用一个完整的句子释义器在整个查询的上下文中引入同义词，以及ii）使用一个在问答对上训练的完整句子SMT模型将查询词翻译成答案词来完成的。我们评估了这些全局上下文感知查询扩展技术在tfidf检索中的作用，tfidf检索从FAQ页面提取的1000万个问答对。实验结果表明，基于SMT的扩展比局部扩展和无扩展的检索性能都有提高。我们通过使用一个更复杂的翻译模型来展示基于翻译的答案检索方法的优势，该模型也是从Web上的FAQ中提取的大量数据中训练出来的。"}
{"pid": "W02-0902", "zh_sum": "从单语语料库中学习翻译词汇本文介绍了从无关单语语料库中构建单词级翻译词汇的工作。我们结合了各种线索，如同源词、相似上下文、词语相似性的保持和词语频率。本文报道了一个德英名词词汇构建的实验结果。在平行测试语料库中，名词翻译的准确率可以达到39%。我们通过使用相同的拼写特征，如同源词和相似上下文，自动归纳出初始种子双语词典。"}
{"pid": "P93-1032", "zh_sum": "从语料库中自动获取大型子分类词典本文提出了一种从未标记文本语料库中生成子分类框架词典的新方法。结果表明，尽管标记器和解析器的错误率很高，但对随机标记器输出上运行的有限状态解析器的结果进行统计过滤可以产生高质量的结果。此外，有人认为，这种方法可以用于学习所有子类别化框架，而以前的方法无法扩展到问题的一般解决方案。我们使用了400万单词的《纽约时报》语料库，只选择带有助动词的从句，然后用有限状态解析器自动分析它们。"}
{"pid": "W02-1018", "zh_sum": "基于短语的统计机器翻译联合概率模型我们提出了一种统计机器翻译联合概率模型，该模型可以自动从双语语料库中学习单词和短语等价物。使用联合模型估计的参数生成的翻译比使用IBM模型4生成的翻译更精确。我们提出了一个联合概率模型，该模型搜索短语对齐空间，同时学习单词和短语的翻译词典，而不考虑潜在的次优单词对齐和短语提取的启发式方法。"}
{"pid": "W03-0428", "zh_sum": "基于字符级模型的命名实体识别我们讨论了两种命名实体识别模型，它们单独使用字符和字符n-gram，或者作为其数据表示的重要部分。第一个模型是具有最小上下文信息的字符级HMM，第二个模型是具有更丰富上下文特征的最大熵条件马尔可夫模型。根据英语测试数据，我们的最佳车型F1总体成绩为86.07%（开发数据为92.31%）。这个数字表示在没有word内部（子字符串）功能的同一型号上，错误减少了25%。我们发现，字符n-gram特征的引入将F1的整体得分提高了20%以上。"}
{"pid": "P98-1010", "zh_sum": "基于记忆的浅层自然语言模式学习方法识别浅层语言模式，如单词之间的基本句法关系，是应用自然语言和文本处理中的一项常见任务。处理这项任务的常见做法是通过繁琐的手动定义可能的模式结构，通常是以正则表达式或有限自动机的形式。提出了一种新的基于记忆的学习方法，该方法基于括号内的训练语料库识别新文本中的浅层模式。训练数据按原样存储在高效的后缀树数据结构中。通过将新文本的子序列与语料库中的正负证据进行比较，在识别时在线进行泛化。这样，训练中就不会丢失任何信息，就像在训练时构建单一广义模型的其他学习系统一样。本文介绍了识别英语名词短语、主谓和动宾模式的实验结果。由于这种学习方法可以方便地移植到新的领域，我们计划将其应用于其他语言的语法模式和用于信息提取的子语言模式。我们将多个单词的词性序列分割成小的词性标题，在训练集中分别计算新词和非新词的词性频率，并利用这些计数检测新词。"}
{"pid": "D08-1024", "zh_sum": "句法和结构翻译特征的在线大幅度训练最小错误率训练（MERT）由于其能够可靠优化的权值数量有限，成为当前统计机器翻译发展的瓶颈。在Watanabe等人工作的基础上，我们探索了使用Crammer等人的MIRA算法作为MERT的替代方法。我们首先表明，通过并行处理和利用更多的解析林，我们可以使用MIRA获得在翻译质量和计算成本方面与MERT相当或超过MERT的结果。然后，我们在两类特征上测试了该方法，这两类特征解决了Hiero层次短语模型中的不足：首先，我们同时训练了大量Marton和Resnik的软语法约束，其次，我们引入了一种新的结构扭曲模型。在这两种情况下，我们在翻译性能上都取得了显著的改进。通过组合优化它们，对于总共56个特征权重，我们在NIST 2006年阿拉伯语-英语评估数据的子集上将性能提高了2.6 BLUE。我们将结构失真特征引入到一个基于层次短语的模型中，目的是为给定源跨度的非终端重排序建模。我们表明，与MIRA等高维优化器相比，MERT具有少量的特征。我们的特性明确地对抗了对规则计数的高估，或具有不好的重叠点、不好的重写或不希望插入目标端终端的规则。"}
{"pid": "W98-1119", "zh_sum": "本文提出了一种识别代词回指的算法，并基于该算法进行了两个实验。我们将多个回指消解因素纳入统计框架，特别是代词和所提议的先行词之间的距离、所提议的先行词的性别/数量/动态性、控制头部信息和名词短语重复。我们将它们组合成一个单一的概率，使我们能够识别所指对象。我们的第一个实验显示了每个信息源的相对贡献，并且证明了所有信息源组合的成功率为82.9%。第二个实验研究了一种无监督学习性别/数字/动画信息的方法。我们给出了一些实验来说明该方法的准确性，并注意到添加了这些信息后，我们的代词解析方法达到了84.2%的准确率。我们在树库中添加了对定代词先行词的注释。我们实现了一个霍布斯距离特征，该特征通过霍布斯（1978）的基于语法的代词解析算法对代词候选先行词的等级进行编码。我们计算话语实体在话语中已经被提及的次数。我们的概率方法结合了三个因素（除了一致性过滤器）：霍布斯算法的结果、取决于文章中句子位置的提及次数，以及代词在局部上下文中出现先行词的概率。我们描述了一种基于完全句法信息的监督概率代词消解算法。"}
{"pid": "P06-1114", "zh_sum": "在开放领域问答中使用文本蕴涵的方法问题语义学研究表明，问题与其答案之间的关系可以用逻辑蕴涵来表达。在本文中，我们展示了如何使用设计用于识别文本蕴涵的计算系统来提高当前开放域自动问答（Q/A）系统的准确性。在我们的实验中，我们表明，当使用文本蕴涵信息对问答系统返回的答案进行过滤或排序时，总的来说，准确率可以提高20%。我们应用TE组件来重新排列由问答任务的检索步骤返回的候选答案。"}
{"pid": "P07-1092", "zh_sum": "三角剖分机器翻译：有效利用多个平行语料库当前基于短语的SMT系统在使用小训练集时表现不佳。这是由于不可靠的翻译估计以及对源语和目标语的覆盖率较低造成的。本文提出了一种利用同一源短语的多个翻译来缓解这一问题的方法。我们的方法的核心是三角测量，即通过中间的第三语言将源语言翻译成目标语言的过程。这允许使用范围更广的平行语料库进行培训，并且可以使用传统的平滑方法与标准短语表相结合。实验结果表明，与基于标准短语的系统相比，BLEU改进了三角化模型。"}
{"pid": "D07-1003", "zh_sum": "什么是危险模型？本文提出了一种句法驱动的问答方法，特别是针对短答案问题的答案句选择问题。我们没有使用句法特征来扩充现有的统计分类器（如之前的工作中所述），而是基于这样一种想法，即问题及其（正确的）答案通过松散但可预测的句法转换相互关联。我们提出了一种概率准同步语法，其灵感来自于机器翻译（D.Smith和Eisner，2006），并通过鲁棒的非词汇语法/对齐模型与（n可选）词汇语义驱动的对数线性模型的混合来参数化。我们的模型将软对齐作为区别性训练中的隐藏变量进行学习。使用TREC数据集的实验结果显示，其性能显著优于强大的最新基线。我们探索使用一种称为准同步语法的形式主义（Smith和Eisner，2006），以便找到一个更明确的模型来匹配依赖集，但仍然允许松散的匹配。我们使用准同步转换将问题中的所有父子路径映射到答案中的任何路径。"}
{"pid": "P06-1066", "zh_sum": "统计机器翻译中基于最大熵的短语重排序模型我们提出了一种新的基于短语的统计机器翻译（SMT）重排序模型，该模型使用最大熵（MaxEnt）模型来预测相邻块（短语对）的重排序。该模型提供了与内容相关的、层次化的短语重新排序，并基于从真实bitext中自动学习的特征进行了泛化。提出了一种从双语数据中提取相邻块所有重排序事件的算法。在我们的汉英翻译实验中，这种基于MaxEnt的重新排序模型在NIST MT-05和IWSLT-04任务中的BLEU分数得到了显著改善。我们改进了基于并置词交叉相邻成分的SMT重排序模型。我们为括号转换语法（BTG）提出了一个成分重新排序模型（Wu，1995），该模型预测了一对子成分在组合形成新成分时重新排序的可能性。在我们基于最大熵的重排序模型MEBTG中，每个子句的翻译都使用了三个规则：词汇规则、直线规则和倒排规则。"}
{"pid": "P91-1022", "zh_sum": "在平行语料库中对齐句子在本文中，我们描述了一种在两个平行语料库中对齐句子及其翻译的统计技术。除了数据中可用的某些锚定点之外，我们用于计算对齐的句子的唯一信息是它们包含的标记数。由于我们没有使用句子的词汇细节，对齐计算速度很快，因此适用于非常大的文本集合。我们使用这项技术对英法汉莎语料库中的数百万个句子进行了对齐，并在随机选择的1000个句子对中实现了99%以上的准确率，我们用手进行了检查。我们表明，即使没有锚定点的好处，对齐句子长度之间的相关性也足够强，我们应该期望达到96%到97%的准确率。因此，这项技术可能适用于比我们所尝试的更广泛的文本。我们能够在完全忽略测试词汇内容的情况下获得这些结果。"}
{"pid": "J09-3003", "zh_sum": "文章：识别上下文极性：探索短语级情感分析的特征自动情感分析的许多方法都是从大量标记有优先极性（也称为语义方向）的单词开始的。然而，单词出现的特定实例所在短语的上下文极性可能与单词先前的极性大不相同。积极的词语用在表达消极情绪的短语中，反之亦然。此外，通常情况下，上下文之外的积极或消极词语在上下文中是中性的，这意味着它们甚至没有被用来表达情感。这项工作的目标是自动区分先前极性和上下文极性，重点是理解哪些特征对这项任务很重要。由于该问题的一个重要方面是识别极性术语何时在中性上下文中使用，因此将评估用于区分中性和极性实例的特征，以及用于区分正负上下文极性的特征。评估包括评估多个机器学习算法的特征性能。对于除一种学习算法外的所有学习算法，将所有功能组合在一起可获得最佳性能。评估的另一个方面考虑中性实例的存在如何影响区分正负极性的特征的性能。这些实验表明，中性实例的存在极大地降低了这些特性的性能，而提高所有极性类性能的最佳方法可能是提高系统识别实例何时为中性的能力。我们探讨了先前极性和上下文极性之间的区别：在上下文中失去极性的单词，或其极性由于上下文而颠倒的单词。"}
{"pid": "W97-0703", "zh_sum": "使用词汇链进行文本摘要，我们研究了一种生成原始文本摘要的技术，该技术不需要对原始文本进行完整的语义解释，而是依赖于从词汇链派生的文本主题进展模型。我们提出了一种新的算法来计算文本中的词汇链，合并了几个强大的知识源：WordNet同义词库、用于识别名词组的词性标记器和浅层解析器，以及源自（Hearst，1994）的分割算法。摘要过程分为三个步骤：首先分割原始文本，构建词汇链，识别强链，并从文本中提取重要句子。本文给出了识别强链和重要句子的实证结果。我们发现，使用词性标记可以消除单词的错误包含，例如read，它在WordNet中既有名词条目，也有动词条目。在自动文本摘要中，为了避免摘要中的冗余内容，采用同义词来识别重复信息。衔接是通过在文本中使用语义相关的术语、指称、椭圆和连词来实现的。"}
{"pid": "C88-2147", "zh_sum": "基于特征结构的树邻接文法我们在基于特征结构的统一系统中嵌入了树邻接文法（TAG）。由此产生的系统，基于特征结构的树邻接语法（FTAG），捕获了因子依赖和递归的原理，这是标记的基础。我们表明，与标记形式主义相比，FTAG具有增强的描述能力。我们考虑了这个系统的一些限制版本和一些可能的语言规定。我们简要描述了一种表示该系统使用的结构的微积分，它扩展了Rounds的工作，Kasper【Rounds et al.1986，Kasper et al.1986】涉及特征结构的逻辑公式。基于特征的标记由一组（辅助或初始）基本树和两个树组合操作组成：替换和附加。"}
{"pid": "P92-1032", "zh_sum": "估计词义消歧程序性能的上限和下限我们最近报告了两个新的词义消歧系统，一个是双语材料（加拿大汉莎词典）培训系统，另一个是单语材料培训系统（罗格特的同义词库和格罗里尔的百科全书）。在使用单语和双语分类器几个月后，我们确信其性能非常好。然而，我们确实希望能够作出更有力的声明，因此，我们决定尝试制定一些更客观的评估措施。尽管有大量关于语义消歧的文献，但这些文献并没有为我们如何确定拟议解决方案（如前一段提到的两种系统）的成败提供太多指导。许多论文完全避免量化评估，因为很难对绩效做出可信的估计。本文将试图确定评估中预期的绩效水平的上限和下限。通过测量忽略上下文并在所有情况下简单地分配最可能的意义的基线系统产生的性能，可以获得75%的下界估计值（模糊类型的平均值）。通过假设我们衡量绩效的能力在很大程度上受到我们从人类告密者那里获得可靠判断的能力的限制，可以获得上界的估计值。毫不奇怪，上限很大程度上取决于给法官的指示。例如，乔根森（Jorgensen）怀疑词典编纂者往往过于依赖一个线人的判断，并发现与判断相比存在很大差异（只有68%的人同意），正如她所怀疑的那样。在我们自己的实验中，我们已经着手寻找词义消歧任务，在这些任务中，法官可以经常达成一致，以便我们可以证明他们的表现优于基线系统。在完全不同的情况下，我们发现96.8%的人同意法官。我们认为，任何广泛覆盖的WSD程序都必须能够比最频繁的感知分类器表现得更好，才值得认真考虑。"}
{"pid": "J93-1003", "zh_sum": "意外和巧合统计的精确方法在文本统计分析方面做了大量工作。在文献中报告的某些情况下，使用了不适当的统计方法，结果的统计显著性尚未得到解决。特别是，渐近正态性假设经常被不合理地使用，导致有缺陷的结果。这种正态分布假设限制了分析罕见事件的能力。不幸的是，很少有事件在真实文本中占很大比例。然而，基于似然比检验的更适用方法可用，可以在相对较小的样本中产生良好的结果。这些测试可以有效地实现，并已用于检测复合术语和确定特定领域术语。在某些情况下，这些措施的效果比以前使用的方法要好得多。在传统列联表方法运行良好的情况下，此处描述的似然比测试几乎相同。本文描述了一种基于似然比的度量方法的基础，该方法可应用于文本分析。自我们首次将G对数似然比统计量引入自然语言处理社区以来，G对数似然比统计量已被广泛用于统计自然语言处理中，作为关联强度的度量，尤其是词汇关联。"}
{"pid": "P96-1008", "zh_sum": "自然语言界面的完全统计方法我们提出了一个完全基于训练的统计模型的自然语言界面系统。该系统包括三个处理阶段：句法分析、语义解释和语篇。每个阶段都被建模为一个统计过程。这些模型完全集成，形成了一个端到端的系统，将输入话语映射到意义表示框架中。我们的方法是完全受监督的，并在SQL中生成最终的含义表示。我们计算了在航空旅行语义框架中，一个成分（如Atlanta）填补了一个语义槽（如Destination）的概率。"}
{"pid": "P06-1095", "zh_sum": "时间关系的机器学习本文研究了一种用于自然语言文本中时间排序和锚定事件的机器学习方法。为了解决数据稀疏性问题，我们使用时间推理作为过采样方法来大幅扩展训练数据量，从而在人类注释数据上使用最大熵分类器，链接标记的预测准确率高达93%。与一系列日益复杂的基线相比，这种方法更具优势，这些基线涉及人类直觉规则的扩展。当机器学习方法试图通过特征工程来提高分类精度时，我们引入了一个时态推理组件来大大扩展训练数据。我们使用闭包引入的链接来增加tlink分类器的训练数据量。"}
{"pid": "W12-3102", "zh_sum": "2012年统计机器翻译研讨会的结果本文介绍了WMT12共享任务的结果，其中包括翻译任务、机器翻译评估指标任务和机器翻译质量运行时评估任务。我们对34个团队提交的103个机器翻译系统进行了大规模手动评估。我们使用这些系统的排名来衡量12个评估指标的自动指标与人类对翻译质量的判断之间的关联程度。今年，我们引入了一项新的质量评估任务，并评估了11个团队提交的报告。我们报告了整个WMT12英语到捷克数据集的几个自动度量。"}
{"pid": "J98-2001", "zh_sum": "基于语料库的明确描述使用调查我们展示了书面语篇中明确描述使用的研究结果，旨在评估使用明确描述解释信息注释语料库的可行性。我们进行了两个实验，在这两个实验中，受试者被要求在33篇报纸文章的语料库中对明确描述的使用进行分类，共包含1412条明确描述。我们测量了注释者之间关于指定给明确描述的类的一致性，以及关于指定给那些定义的先行词的一致性，这些定义被注释者归类为与文本中的先行词相关。从语料库注释的角度来看，这项研究最有趣的结果是，我们使用霍金斯和普林斯的分类方案得到的一致性很低（K=0.63）；使用Fraurud提出的简化方案获得了更好的结果（K=0.76），该方案仅包括两类，第一类和第二类。关于前因的协议也不完整。这些发现提出了关于通过将结果与标准化注释进行比较来评估系统以进行明确描述解释的策略的问题。从语言学的角度来看，最有趣的观察是我们语料库中大量的话语新定义（在我们的一个实验中，集合中约50%的定义被归类为话语新定义，30%的定义被归类为回指，18%的定义被归类为联想/桥接）以及似乎不需要完全消除歧义的定义的存在。我们提出了一种注释方案，这是基于语料库的明确描述（DD）使用分析的产物，该分析表明，语料库中超过50%的明确描述是新的或不熟悉的话语。"}
{"pid": "W09-0432", "zh_sum": "基于单语资源的统计机器翻译领域自适应领域自适应最近受到了统计机器翻译的关注，以应对测试条件偏离训练条件时出现的性能下降。其基本思想是，可以利用域内训练数据来适应已开发系统的所有组件。之前的工作表明，通过适应有限的领域内双语数据，性能提高很小。在这里，我们的目标是通过在源语言或目标语言中利用大型但廉价的领域内单语数据来显著提高性能。我们建议通过将单语顺应数据翻译成对应语言来合成一个双语语料库。调查是在最先进的基于短语的系统上进行的，该系统以联合国语料库的西班牙语-英语部分为基础，并根据相应的欧洲语料库数据进行改编。在使用基线翻译领域文本后，对翻译、重新排序和语言模型进行评估。通过在开发集上优化这些模型的插值，BLEU分数在测试集上从22.60%提高到28.10%。为了使用源端单语数据，我们首先使用最佳配置（基线+域内词典+印支语模型）翻译源端单语数据，并为每个源端句子获得1个最佳翻译。我们采用了具有自动翻译的SMT系统，并根据moses使用的单词对齐对翻译和重新排序模型进行了培训。"}
{"pid": "J93-1004", "zh_sum": "一个双语语料库中句子对齐的项目机器翻译（如Brown et al.1990）和双语词典编纂（如Klavans和Tzoukermann 1990）的研究人员最近对双语语料库、文本主体（如加拿大汉莎（议会议事录））感兴趣，这些语料库有多种语言（如法语和英语）。一个有用的步骤是对齐句子，即识别一种语言的句子与另一种语言的句子之间的对应关系。本文将描述一种基于简单的字符长度统计模型的句子对齐方法和程序（align）。该程序利用了一种语言中的长句往往被翻译成另一种语言中的长句，而短句往往被翻译成短句这一事实。根据两个句子（字符）长度的标度差异和差异的方差，将概率分数分配给每个拟议的句子对应关系。该概率得分用于动态规划框架，以找到句子的最大似然对齐。值得注意的是，这样一种简单的方法效果如此之好。评估基于瑞士联合银行（UBS）发布的英语、法语和德语三种语言的经济报告语料库。除了4%的句子外，该方法对所有句子都正确对齐。此外，还可以提取错误率小得多的大型亚晶。通过选择80%对齐的最佳评分，错误率从4%降低到0.7%。英语-法语亚词组的错误多于英语-德语亚词组，这表明错误率将取决于所考虑的语料库；然而，这两种语言都很小，都希望这种方法对许多语言对有用。为了进一步研究双语语料库，加拿大汉莎词典（约9000万单词，一半为英语，一半为法语）的更大样本已与align计划保持一致，并将通过计算语言学协会（ACL/DCI）的数据收集倡议提供。此外，为了促进align程序的复制，附录中提供了align程序较难的核心的详细c代码。我们提出了一种混合方法，基本假设是，一种语言中的长句倾向于翻译成另一种语言中的长句，而短句倾向于翻译成短句。我们提出了一种动态规划算法，用于翻译的句子级对齐，该算法利用了两个事实：翻译句子的长度大致对应于原始句子的长度，而翻译文本中的句子序列大致对应于原始句子的顺序。"}
{"pid": "W07-2002", "zh_sum": "SemEval-2007任务02：评估词义归纳和辨别系统本任务的目标是允许对词义归纳和辨别系统进行比较，并将这些系统与其他监督和基于知识的系统进行比较。总共有6个参与系统。我们重新使用了任务17的SemEval-2007英语词汇样本子任务，并建立了聚类式无监督评估（使用Notes感官作为金标准）和监督评估（使用部分数据集进行映射）。我们对参与任务17词汇样本子任务的系统的结果进行了比较。SENSEVAL-4的词义归纳任务的目的是将100个不同单词（35个名词和65个动词）的27132个实例分为词义或类别。基于图的方法已经被用于词义归纳。"}
{"pid": "P05-1071", "zh_sum": "阿拉伯文标记化词性标记和词性消歧一举，我们提出了一种在一个过程中使用词性分析器对阿拉伯文进行标记化和词性标记（包括词性标记）的方法。我们学习单个形态特征的分类器，以及使用这些分类器从分析器输出的条目中进行选择的方法。我们在90年代中期获得了所有任务的准确率。为了选择最佳的Buckwalter形态学分析器（BAMA）结果，我们只需计算每个候选分析中语言特征集的预测值数量。"}
{"pid": "P98-2180", "zh_sum": "MindNet：从文本中获取和构建语义信息，作为根据两个机器可读词典（MRD）中的定义和例句自动构建的词汇知识库，MindNet体现了几个不同于先前使用MRD的功能。然而，它不仅仅是这个静态资源。MindNet代表了从自然语言文本中获取、构建、访问和利用语义信息的一般方法。MindNet既是一种抽取方法，也是一种不同于词网的词汇本体，因为它是从词典自动创建的，其结构基于此类资源。"}
{"pid": "P96-1006", "zh_sum": "整合多个知识源进行词义消歧：基于范例的方法本文提出了一种基于范例学习算法的词义消歧新方法。该方法整合了多种知识源来消除词义歧义，包括相邻词的词性、形态、无序的周围词集、局部搭配和动宾句法关系。我们在之前工作中使用的一个公共数据集上，以及在我们单独构建的一个大型语义标记语料库上测试了我们的WSD程序LEXAS。LEXAS在公共数据集上实现了更高的准确性，并且在使用WoRDNET精练词义标记的大型语料库中，对于高度歧义的单词，其表现优于最常见的启发式方法。我们获得了87%的名词兴趣总体准确率，并发现当我们的特征集仅包含共现特征时，准确率仅下降到80%。我们得出结论，对于WSD来说，搭配信息比句法信息更重要。我们的DSO语料库关注191个常见多义词（名词和动词），每个词包含约1000个句子。"}
{"pid": "P07-1125", "zh_sum": "弱监督学习用于模糊限制语分类在科学文献中，我们使用弱监督机器学习研究生物医学文本中推测语言（“模糊限制语”）的自动分类。我们的贡献包括对任务的精确描述和注释指导、分析和讨论、概率弱监督学习模型以及对所提出方法的实验评估。我们证明了使用弱监督ML进行套期保值分类是可行的，并指出了未来研究的方向。我们使用单个单词作为输入特征，以便将生物文章中的句子分为推测性或非推测性。我们扩展了Light等人（2004）的工作，改进了他们的注释指南，并创建了一个用于推测性句子分类的公共可用数据集（FlyBase数据集）。我们发现，我们的模型无法识别知识匮乏的断言性陈述，这些陈述通常在句法上而不是词汇上进行标记。"}
{"pid": "P08-1102", "zh_sum": "基于级联线性模型的汉语分词与词性标注联合研究提出了一种基于级联线性模型的汉语分词与词性标注联合研究方法。级联模型以基于字符的感知器为核心，结合语言模型等实值特征，能够有效地利用不方便直接并入感知器的知识源。实验表明，该级联模型在单纯分词、联合分词和词性标注方面都取得了较好的精度。在Penn Chinese Treebank 5.0上，我们在仅感知机基线上的切分和联合切分以及词性标注的错误率分别降低了18.5%和12%。对于CTB-5，我们将Duan等人（2007）的拆分称为CTB-5d，将Jiang等人（2008）的拆分称为CTB-5j。"}
{"pid": "C04-1059", "zh_sum": "基于结构化查询模型的统计机器翻译语言模型自适应我们探索了用于统计机器翻译的无监督语言模型自适应技术。机器翻译输出的假设被转换为不同表示能力级别的查询，并用于从非常大的单语文本集合中提取相似的句子。然后根据检索到的数据构建特定的语言模型，并使用通用背景模型进行插值。实验表明，当使用这些适应的语言模型进行翻译时，有显著的改进。我们在语言模型调整中采用了一种稍微不同的句子级策略，首先生成一个带有基线系统的nbest列表，然后在单语目标语言语料库中找到相似的句子。我们使用机器翻译输出作为查询，构建特定的语言模型，从大型单语语料库中提取相似的句子。我们将最初的SMT假设转换为查询，并从一个大型单语集合中检索到类似的句子。"}
{"pid": "P83-1020", "zh_sum": "D理论：谈论树语言学家，包括计算语言学家，一直喜欢谈论树。在这篇文章中，我们概述了一个关于谈论树的语言结构理论；我们称之为描述理论（D理论）。虽然在D理论的全貌出现之前（以及在我们能够构建利用它的程序之前），必须解决一些重要问题，但我们相信，该理论最终将提供一个框架，以一种内在计算性的方式解释自然语言的语法和语义。本文将主要关注这一理论的一组动机，即在确定性句法分析框架内处理某些句法现象所产生的动机。我们的D理论模型非常强大，因为它允许将节点最右边的子节点降低到兄弟节点之下。"}
{"pid": "N12-1052", "zh_sum": "跨语言词簇用于语言结构的直接转换已有研究表明，结合来自大型未标记语料库的词簇特征可以显著提高语言结构的预测能力。虽然之前的工作主要集中在英语上，但我们从两个方面将这些结果扩展到了其他语言。首先，我们证明了这些结果适用于跨家族的许多语言。其次，更有趣的是，我们提供了一种归纳跨语言聚类的算法，并且我们表明从这些聚类中提取的特征显著提高了跨语言结构预测的准确性。具体而言，我们表明，通过增加具有跨语言聚类特征的直接迁移系统，在英语树库上训练并迁移到外语的去西化依赖解析器的相对误差可以减少高达13%。当将相同的方法应用于命名实体识别器的直接传输时，我们观察到相对改进高达26%。"}
{"pid": "I05-2038", "zh_sum": "为优化生物文本挖掘的自然语言处理（NLP）工具，构建了基于生物医学领域文本的GENIA语料库语言注释语料库的语法注释。随着信息提取的重点从命名实体等“名义”信息转移到实体功能和交互等“口头”信息，解析器的应用已成为关键技术之一，因此对句子句法结构标注语料库的需求也越来越大。基于Penn Treebank II（PTB）方案，以基于XML的格式对由500篇MEDLINE摘要组成的GENIA语料库的一个子集进行了语法结构注释。注释者之间的一致性测试表明，研究摘要的写作风格而非内容是树注释困难的根源，而且，在没有太多生物学知识的情况下，语言学家可以稳定地进行注释，并针对科学文本中的语言现象提供适当的指导。我们的GENIA树库语料库估计没有祈使句，只有七个疑问句。"}
{"pid": "P05-1047", "zh_sum": "IE模式归纳的语义方法本文提出了一种获取信息抽取模式的新算法。该方法假设有用的模式将具有与已确定为相关的模式相似的含义。使用标准向量空间模型的变体对模式进行比较，其中使用来自本体的信息来捕获语义相似度。评估表明，与以前报道的以文档为中心的方法相比，该算法性能良好。我们提出了一种弱监督的句子过滤方法，该方法使用语义相似度和自举来获取IE模式。我们使用主谓宾三元组作为特征。"}
{"pid": "P98-2173", "zh_sum": "使用反馈文本进行多语种创作尝试自动化多语种文档的制作有明显的原因，特别是对于受限领域中的常规主题（如技术说明）。已经采用了两种方法：源文本的机器翻译（MT）和知识库中的多语言自然语言生成（M-NLG）。对于机器翻译来说，信息提取是一个主要的难点，因为必须通过对源文本的分析来获得意义；M-NLG避免了这一困难，但乍一看似乎需要一个昂贵的知识工程阶段来编码含义。本文介绍了一种在知识编辑阶段使用M-NLG的新技术。由可能不完整的知识库生成的“反馈文本”用自然语言描述了迄今为止编码的知识以及扩展它的选项。这种方法允许任何说一种受支持语言的人用所有语言生成文本，只需要作者在主题方面的专业知识，而不需要知识工程方面的专业知识。我们提出所见即所得（WYSIWYM）作为一种通过直接操纵自然语言文本中呈现的结构来创作语义信息的方法。在这个系统中，逻辑形式以交互方式输入，表达式的相应语言实现以多种语言生成。"}
{"pid": "P87-1022", "zh_sum": "代词的中心化方法本文提出了一种形式化的中心化方法来建模语篇中的注意结构，并将其作为跟踪语篇上下文和绑定代词的算法的基础。如[GJW86]所述，将注意力集中在语篇实体上的过程会产生持续、保留和转移的句间过渡状态。我们建议对这些状态进行扩展，以处理多个歧义代词的其他情况。该算法已在作为数据库查询应用程序接口的HPSG自然语言系统中实现。我们的定心算法扩展了定心过渡关系的概念，这种关系在相邻的话语中保持不变，以区分移位类型。据预测，最常见的过渡态分类在连续、保留、平滑移位和粗糙移位的顺序上越来越不一致。度量M.BFP使用4元组上的词典排序来确定过渡状态。核心的中心化方法只处理最后一句话。"}
{"pid": "J90-2002", "zh_sum": "机器翻译的统计方法本文提出了一种机器翻译的统计方法。我们描述了我们的方法在法语到英语翻译中的应用，并给出了初步结果。我们直接从大型平行双语文本语料库中估计词与词之间的对应关系和词的重新排序模型的参数。"}
{"pid": "W03-1011", "zh_sum": "基于精确性和查全率的概念，我们提出了一个分布相似性的通用框架。该框架内的不同参数设置近似于现有的不同相似性度量，以及迄今为止尚未探索的更多相似性度量。我们证明，对于高频和低频名词的两个评估任务，最佳参数设置优于现有的两个最先进的相似性度量。我们提出了一个分布相似性的一般框架，该框架由精确性和召回率的概念组成。"}
{"pid": "P09-1077", "zh_sum": "文本中内隐语篇关系的自动意义预测我们进行了一系列实验，以自动识别内隐语篇关系的意义，即没有标记语篇连接词如“but”或“because”的关系。我们使用报纸文本中的隐式关系语料库，并在代表感官自然分布的测试集上报告结果。我们使用了几种语言信息特征，包括极性标记、Levin动词类、动词短语长度、情态、上下文和词汇特征。此外，我们回顾了过去使用未注释文本中的词汇对作为特征的方法，解释了它们的一些缺点，并提出了修改建议。与数据密集型方法相比，我们的最佳功能组合的性能比基线高出4%，意外情况下的性能比基线高出16%。虽然在比较关系上，只有轻度改善（+1.07%），但我们的两个最佳系统的f分数都比年最先进的系统提高了约10%（Pitler et al，2009a）。我们的分析表明，排名靠前的词对（按信息增益排序）都包含常见的虚词，而不是想象中的语义相关的内容词。"}
{"pid": "P10-2041", "zh_sum": "语言模型训练数据的智能选择我们解决了选择非特定领域的语言模型训练数据来构建辅助语言模型以用于机器翻译等任务的问题。我们的方法是根据特定领域和非特定领域的语言模型，对用于生成后一种语言模型的文本源的每个句子进行交叉熵比较。我们表明，与随机数据选择和其他两种先前提出的方法相比，这种方法可以生成更好的语言模型，只需较少的数据即可进行训练。在交叉熵差分选择中，句子的分数是域内交叉熵减去背景交叉熵。这项技术已被用于用新闻专线数据（34亿字）补充欧洲议会文本（4800万字）。"}
{"pid": "J05-3002", "zh_sum": "多文档新闻摘要的句子融合系统可以生成信息丰富的摘要，突出显示许多在线文档中的常见信息，这将帮助Web用户在不进行广泛阅读的情况下精确定位所需信息。在本文中，我们介绍了句子融合，这是一种新的文本到文本生成技术，用于合成跨文档的公共信息。句子融合包括自下而上的局部多序列对齐，以识别传递相似信息的短语，以及统计生成，以将常见短语组合成一个句子。句子融合将摘要领域从使用纯粹的抽取方法转移到生成包含任何输入文档中都找不到的句子的摘要，并且可以跨源合成信息。我们用依赖树表示输入，对齐一些单词将输入树合并到一个晶格中，然后提取一个连接的依赖树作为输出。我们介绍了将多个句子转换为单个摘要句子的问题。"}
{"pid": "C02-1139", "zh_sum": "识别回指和非回指名词短语以提高共指消解我们提出了一种监督学习方法来识别回指和非回指名词短语，以及如何将这些信息纳入共指消解系统。结果系统在相应的MUC共指数据集上优于最佳的MUC-6和MUC-7共指解析系统，分别获得66.2和64.0的F测度。除了共指模型外，我们还训练了一个单独的回指分类器。我们质疑加入此类探测器的动机，报告没有改进，甚至性能更差。"}
{"pid": "W03-1812", "zh_sum": "多词表达可分解性的实证模型本文提出了一种基于潜在语义分析的多词表达可分解性具体模型的构建方法。我们使用潜在语义分析来确定多词表达式与其组成词之间的相似度，并声称相似度越高表示分解性越强。我们在英语名-名复合词和动词助词上测试了该模型，并评估了其与WordNet中相似性和上下义值的相关性。基于基于相似度排序的数据分区上的平均下义关系，我们提供了计算出的相似度与WordNet的语义关系内容相关的证据。我们研究短语的向量提取，因为他们对多词表达式的可分解性感兴趣。我们提出了一个基于LSA的MWE可分解性度量模型，通过检查MWE与组成词之间的相似度，相似度越高表示MWE的可分解性越强。"}
{"pid": "E99-1023", "zh_sum": "表示文本块在词块中划分句子是句法分析、信息提取和信息检索的一个有用的预处理步骤。（Ramshaw和Marcus，1995）通过将数据表示转换为标记任务，为分块引入了一种“方便”的数据表示。在这篇文章中，我们将研究七种不同的数据表示方法来识别名词短语组块。我们将展示数据表示的选择对组块性能的影响很小。然而，配备了最合适的数据表示，我们基于记忆的学习分块器能够改善标准数据集的最佳发布分块结果。我们详细描述了IOB方案。"}
{"pid": "N03-1022", "zh_sum": "COGEX：一种用于问答的逻辑证明器最近的TREC结果表明需要更深入的文本理解方法。本文介绍了自动推理应用于问答系统的思想，并说明了将逻辑证明器集成到问答系统中的可行性。方法是将问答段落转换为逻辑表示。世界知识公理和语言公理都被提供给了证明者，从而加深了对问题文本和答案文本之间关系的理解。此外，证据的痕迹提供了答案证明。结果表明，验证器将质量保证系统在TREC问题上的性能提高了30%。COGEX使用其逻辑证明器提取问题与其候选答案之间的词汇关系。"}
{"pid": "P02-1001", "zh_sum": "概率有限状态传感器的参数估计加权有限状态传感器缺乏训练算法。对于通过有限状态操作（如合成、最小化、并集、串联和闭合）组装的传感器，培训更加困难，因为这会产生棘手的参数绑定。我们制定了一个“参数化FST”范式，并为其提供了训练算法，包括一个通用的簿记技巧（“期望半环”），它可以干净有效地计算期望和梯度。我们使用有限状态操作，例如合成，在将结果传递给前向-后向算法之前，将权重完全组合在期望半环内。我们声称，在期望半环下的解析等价于PCFGs的内外算法。本文给出了概率有限状态传感器参数估计的一般EM算法。我们描述了参数学习的期望半环。"}
{"pid": "N07-1018", "zh_sum": "基于马尔可夫链蒙特卡罗的概率上下文无关文法贝叶斯推断本文提出了两种马尔可夫链蒙特卡罗（MCMC）算法，用于从终端字符串对概率上下文无关文法（PCFG）进行贝叶斯推断，为使用内外算法进行最大似然估计提供了一种替代方法。我们通过估计描述班图语塞索托形态的稀疏语法来说明这些方法，证明在最大似然方法（如内外算法）仅产生平凡语法的情况下，使用适当的先验贝叶斯技术可以推断语言结构。我们描述了用于PCFG规则概率贝叶斯推断的Gibbs采样器。我们引入了适配器语法，这是一种单语语法形式主义，其中非终端在一个步骤中重写为一个完整的子树。"}
{"pid": "P98-2127", "zh_sum": "自动检索和聚类相似单词是自然语言学习面临的最大挑战之一。我们首先根据单词的分布模式定义了一个单词相似度度量。相似性度量允许我们使用已解析的语料库构建同义词库。然后，我们为自动构建的同义词库提出了一种新的评估方法。评估结果表明，这些叙词表比Roget叙词表更接近WordNet。我们使用依赖关系作为单词特征来计算大型语料库中的单词相似度。"}
{"pid": "P06-1004", "zh_sum": "口语演讲切分的最小割模型我们考虑了无监督演讲切分的任务。我们将分割形式化为一个图分割任务，该任务优化了归一化割准则。我们的方法超越了本地化的比较，考虑了长期的内聚依赖性。我们的结果表明，全局分析提高了分割精度，并且在存在语音识别错误的情况下具有鲁棒性。我们根据句子间余弦相似度的变化优化了归一化最小割准则。我们的问题是在课程讲课的成绩单中找到话题的界限。我们创建了一个由四个注释器分割的课程讲座语料库，注意注释器在不同的粒度级别上进行操作。"}
{"pid": "D11-1125", "zh_sum": "调整为排名我们提供了一种简单、有效和可扩展的方法，用于基于成对排序方法的统计机器翻译参数调整（Herbrich et al.，1999）。与流行的MERT算法（Och，2003）不同，我们的成对排序优化（PRO）方法不仅限于少数参数，而且可以轻松处理具有数千个特征的系统。此外，与最近建立在Crammer和Singer（2003）的MIRA算法基础上的方法不同（Watanabe et al.，2007；Chiang et al.，2008b），PRO很容易实现。它使用现成的线性二元分类器软件，可以在几个小时内构建在现有MERT框架之上。我们通过将PRO与MERT和MIRA进行比较，建立了PRO的可扩展性和有效性，并使用大规模数据场景在各种语言对中演示了基于短语和基于语法的系统的对等性。PRO将调整问题转换为成对翻译候选之间的排序问题。我们在n个最佳列表中优化排名，但以在线方式学习参数。我们最小化从合并的n-Best中抽样的逻辑损失，并使用BLEU语句确定排名。"}
{"pid": "P09-2004", "zh_sum": "在语篇中，使用句法来消除显式话语连接词的歧义话语连接词是诸如once、since等词或短语，相反，它们明确表示存在话语关系。在语篇处理过程中，有两种类型的歧义需要解决。首先，一个词在话语或非话语用法之间可能存在歧义。例如，once可以是一个时间话语连接词，也可以是一个意思为“以前”的简单单词。其次，一些连接词在所标记的关系方面存在歧义。例如，因为可以用作时间连接词或因果连接词。我们证明了句法特征可以提高两种消歧任务的性能。我们报告了识别话语与非话语使用的最新研究结果，以及人类在语义消歧方面的表现。我们表明，如果存在显式标记，关系的类别可以通过f分数高于90%来消除歧义。然而，预测内隐语篇关系的类别要困难得多。"}
{"pid": "N03-2002", "zh_sum": "分解语言模型和广义并行退避我们介绍了分解语言模型（FLM）和广义并行退避（GPB）。FLM将单词表示为特征束（例如，形态类、词干、数据驱动的聚类等），并归纳出一个概率模型，该模型覆盖束序列而不仅仅是单词。GPB将标准回退扩展到一般条件概率表，其中变量可能是异构类型，不存在明显的自然（暂时）回退顺序，并且允许使用多个动态回退策略。这些方法在JHU 2002研讨会期间实施，作为SRI语言建模工具包的扩展。本文提供了CallHome阿拉伯语和Penn Treebank Wall Street Journal文章的初步困惑结果。值得注意的是，带有GPB的FLM可以生成复杂度显著较低的二元图，有时甚至低于高度优化的基线三元图。在多遍语音识别上下文中，使用二元图创建第一遍二元图格或N-最佳列表，这些结果具有高度相关性。我们表明，因子语言模型在复杂性方面优于标准的n-gram技术。分解语言模型（FLM）基于单词作为特征向量的表示，可以在一个统一的原则框架内利用除单词之外的各种附加信息源，如词性（POS）信息、形态信息或语义特征。"}
{"pid": "W06-3808", "zh_sum": "星星不多看星星：基于图的半监督学习用于情感分类我们提出了一种基于图的半监督学习算法来解决评级推理的情感分析任务。给定一组文档（如电影评论）和相应的评分（如“四星”），该任务要求根据文本表达的感知情绪推断未标记文档的数字评分。特别是，我们对标记数据稀缺的情况感兴趣。我们将此任务置于半监督环境中，并证明在学习过程中考虑未标记的评论可以提高评分推理性能。我们通过在标记和未标记的数据上创建一个图来对该任务的某些假设进行编码。然后，我们解决一个优化问题，以获得整个图上的平滑评级函数。当只有有限的标记数据可用时，与在训练期间忽略未标记示例的其他方法相比，该方法实现了显著更好的预测精度。我们采用基于半监督图的方法进行情感分析，但不以标记特征的形式纳入词汇先验知识。我们提出了一种半监督学习方法来解决标记训练数据稀缺情况下的评分推理问题。"}
{"pid": "N04-1030", "zh_sum": "基于支持向量机的浅层语义分析在本文中，我们提出了一种用于浅层语义分析的机器学习算法，扩展了Gildea和Jurafsky（2002）、Surdeanu等人（2003）等的工作。我们的算法基于支持向量机，与早期的分类器相比，支持向量机的性能得到了改善。我们通过大量新特性展示了性能改进，并衡量了它们推广到从AQUAINT语料库中提取的新测试集的能力。我们首先构建一棵解析树，然后通过将从解析树中提取的手工构建的特征输入机器学习系统来标记句法成分。"}
{"pid": "P02-1006", "zh_sum": "为问答系统学习表面文本模式本文探讨了表面文本模式在开放领域问答系统中的作用。为了获得一组最佳模式，我们开发了一种自动学习此类模式的方法。通过向Altavista提供一些手工制作的每种问题类型的示例，可以在引导过程中从Internet构建带标签的语料库。然后从返回的文档中自动提取模式并进行标准化。我们计算每个模式的精度，以及每个问题类型的平均精度。然后应用这些模式来找到新问题的答案。使用TREC-10问题集，我们报告了两个案例的结果：来自TREC-10语料库和网络的答案。我们提出了一种用于类型偏好的替代本体，并描述了一种使用该替代本体使用表面文本模式提取特定答案的方法。"}
{"pid": "P05-1013", "zh_sum": "伪投影依赖分析为了充分发挥基于依赖的句法分析的潜力，需要允许非投影依赖结构。我们展示了一个数据驱动的确定性依赖解析器如何将其自身局限于投影结构，与图转换技术相结合来生成非投影结构。使用布拉格依赖树库的数据进行的实验表明，组合系统可以处理非投射结构，其精度足以显著提高整体解析精度。这使得捷克语的鲁棒非投影分析具有最佳的报告性能。在我们的伪射影方法中，非射影链接在树中向上提升以实现射影性，并且使用特殊的跟踪标签在解析时恢复非射影链接。我们展示了如何通过使用图变换技术来预处理训练数据和后处理解析器输出，即所谓的伪投影解析，来解除对投影依赖图的限制。为了处理非投影关系，我们建议对依赖关系解析器应用预处理步骤，即重复将非投影弧提升到其头部，直到树成为伪投影。我们注意到，由于非投影依赖的数量远小于投影依赖的数量，因此对所有情况执行非投影分析是无效的。"}
{"pid": "W97-0302", "zh_sum": "全局阈值和多遍分析我们提出了一种改进的经典波束阈值技术，在相同性能水平下，比传统方法快一个数量级。我们还提出了一种新的阈值技术，即全局阈值技术，它与新的波束阈值技术相结合，提供了两个额外的改进因素，以及一种新的技术，即多遍解析技术，它可以与其他技术相结合，以获得另一个50%的改进。我们使用一种新的搜索算法来同时优化各种算法的阈值参数。我们描述了一种生成标准上下文无关语法的简单但粗糙的近似语法的方法。"}
{"pid": "P02-1039", "zh_sum": "基于语法的统计机器翻译解码器本文描述了一种基于语法的翻译模型的解码算法（Yamada和Knight，2001）。该模型已扩展到包含此处所示的短语翻译。与传统的逐词统计模型不同，基于语法的模型的解码器在给定外语句子的情况下建立了英语解析树。由于在实际环境中模型的大小变得巨大，并且解码器为每个单词对齐考虑多个语法结构，因此需要几种修剪技术。我们在一个汉英翻译系统中测试了我们的解码器，得到了比IBM Model 4更好的结果。我们还讨论了有关此解码器与语言模型之间关系的问题。我们提出了一种基于语法的解码器，该解码器基于对输入句子语法分析树的重新排序操作来限制单词的重新排序。"}
{"pid": "C04-1200", "zh_sum": "确定意见的情感识别情感（意见的情感部分）是一个具有挑战性的问题。我们提出了一个系统，在给定主题的情况下，自动查找对该主题持有意见的人以及每个意见的情绪。该系统包含一个用于确定单词情感的模块和另一个用于在句子中组合情感的模块。我们在单词和句子层次上对情感进行分类和组合的各种模型进行了实验，取得了令人满意的结果。我们试图通过组合句子中的情感词来确定给定句子的最终情感方向。我们从两个积极和消极种子词列表开始。我们使用WordNet同义词和反义词来扩展两个积极和消极种子词列表。"}
{"pid": "D07-1114", "zh_sum": "在意见挖掘中提取方面评价和关系方面意见提取技术允许用户检索和分析散布在Web文档中的人们的意见。我们将意见单位定义为四重单位，包括意见持有人、被评估主体、被评估主体所在的部分或属性以及表示积极或消极评估的评估值。我们使用此定义作为意见提取任务的基础。我们专注于意见提取的两个重要子任务：（a）提取方面评价关系，和（b）提取关系方面，并使用结合上下文和统计线索的方法处理每个任务。我们在日本博客帖子上的实验表明，使用上下文线索可以提高这两项任务的性能。我们分析了标注的意见表达语料库，发现许多意见表达被用于多个领域。我们采用监督学习技术来搜索有用的句法模式作为上下文线索。"}
{"pid": "N07-1029", "zh_sum": "结合多个机器翻译系统的输出目前有几种基于不同范式的机器翻译（MT）方法；e、 例如，短语、层次和基于语法。这三种方法尽管使用了相当不同的语言知识水平，但仍能产生类似的翻译准确性。这样多种系统的可用性使得人们越来越有兴趣通过组合多个系统的输出来找到更好的翻译。本文描述了三种不同的机器翻译系统组合方法。这些组合方法在句子、短语和单词级别上操作，利用来自N个最佳列表、系统分数和目标到源短语对齐的信息。单词级组合提供了最稳健的收益，但通过组合所有三种方法，在开发测试集（NIST MT05和GALE 2006干运行的新闻组部分）上取得了最好的结果。我们使用最小翻译错误率（TER）（Snover et al，2006）对齐来构建混淆网络。我们从输入系统收集源到目标的对应关系，仅使用这些短语创建新的翻译选项表，并重新解码源语句以生成更好的翻译。"}
{"pid": "J99-3001", "zh_sum": "基于信息结构中的功能中心参照连贯考虑到来自自由语序语言（德语）的经验证据，我们建议在中心模型中修改指导前瞻性中心列表中话语实体排序的原则。我们主张语法角色标准应该被反映话语功能信息结构的标准所取代。这些新的标准是基于新旧话语实体之间的区别。我们证明，这种中心化的功能模型可以成功地应用于分析几种形式的指称文本现象，即。代词回指、名词回指和功能回指。两项评估研究证实了我们的方法论和实证主张。在第一个实验中，我们比较了语法角色驱动中心算法和功能中心算法对代词回指消解的成功率。第二项研究涉及一种新的基于成本的评估方法，用于评估中心数据，该方法可以直接从中心模型的认知负荷前提中推导出来并加以证明。我们引入了功能中心理论，这是中心理论的一种变体，它利用了听者新旧实体之间的信息状态差异。"}
{"pid": "P05-1022", "zh_sum": "从粗到精的N-Best解析和MaxEnt判别重排序判别重排序是构造高性能统计解析器的一种方法（Collins，2000）。区分性重排程序需要为每个句子提供候选语法分析的来源。本文描述了一种基于从粗到细的生成解析器（Charniak，2000）构造50个最佳解析器集的简单而新颖的方法。此方法生成的50个最佳列表的质量大大高于以前获得的质量。我们使用这些语法分析作为MaxEnt重排程序的输入（Johnson et al.，1999；Riezler et al.，2002），该程序从每个句子的语法分析集中选择最佳语法分析，对于长度小于等于100的句子，f分数为91.0%。我们使用修剪，即对粗粒度分析林中的边进行修剪，以允许使用细粒度类别进行全面评估。我们展示了在词典化的基础解析器上合成局部树特征的准确性改进。为了提高性能和稳健性，对特征进行修剪，以便选定的特征必须将n-best列表中F1得分最高的分析与F1得分次优的分析至少区分五次。"}
{"pid": "N03-1014", "zh_sum": "为大范围统计分析归纳历史表示我们提出了一种神经网络方法，用于归纳分析历史的表示，并使用这些历史表示来估计统计左角解析器所需的概率。生成的统计解析器在Penn Treebank上实现了性能（89.1%F-measure），尽管使用了较小的词汇量和较少的先验语言知识，但仅比当前用于此任务的最佳解析器低0.6%。这一成功的关键是使用结构确定的软偏差来诱导解析历史的表示，而不使用硬独立假设。在以前使用神经网络解析自然语言的工作中，经验上最成功的是我们使用简单同步网络的工作。我们通过改变选择输入标记词对的频率截止值来测试较大的输入词汇对SSN性能的影响。"}
{"pid": "N04-1001", "zh_sum": "多语言实体检测和跟踪的统计模型实体检测和跟踪是自然语言任务中一个相对较新的补充。在本文中，我们提出了一个与统计语言无关的框架，用于识别和跟踪无限制文本文档中实体的命名、名词和代词引用，并将它们链接到文本中每个逻辑实体对应的集群中。提及检测模型和新的实体跟踪模型都可以使用任意的特征类型，能够集成大量的词汇、句法和语义特征。此外，提及检测模型主要使用来自不同命名实体分类器的特征流。在阿拉伯文、中文和英文文本中进行了多次实验，对所提出的框架进行了评估；基于此处描述的方法并提交给最新的自动内容提取（ACE）评估的系统在所有三种评估语言中都取得了顶级结果。"}
{"pid": "P06-1032", "zh_sum": "使用短语统计机器翻译（SMT）技术纠正ESL错误本文对使用短语统计机器翻译（SMT）技术识别和纠正英语作为第二语言（ESL）学习者的写作错误进行了初步研究。我们以中国学习者错误语料库（CLEC）中发现的大量名词错误为例来指导工程训练集的创建，表明应用SMT范式可以捕捉到为母语人士设计的广泛使用的校对工具没有很好解决的错误。我们的系统能够纠正在万维网上发现的一组自然发生的大量名词错误中61.81%的错误，建议努力收集编辑前和编辑后的ESL写作样本的可对齐语料库，可以开发基于SMT的写作辅助工具，能够修复ESL学习者写作中发现的许多复杂的句法和词汇问题。我们利用短语统计机器翻译（SMT）技术来纠正ESL写作错误，并证明这种数据密集型SMT方法非常有前景，但我们也指出SMT方法依赖于大量训练数据的可用性。"}
{"pid": "W06-2920", "zh_sum": "CoNLL-X多语言依赖解析共享任务每年的计算自然语言学习会议（CoNLL）都有一个共享任务，参与者在完全相同的数据集上训练和测试他们的系统，以便更好地比较系统。第十个CoNLL（CoNLL-X）看到了一个关于多语言依赖项解析的共享任务。在本文中，我们描述了13种语言的树库如何转换为相同的依赖格式，以及如何衡量解析性能。我们还概述了参与者采用的解析方法以及他们取得的结果。最后，我们试图得出关于多语言语法分析的一般结论：是什么使特定语言、树库或注释方案更容易或更难语法分析，以及哪些现象对任何依赖性语法分析器都是挑战性的？CoNLL-X共享任务侧重于多语言依赖项解析。"}
{"pid": "N07-1011", "zh_sum": "共指消解的一阶概率模型传统的名词短语共指消解系统只表示名词短语对的特征。在本文中，我们提出了一种机器学习方法，该方法能够在名词短语集合上实现特征，从而产生一阶概率模型来进行共指。我们概述了一组使这种方法实用的近似方法，并将我们的方法应用于ACE共指数据集，与只考虑名词短语对特征的可比方法相比，误差减少了45%。该结果展示了如何将一阶逻辑表示合并到概率模型中并有效缩放的示例。我们提出了一个系统，该系统使用在线学习方法来训练分类器来判断两个实体是否相互关联。我们引入了一阶概率模型，该模型实现了多个提及集上的特征，从而直接对实体进行操作。"}
{"pid": "P83-1019", "zh_sum": "句法非流利性的确定性分析人们经常注意到，自然使用的自然语言不符合自然语法。自发的语音包含各种各样的错误开始、犹豫和自我纠正，这些都会破坏字符串的良好形式。然而，令人不解的是，尽管人们明显偏离了语法规范，但他们几乎不难理解日常生活中必不可少的非流利语言。更令人不解的是，儿童能否根据一组明显语法和非语法的字符串提供的证据成功地掌握一种语言的语法。我们通过向确定性解析器添加规则来解决更正自修复的问题，该解析器将删除必要的文本。我们根据对确定性解析器的扩展定义了修复类型和相关的纠正策略。"}
{"pid": "P07-1123", "zh_sum": "通过跨语言投射学习多语主观语言本文探讨了利用英语中可用的工具和资源在新语言中生成主观分析资源的方法。如果在英语和选定的目标语言（例如双语词典或平行语料库）之间架起一座桥梁，这些方法可以用来快速创建新语言中的主观性分析工具。我们讨论了基于词典的翻译方案的不同缺点，以实现更面向语义的任务主观分析。相反，我们建议使用一个平行语料库，在源语言中应用分类器，并在目标语言中使用相应的句子来训练一个新的分类器。我们使用双语词典和手动翻译的平行语料库，根据罗马尼亚语的主观性水平生成句子分类器。"}
{"pid": "W05-1513", "zh_sum": "具有线性运行时复杂性的基于分类器的解析器我们提出了一种基于分类器的解析器，它在线性时间内生成组成树。解析器使用基本的自底向上的shift-reduce算法，但使用分类器来确定解析器操作，而不是语法。这可以看作是Nivre和Scholz（2004）的确定性依赖解析器对完整成分解析的扩展。我们表明，如果在分类中使用适当的特征集，一个非常简单的单路径贪婪解析器可以与更复杂的解析器执行相同的精度级别。我们在Penn Treebank《华尔街日报》第23节上评估了我们的解析器，获得的准确率和召回率分别为87.54%和87.61%。我们提出了一种基于选区的句法分析方法来确定句子依赖结构。"}
{"pid": "P92-1005", "zh_sum": "语义解释的单调语义解释方面，如量词范围界定和引用解析，通常通过计算实现，包括信息丢失和语义表示的破坏性操作。本文描述了如何使用修改后的准逻辑形式（QLF）表示进行单调参考解析和范围界定。给出了QLF的语义，在QLF表达式解析时，公式的表示被单调扩展。我们使用准逻辑形式，这是组合语义的一种单调表示。准逻辑形式允许对几种类型的信息进行欠规范，例如回指、省略和语义关系。"}
{"pid": "H05-1012", "zh_sum": "阿拉伯语-英语机器翻译中的最大熵词语对齐算法本文提出了一种基于监督训练数据的阿拉伯语-英语最大熵词语对齐算法。我们证明了为机器翻译中的问题创建训练材料是可行的，并且监督和非监督方法的混合产生了优异的性能。路线中使用的概率模型直接建模链路决策。与传统的单词对齐技术相比有了显著的改进，并且在一些机器翻译测试中也得到了改进。该算法的性能与人工标注性能进行了对比。我们提出了一个区分训练的1-to-N模型，该模型具有专为阿拉伯语设计的特征函数。我们在一个由一万个单词对齐的阿拉伯语-英语句子对组成的语料库上训练了一个判别模型，该模型的性能优于GIZA++基线。"}
{"pid": "E06-1038", "zh_sum": "基于软句法证据的区分性句子压缩我们提出了一个句子压缩模型，该模型使用了一个区分性的大幅度学习框架，结合了一个在压缩的二元图上定义的新特征集，以及辅助依赖和短语结构解析器提供的深层句法表示。解析器在域外进行训练，并且包含大量噪声。我们认为，学习算法的区分性质允许模型学习相对于特征集中任何噪声的权重，以直接优化压缩精度。这与当前最先进的模型（Knight和Marcu，2000）不同，后者在计算模型参数时，将压缩和未压缩句子的有噪解析树视为黄金标准。我们提供了一种类似维特比（Viterbi）的动态规划算法，以无约束形式或具有特定长度约束的方式从格中恢复保序二元图的最高得分序列。我们使用两个解析器（一个基于短语的解析器和一个依赖性解析器）的输出作为区分模型中的特征，该模型分解成对的连续单词。我们使用半马尔可夫模型，该模型允许合并语言模型进行压缩。"}
{"pid": "W04-3103", "zh_sum": "生物科学语言：事实推测和陈述在这两者之间，我们探讨了推测性语言在MEDLINE摘要中的使用。人工注释实验的结果表明，推测句的概念可以被人类可靠地注释。此外，自动化方法的实验也表明，也可以开发可靠的自动化方法。本文还介绍了分布观测结果，并讨论了一个能够识别推测性语言的系统的可能用途。我们专注于介绍问题、探索注释问题和概述潜在的应用，而不是ML方法的特殊性，并在Medline摘要集合上使用手工制作的子串匹配分类器和监督SVM给出了一些结果。我们探讨了在生物医学中注释推测性语言的问题，并概述了潜在的应用。我们提出了一项关于在生物医学文献中注释模糊限制语的研究。"}
{"pid": "W03-1017", "zh_sum": "关于回答意见问题：将事实与意见分离并识别意见句的极性意见问题回答是自然语言处理的一项具有挑战性的任务。在本文中，我们讨论了意见问答系统的一个必要组成部分：从文档和句子两个层面将意见与事实分离。我们提出了一种贝叶斯分类器，用于区分观点占主导地位的文档，如社论和常规新闻故事，并描述了三种无监督的统计技术，用于在句子层面检测观点，这项任务非常困难。我们还提出了第一个模型，根据意见中表达的主要观点，将意见句分为积极或消极。报告了从大量新闻故事收集和对400个句子的人类评估中得出的结果，表明我们在文档分类方面取得了非常高的性能（精确度和召回率高达97%），在检测意见并在句子级别将其分类为积极、消极或中立（精确度高达91%）。在句子层面，我们建议根据固执己见的句子所表达的主要观点，将意见句分为积极或消极两类。"}
{"pid": "J01-3001", "zh_sum": "词义消歧中知识源的相互作用词义消歧（WSD）是一项计算语言学任务，可能得益于人工智能研究中结合不同知识源的传统。探索这一假设的一个重要步骤是确定哪些语言知识源最有用，以及它们的结合是否会带来更好的结果。我们提出了一种使用多个知识源的感知标记器。在我们的评估语料库中，测试准确率超过94%。我们的系统试图消除运行文本中所有内容词的歧义，而不是仅限于处理有限的词汇。有人认为，这种方法更有可能帮助创建实用系统。我们提出了一个分类器组合框架，其中消歧方法（模拟退火、主题代码和选择限制）使用基于TiMBL记忆的方法进行组合（Daelemans et al，1999）。我们使用朗文当代英语词典（LDOCE）作为词义量表。我们使用焦点词本身的词性标记来帮助消除与句法差异相关的语义歧义。我们认为，同时使用句法和词汇特征可以提高排歧的准确性。"}
{"pid": "P83-1021", "zh_sum": "解析为演绎通过探索解析与演绎之间的关系，获得了一种新的、更普遍的图表解析观点，它包括基于统一的语法形式的解析，是确定分句早期演绎证明程序的基础。对于一类有趣的语法，讨论了这种方法的效率。我们扩展了Earley演绎工作，仅通过设置少量参数的值就可以在解析和生成模式中使用它。我们提出了Earley统一语法算法的版本，其中统一是负责属性赋值的唯一操作。"}
{"pid": "J94-2001", "zh_sum": "用概率模型标记英语文本本文介绍了一些使用概率模型标记英语文本的实验，即在句子上下文中为每个单词指定正确的标记（词性）。这些实验的主要创新之处在于在模型训练中使用了未标记的文本。我们使用了一个简单的三类Marlcov模型，并根据提供的训练数据的种类和数量，寻找估计该模型参数的最佳方法。比较并结合了两种方法：使用手动标记的文本并计算相对频率计数，使用没有标记的文本并根据最大似然原则将模型训练为隐马尔可夫过程。实验表明，使用尽可能多的标记文本可以获得最佳的训练效果。他们还表明，最大似然训练（通常用于从训练数据估计隐马尔可夫模型参数的过程）不一定会提高标记精度。事实上，它通常会降低这种准确性，除非只有有限数量的手动标记文本可用。我们试图通过对未标记数据的期望最大化来改进HMM词性标注。我们介绍了使用通过期望最大化训练的二元隐马尔可夫模型（HMM）的标准过程。在词性标注方面，我们介绍了一种他称之为最大似然标注的方法。"}
{"pid": "W03-1028", "zh_sum": "改进的自动关键词提取算法在提供更多语言知识的情况下，本文讨论了使用有监督机器学习算法从摘要中自动提取关键词的实验。本文的主要观点是，通过向表示中添加语言知识（如句法特征），而不是仅依赖统计数据（如术语频率和n-gram），通过专业索引器先前指定的关键字来衡量，可以获得更好的结果。更详细地说，提取NP组块比提取n组块的精度更高，并且通过添加指定给术语的词性标记作为特征，结果得到了显著改善，与所应用的术语选择方法无关。我们提出了一个从摘要中提取关键词的系统，该系统使用具有词汇和句法特征的监督学习，这被证明比以前发表的结果有显著改进。"}
{"pid": "P02-1053", "zh_sum": "竖起大拇指还是竖起大拇指？语义定向在评论无监督分类中的应用本文提出了一种简单的无监督学习算法，用于将评论分类为推荐（竖起大拇指）或不推荐（竖起大拇指）。评论的分类是通过评论中包含形容词或副词的短语的平均语义方向来预测的。当一个短语有良好的联想（如“细微差别”）时，它具有积极的语义方向；当它有不好的联想（如“非常傲慢”）时，它具有消极的语义方向。本文将短语的语义方向计算为给定短语与单词“优秀”之间的互信息减去给定短语与单词“差”之间的互信息。如果一篇评论的词组的平均语义倾向是积极的，则该评论被归类为推荐评论。当对来自ePionions的410条评论进行评估时，该算法的平均准确率达到74%，这些评论来自四个不同的领域（汽车、银行、电影和旅游目的地的评论）。准确率从汽车评论的84%到电影评论的66%不等。我们描述了一种基于观察单词与其他情感已知的单词的共现情况来自动构建词典的方法。"}
{"pid": "P96-1038", "zh_sum": "语调变化与语篇结构关系的语料库研究。我们研究了说话风格（阅读与自发）和话语分割方法（文本单独与文本和言语）对这种关系性质的影响。我们还比较了一个语段中初始、中间和最后话语的声学韵律特征。我们发现，在独白的语篇切分中，语音能够提高注释者之间的一致性。我们介绍了波士顿方向语料库，这是一个公开的语音语料库，带有手动ToBI注释，用于自动韵律标注的实验。"}
{"pid": "P98-2143", "zh_sum": "知识有限的稳健代词消解大多数传统的回指消解方法严重依赖于语言和领域知识。然而，开发基于知识的系统的缺点之一是，这是一项非常劳动密集和耗时的任务。本文提出了一种健壮的、知识贫乏的方法来解析技术手册中的代词，该方法对词性标记器预处理的文本进行操作。根据协议和一些先行指标检查输入。每个指标都会为候选人分配分数，得分最高的候选人将作为先行项返回。评估报告的成功率为89.7%，优于选择用于比较和在相同数据上测试的方法的成功率。此外，初步实验表明，该方法可以在修改最少的情况下成功地适用于其他语言。我们首先应用一组约束来过滤语法不兼容的候选先行词，然后使用显著性因子对其余候选先行词进行排序。我们发现，由于评价数据的差异以及每个回指消解系统所使用的预处理工具的多样性，目前对回指消解算法和系统的评价没有任何可供比较的共同点。"}
{"pid": "D11-1141", "zh_sum": "推特中的命名实体识别：一项实验研究，人们每天推特超过一亿次，产生了一个嘈杂、非正式但有时信息丰富的140个字符的信息集，以前所未有的方式反映了时代精神。标准NLP工具在推特上的性能严重下降。本文通过重新构建NLP管道来解决这个问题，从词性标记开始，通过组块，到命名实体识别。我们的新型T-NER系统与斯坦福大学的NER系统相比，F1成绩翻了一番。T-NER利用tweet中固有的冗余来实现这一性能，使用LabeledLDA利用Freebase字典作为远程监控的来源。LabeledLDA优于联合训练，与十种常见实体类型相比，F1增加了25%。我们的NLP工具可在以下网址获得：http://github。com/aritter/twitter\\nlp我们使用令牌统一表作为功能，包括任何哈希标记，但忽略twitter提及、URL和纯数字令牌。我们的系统利用CRF模型分割命名实体，然后使用基于LabeledLDA的远程监督方法对命名实体进行分类。"}
{"pid": "P99-1016", "zh_sum": "从文本中自动构建一个以超词标记的名词层次结构之前的工作表明，可以使用自动方法来构建语义词典。这项工作更进一步，不仅自动创建相关单词的集群，而且自动创建名词及其超义词的层次结构，类似于WordNet中手工构建的层次结构。我们让三位法官评估下位词层次结构中的十个内部节点，这些节点至少有二十个后代。"}
{"pid": "C04-1051", "zh_sum": "大型释义语料库的无监督构建：利用大规模平行新闻源我们研究了从数千个网络新闻源收集的临时和主题聚集的新闻文章语料库中获取单语句子级释义的无监督技术。采用了两种技术：（1）简单的字符串编辑距离，（2）启发式策略，将同一集群中不同新闻故事的初始（大概是摘要）句子配对。我们使用单词对齐算法和借用机器翻译的度量来评估这两个数据集。结果表明，编辑距离数据比启发式数据更干净、更容易对齐，在类似提取的测试集上，总体对齐错误率（AER）为11.58%。然而，在启发式策略提取的测试数据上，两个训练集的性能相似，AER分别为13.2%和14.7%。对每组100对句子的分析表明，编辑距离数据缺乏许多复杂的词汇和句法变化，这些变化是单语释义的特征。摘要句虽然不太容易对齐，但更多地保留了学习释义关系时最感兴趣的非琐碎的交替。我们介绍微软研究释义语料库（MSRPC）。我们使用网络聚合新闻故事来学习句子级和单词级对齐。"}
{"pid": "E06-1031", "zh_sum": "CDER：使用块移动的高效机器翻译评估大多数最先进的机器翻译评估方法都会给词块的移动带来高成本。在许多情况下，尽管这样的动作仍然会产生正确或几乎正确的句子。在本文中，我们将提出一种新的评估方法，该方法将块重新排序显式建模为一种编辑操作。我们的度量可以在二次时间内精确计算。我们考虑了单词替换和重新排序的编辑距离。我们的CDER度量基于编辑距离，例如众所周知的WER，但允许对块进行重新排序。"}
{"pid": "J93-1005", "zh_sum": "结构歧义和词汇关系我们提出，许多歧义介词短语附件可以根据介词与动词和名词词头的关联相对强度来解决，这是根据自动解析语料库中的分布情况来估计的。这表明，分布式方法可以提供解析问题的近似解决方案，在最坏的情况下，解析问题需要复杂的推理。我们是第一个证明基于语料库的PP依恋歧义消解方法可以获得良好的结果。我们提出了一种最早的基于语料库的方法，通过计算动词和名词与介词的共现频率（词汇关联），利用词汇偏好来处理介词短语附着。我们使用部分解析器从语料库中提取（v，n，p）元组，其中p是动词v和名词n之间连接不明确的介词。"}
{"pid": "W06-3601", "zh_sum": "具有扩展局部域的语法定向翻译器语法定向翻译器首先将源语言输入解析为解析树，然后递归地将树转换为目标语言中的字符串。我们通过在源端具有多级树的扩展树到字符串转换器来模拟这种转换，这使我们的系统具有更大的表达能力和灵活性。我们还定义了一个直接概率模型，并使用线性时间动态规划算法来搜索最佳导数。然后将该模型扩展到一般对数线性框架，以便重新存储其他特性，如n-gram语言模型。我们设计了一个简单而有效的算法来生成n-gram重排序的非重复k-best翻译。给出了初步的英汉翻译实验结果。我们研究了一种基于TSG的树串对齐模型。我们定义了扩展树串转换器。"}
{"pid": "W96-0213", "zh_sum": "词性标注的最大熵模型本文提出了一种统计模型，该模型从标注了词性标注的语料库中进行训练，并将其分配给以前未看到的文本，准确率达到96.6%。该模型可以归类为最大熵模型，同时使用许多上下文“特征”来预测词性标签。此外，本文还演示了如何使用专门的特征来建模困难的标记决策，讨论了在实现这些特征过程中发现的语料库一致性问题，并提出了一种缓解这些问题的训练策略。我们假设一个单词的标记与前面两个单词的标记无关。我们发布了一个公开的最大熵标记器。"}
{"pid": "P95-1021", "zh_sum": "D-Tree语法DTG旨在共享标记的一些优点，同时克服其一些局限性。DTG涉及两种组合操作，称为子分割和姐妹附加。DTG最显著的特点是，与标记不同，两个DTG操作与词汇项的关联方式完全一致：子部分总是对应于补语，姐妹附加语总是对应于修饰语。此外，DTG与TAG不同，它可以为英语和克什米尔语中的em-wh运动提供统一的分析，尽管克什米尔语中的wh元素出现在句子的第二位置，而不是英语中的句子初始位置。在正确建模依赖关系的过程中，我们扩展了较弱的生成能力，从而导致更大的解析复杂性。"}
{"pid": "P09-1040", "zh_sum": "期望线性时间下的非投影依赖分析我们提出了一种新的依赖分析转换系统，该系统只在相邻单词之间构建弧，但可以通过交换输入中单词的顺序来解析任意非投影树。在最坏的情况下，添加交换操作会将确定性解析的时间复杂性从线性更改为二次，但基于树库数据的经验估计表明，对于语料库中证明的数据范围，预期的运行时间实际上是线性的。对来自五种语言的数据的评估显示了最先进的准确性，特别是对标记的精确匹配分数的评估结果。我们提出了投影堆栈算法。"}
{"pid": "P06-1067", "zh_sum": "统计机器翻译的失真模型在本文中，我们认为n-gram语言模型不足以解决机器翻译所需的单词重新排序问题。我们提出了一种新的失真模型，可以与现有的基于短语的SMT解码器一起使用，以解决这些n-gram语言模型的局限性。我们给出了阿拉伯语到英语机器翻译的实证结果，当使用我们提出的模型时，这些结果显示出统计上的显著改进。我们还提出了一种基于词对齐的新度量方法来度量任意两种语言之间的词序相似性（或差异）。我们的词汇化失真模型预测从最后一个翻译的单词到下一个翻译的单词的跳跃，每个可能的跳跃长度都有一个类。我们发现确定性字重排序超出了优化范围，解码器无法撤消。"}
{"pid": "P87-1015", "zh_sum": "描述由各种语法形式主义产生的结构描述，我们根据路径的复杂性以及每个系统可以生成的结构描述集中路径之间的关系来考虑由各种语法形式主义产生的结构描述。在考虑形式主义之间的关系时，我们表明，从形式主义的细节中抽象出来，并检查其派生树的属性所反映的派生过程的性质是有用的。我们发现，所考虑的几种形式主义可以被视为密切相关，因为它们具有与上下文无关语法生成的派生树集具有相同结构的派生树集。在此基础上，我们描述了一类形式主义，我们称之为线性上下文无关重写系统，并表明它们在多项式时间内是可识别的，并且只生成半线性语言。我们介绍了线性上下文无关重写系统（LCFRS），它是一种轻度上下文敏感的形式主义，允许派生字符串元组，即不连续短语。"}
{"pid": "P91-1023", "zh_sum": "一个双语语料库中句子对齐的项目机器翻译（例如Brown等人，1990）和双语词典编纂（例如Klavans和Tzoukermann，1990）的研究人员最近对研究平行文本感兴趣，例如加拿大汉撒（议会议事录）等文本，这些文本有多种语言（法语和英语）。本文描述了一种基于简单的字符长度统计模型在这些平行文本中对齐句子的方法。该方法是在瑞士经济报告的一小部分三种语言样本上开发和测试的。一个更大的9000万字加拿大汉撒文样本已经整理好并捐赠给了ACL/DCI。我们提取成对的锚定词，如数字、专有名词（组织、人物、头衔）、日期和货币信息。我们发现目标句与源句的字节长度比是正态分布的。我们证明了一种全局对齐动态规划算法的有效性，该算法的基本相似性分数基于以字符为单位的句子长度差异。"}
{"pid": "W96-0214", "zh_sum": "解析DOP模型的有效算法在自然语言文本的面向数据的解析（DOP）中取得了良好的结果（Bod，1993c）。不幸的是，现有算法计算量大，难以实现。由于两个因素，以前的算法代价高昂：必须生成的规则的指数数量和蒙特卡罗解析算法的使用。在本文中，我们通过将DOP模型简化为一个小的、等价的概率上下文无关文法来解决第一个问题。我们通过一种新的确定性解析策略来解决第二个问题，该策略最大化了正确成分的预期数量，而不是正确解析树的概率。通过优化，实验得出了97%的交叉括号率和88%的零交叉括号率。这与Bod报告的结果有显著差异，与Pereira和Schabes（1992）在相同数据上的重复实验结果具有可比性。我们表明，Bod的结果至少部分是由于选择了非常偶然的测试数据，部分是由于使用了比其他研究人员更干净的数据。我们给出了DOP模型到等效PCFG的多项式时间转换，其大小在训练集的大小上是线性的。"}
{"pid": "P95-1026", "zh_sum": "无监督词义消歧与有监督方法本文提出了一种用于词义消歧的无监督学习算法，该算法在未标注的英语文本上进行训练时，其性能与需要耗时手工标注的有监督技术相当。该算法基于两个强大的约束条件，即单词往往每个话语有一个意义，每个搭配有一个意义，并在迭代引导过程中加以利用。测试精度超过96%。我们引入了感知一致性的概念，并将其扩展到相关文档中的操作符。我们提出了一种半监督的自训练算法，用于词义消歧。"}
{"pid": "P02-1062", "zh_sum": "命名实体提取的排序算法：Boosting和投票感知器本文描述了从最大熵标记器中重新排列前N个假设的算法，其应用是在web数据语料库中恢复命名实体边界。第一种方法使用boosting算法来解决排序问题。第二种方法使用投票感知器算法。与最大熵基线相比，这两种算法都有相当大的改进。投票感知器算法可以更有效地进行训练，但需要在测试示例上进行一些计算。我们描述了一个从单词到单词类型的映射，该映射将具有相似正交形式的单词分组到类中。"}
{"pid": "J93-1002", "zh_sum": "基于统一语法的自然语言（语料库）广义概率LR分析我们描述了基于LR分析技术构建覆盖范围非常广的自然语言概率分析系统（NL）的工作。该系统旨在根据每个分析中部署的单个规则的出现频率，对NL语法生成的大量语法分析进行排序。我们讨论了从基于统一的语法形式主义构造LR解析表的全自动过程，并考虑了替代LALR（1）解析表构造方法对大型语法的适用性。解析表用作两个解析器的基础；一种用户驱动的交互式系统，提供了一种计算上易于处理且劳动效率高的方法，对驱动概率解析器所需的统计信息进行监督培训。后者是通过将概率与LR解析表直接关联来构造的。这种技术优于基于概率词法标记或概率上下文无关语法的解析器，因为它允许更依赖上下文的概率语言模型，以及使用更适合语言学的语法形式。我们比较了Tomita（1987）广义LR解析算法的优化变体与（高效索引和优化的）图表解析器的性能。我们对《朗文当代英语词典》（LDOCE）中的150个名词定义进行了初步研究培训，并对这些名词定义和另外55个名词定义进行了重新测试，结果令人鼓舞。最后，我们讨论了当前系统的局限性以及处理词汇（句法和语义）出现频率的可能扩展。我们在统计分析方面的工作使用了一个经过调整的系统版本，该系统能够处理标记输入，忽略单词以分析标记序列。我们的统计解析器是ANLT语法开发系统的扩展。"}
{"pid": "P05-1077", "zh_sum": "随机化算法和NLP：使用对位置敏感的哈希函数进行高速名词聚类在本文中，我们探讨了随机化算法在处理大量数据方面的能力。我们应用这些算法从7000万页中生成名词相似性列表。我们将要计算的元素数量从二次型减少到实际线性型。我们证明，使用LSH最近邻计算可以在O（nd）时间内完成。我们的方法在提取同义词时可以产生70%以上的准确率。"}
{"pid": "P01-1019", "zh_sum": "基于约束的语法中语义构造的代数我们开发了一个框架，用于在类型化特征结构逻辑（包括HPSG）中表示的语法中形式化语义构造。该方法提供了lambda演算的替代方法；它在很大程度上保持了基于统一的组合方法所需的灵活性，同时限制了允许的操作，以捕获基本的泛化并提高可维护性。语义解释是使用最小递归语义（MRS）表示的，该语义提供了使用谓词演算和广义量词的术语用扁平的、未指定的语义表示解释的方法。MRS由一包带标签的基本谓词及其参数、一系列作用域约束和一对关系组成，这些关系为表示提供了一个挂钩—一个标签，它必须覆盖所有句柄，还有一个索引。"}
{"pid": "C00-1007", "zh_sum": "利用概率层次模型生成以前的随机生成方法不包括基于树的语法表示。虽然这对某些应用程序来说可能是足够的，甚至是有利的，但其他应用程序可以从使用尽可能多的语法知识中获益，只剩下那些不由语法决定的问题，而留给随机模型。我们给出的初步结果表明，基于树注释语料库的树模型改进了基于未注释语料库的树模型，基于树的随机模型和手工制作的语法都优于这两种模型。我们的系统FERGUS将依赖结构作为输入，并通过从带注释的语料库中自动获取的随机树模型生成XTAG派生。Fergus系统使用统计树模型来选择可能的树，并使用单词n-gram模型对从最佳树生成的字符串候选进行排序。"}
{"pid": "W00-0712", "zh_sum": "基于潜在语义分析的无知识形态学归纳形态学归纳是机器可读词典自动学习和语法归纳等重要任务的子问题。以前的形态学归纳方法仅仅依赖于假设词干和词缀的统计数据来选择哪些词缀是合法的。依赖词干和词缀统计数据而非语义知识会导致一些问题，例如有效词缀的不当使用（“ally”词干到“all”）。我们介绍了一种基于语义的形态学学习算法，该算法仅在词干和词干加词缀语义足够相似时才提出词缀。我们使用潜在语义分析实现了我们的方法，并表明我们的纯语义方法提供了与当前最先进的系统相媲美的形态学归纳结果。我们生成一个包含N个候选后缀的列表，并使用该列表来识别共享同一词干的单词对。我们试图从一个未定义的trie搜索开始对形态学相关的单词进行聚类，该搜索包含一个最小可能词干长度参数和一个潜在词缀候选上界，该上界受单词上下文向量空间中语义相似性的约束。"}
{"pid": "J08-2002", "zh_sum": "语义角色标注的全局联合模型我们提出了一个语义角色标注模型，该模型有效地捕捉了语义参数框架是一个联合结构的语言直觉，参数之间具有很强的依赖性。我们展示了如何将这些强依赖性合并到统计联合模型中，该模型在多个参数短语上具有丰富的特征集。所提出的模型大大优于类似的最先进的局部模型，该模型不包括不同参数之间的依赖关系。当使用正确的语法分析树作为输入时，以及当使用自动派生的分析树时，我们评估了在Propbank语料库中合并此联合信息的收益。Propbank上黄金标准解析树的所有参数的错误减少了24.1%，核心参数的错误减少了36.8%。对于自动解析树，所有参数和核心参数的错误减少分别为8.3%和10.3%。我们还展示了CoNLL 2005共享任务数据集的结果。此外，我们还探讨了如何考虑多重语法分析来处理解析器噪声和不确定性。我们提出了一个重新排序模型来联合学习SRL任务中多个成分的语义角色。"}
{"pid": "P06-2014", "zh_sum": "通过有区别的训练词对齐方法对词对齐进行软句法约束，可以确保它们的对齐与单语依赖树指定的短语保持衔接，从而获得有价值的指导。然而，这种硬约束也会排除正确的对齐，并且随着对齐模型变得更加复杂，其效用会降低。我们使用一个公开的结构化输出SVM来创建一个具有软内聚约束的最大裕度语法对齐器。据我们所知，生成的对齐器是第一个使用判别学习方法来训练ITG位文本解析器的对齐器。我们使用依赖结构作为软约束来改进ITG框架中的单词对齐。我们将软语法ITG（Wu，1997）约束引入到判别模型中，并使用ITG解析器来约束对维特比对齐的搜索。"}
{"pid": "W99-0604", "zh_sum": "统计机器翻译的改进对齐模型本文描述了统计机器翻译的改进对齐模型。统计翻译方法使用两种类型的信息：翻译模型和语言模型。所使用的语言模型是二元图或通用m-gram模型。翻译模型被分解为词法模型和对齐模型。我们描述了两种不同的统计翻译方法，并给出了实验结果。第一种方法基于单个单词之间的依赖关系，第二种方法明确考虑了浅短语结构，使用了两种不同的对齐级别：短语之间的短语级对齐和单个单词之间的单词级对齐。我们使用Verbmobil任务（德语-英语，6000单词词汇）呈现结果，这是一项有限领域的口语任务。对文本转录和语音识别器输出进行了实验测试。为了获得最佳的单一对齐，我们使用一种后期算法来合并方向对齐。我们提出了一种启发式方法，其中所有对齐的短语对（x？，a？，y？）提取满足以下条件：（1）x？你呢？由x和y的连续单词组成，且两个单词的长度最多为k，（2）a？x的单词之间是否对齐？你呢？由a、（3）a诱发？包含至少一个链接，并且（4）a中没有只有一端在x中的链接？还是y？。"}
{"pid": "P02-1031", "zh_sum": "谓语变元识别的句法分析的必要性用语义角色或变元结构信息注释的广泛覆盖语料库首次变得可用。统计系统经过训练，可以从统计解析器输出的未注文本中自动标记语义角色。在本文中，我们量化了解析器准确性对这些系统性能的影响，并研究了更平坦的“分块”输入表示是否可以同样有效地用于语义角色识别的问题。我们注意到，这种深层句法特征在连接语义角色和表层语法功能方面起着重要作用。我们对这组特征进行了实验：Pred-HW、Arg-HW、短语类型、位置、路径、语音。"}
{"pid": "W00-1401", "zh_sum": "某些发电应用程序的评估指标可能会从使用随机方法中获益。在开发随机方法时，能够快速评估不同方法或模型的相对优点至关重要。在本文中，我们提出了几种用于基线定量评估的内在（系统内部）指标。然后，应将这种定量评估扩大到更全面的评估，以检查定性方面。为此，我们描述了一个测试定量指标和人类定性判断之间相关性的实验。实验证实，内在指标不能取代人类的评估，但有些指标与人类对质量和可理解性的判断显著相关，可用于开发过程中的评估。我们提出简单字符串精度作为自然语言生成的基线评估指标。"}
{"pid": "P99-1042", "zh_sum": "深度阅读：阅读理解系统本文介绍了深度阅读的初步工作，深度阅读是一种自动阅读理解系统，它接受任意文本输入（故事）并回答有关问题。我们已经获得了60个开发语料库和60个三至六年级材料的测试故事；每个故事后面都有简短的回答问题（还提供了答案键）。我们使用这些来构建和评估一个基线系统，该系统使用模式匹配（单词包）技术，并辅以额外的自动语言处理（词干提取、名称识别、语义类识别和代词解析）。这个简单的系统在30-40%的时间里检索包含答案的句子。我们使用了一种词汇统计袋的方法，将问题与故事中词汇上最相似的句子进行匹配。"}
{"pid": "J08-2005", "zh_sum": "句法分析和推理在语义角色标注中的重要性我们提出了一个语义角色标注的通用框架。该框架将机器学习技术与基于整数线性规划的推理过程相结合，将语言和结构约束纳入全局决策过程。在此框架下，我们研究了句法分析信息在语义角色标注中的作用。我们表明，到目前为止，完整的语法分析信息在识别参数方面最为相关，尤其是在第一阶段修剪阶段。令人惊讶的是，修剪阶段的质量不能完全根据其召回率和精确度来确定。相反，它取决于输出候选的特性，这些特性决定了下游问题的难度。基于这一观察结果，我们提出了一种通过联合推理将不同语义角色标记系统组合起来的有效且简单的方法，这显著提高了其性能。我们的系统已经在CoNLL-2005语义角色标注共享任务中进行了评估，并在19名参与者中获得了最高的F1分数。动词SRL系统包括四个阶段：候选生成、论元识别、论元分类和推理。"}
{"pid": "D12-1133", "zh_sum": "基于转换的词性标注和标记非投影依赖分析系统大多数当前的依赖分析程序都假定在分析开始之前，使用词性标注器在形态学上消除了输入单词的歧义。我们提出了一个基于转换的非投影树词性标注和标记依赖分析联合系统。对汉语、捷克语、英语和德语的实验评估表明，与管道系统相比，标记和语法分析的准确性都有了持续的提高，从而改善了所有语言的最新结果。我们介绍了一个基于转换的系统，该系统可以联合执行词性标记和依赖性分析。"}
{"pid": "N03-1024", "zh_sum": "基于语法的多重翻译对齐：提取意译并生成新句子我们描述了一种基于语法的算法，该算法可以从语义等价的翻译集自动构建有限状态自动机（词格）。这些FSA很好地代表了释义。它们可以用来提取词汇和句法释义对，并生成新的、看不见的句子，这些句子表达的含义与输入集中的句子相同。我们的FSA还可以预测替代语义呈现的正确性，这可以用来评估翻译的质量。我们描述了一种基于语法的算法，该算法从平行翻译中构建单词格，可用于生成新的para短语。我们提出了一种完全由句子的句法表示驱动的对齐平行句子集的算法。"}
{"pid": "W01-1313", "zh_sum": "为事件从句分配时间戳我们描述了一种将描述某种情况发展的新闻故事的内容排列成时间线的过程。我们描述了系统中处理1的部分。将句子分成事件从句和2。解析显式和隐式时间引用。评估显示，与人类相比，其表现为52%。我们根据文章日期的最新指定日期推断时间值。"}
{"pid": "P05-1012", "zh_sum": "依赖关系分析器的在线大幅度训练我们为线性评分依赖关系分析器提供了一种有效的训练算法，该算法在依赖树的有效解析技术的基础上，实现了在线大幅度多类训练（Crammer和Singer，2003；Crammer et al.，2003）（Eisner，1996）。经过培训的解析器在没有特定语言增强的情况下，对英语和捷克语都能达到有竞争力的依赖准确性。我们已经实现了时间复杂度为O（n3）且没有语法常量的解析器。我们使用每个词形的前缀，而不是词形本身作为特征。我们的依赖关系解析器实现了与Charniak（2000）一样好的精度，速度比Collins（1997）快十倍，比Charniak（2000）快四倍。"}
{"pid": "J92-4003", "zh_sum": "基于类的自然语言N-Gram模型我们解决了从文本样本中的先前单词预测单词的问题。特别地，我们讨论了基于词类的n-gram模型。我们还讨论了根据单词与其他单词共现的频率将单词分配给类的几种统计算法。我们发现，我们能够提取具有基于语法分组或基于语义分组风格的类，这取决于基础统计数据的性质。我们提出了一种窗口方法，将两个单词的语义粘性概念引入其中，作为它们之间相对频繁的紧密出现（距离小于500个单词）。"}
{"pid": "W03-0501", "zh_sum": "Hedge Trimmer：标题生成的解析和修剪方法本文介绍了Hedge Trimmer，这是一个标题生成系统，它使用语言激励的启发式方法为报纸故事创建标题，以指导潜在标题的选择。我们提出了可行性测试，用于确定通过从故事中按顺序选择单词来构建标题的方法的有效性。此外，我们还描述了实验结果，证明了我们的语言激励方法在基于HMM的模型上的有效性，使用人类评估和自动度量来比较这两种方法。我们的方法侧重于从文档中提取一个或两个信息性句子，并对其进行语言转换，以减少摘要长度。"}
{"pid": "P02-1038", "zh_sum": "统计机器翻译的判别训练和最大熵模型我们提出了一个基于直接最大熵模型的自然语言统计机器翻译框架，其中包含了广泛使用的源-通道方法作为特例。所有知识源都被视为特征函数，它依赖于源语言句子、目标语言句子和可能的隐藏变量。这种方法允许通过添加新的特征函数轻松扩展基线机器翻译系统。我们表明，使用这种方法可以显著改进基线统计机器翻译系统。"}
{"pid": "P01-1017", "zh_sum": "语言模型的即时头解析我们提出了两种基于“即时头”解析器的语言模型，我们称之为一种解析器，它将所有事件条件化为c开头的成分c之下。虽然所有最准确的统计解析器都是即时头类型，但之前的语法语言模型都没有使用这种技术。这两种模型的复杂性在三元模型基线以及之前最好的基于语法的语言模型的基础上得到了显著改善。对于我们的两个模型中更好的一个，这些改进分别为24%和14%。我们还建议，对底层解析器的改进应该会显著改善模型的复杂性，而且即使在短期内，直接头语言模型也有很大的改进潜力。该模型识别了有助于语言建模的语法结构和词汇依赖关系。这些上下文包括句法结构，如父母和祖父母类别标签，以及词汇项目，如父母或兄弟组成部分的头部。"}
{"pid": "P98-1106", "zh_sum": "伪射影性一种多项式可分解的非射影依赖语法我们提出的伪射影语法可以在多项式时间内进行解析，并通过一种间隙线程捕获非局部依赖，但该语法生成的结构是严格射影的。"}
{"pid": "P05-1015", "zh_sum": "看星星：利用阶级关系对评级量表进行情感分类，我们解决了评级推理问题，其中，与之前的情感分析工作一样，我们不能简单地决定评论是竖起大拇指还是竖起大拇指，而必须确定作者对多点量表的评价（例如，一到五颗“星星”）。这个任务代表了标准多类文本分类的一个有趣的转折，因为类标签之间有几个不同程度的相似性；例如，从直觉上看，三颗星比一颗星更接近四颗星。我们首先评估任务中的人的绩效。然后，我们应用一种基于问题度量标签公式的元算法，该算法改变给定n元分类器的输出，以明确尝试确保相似项目接收相似标签。我们表明，当我们采用一种新的适合该问题的相似性度量时，元算法可以比支持向量机的多类和回归版本都有显著的改进。我们创建了一个由电影评论组成的情感注释数据集，以训练分类器来识别全长评论中的积极句子。"}
{"pid": "J04-2003", "zh_sum": "在统计机器翻译中，源语言和目标语言中的单词之间的对应关系是从平行语料库中学习的，通常很少或根本没有使用语言知识来构建底层模型。特别是，现有的机器翻译统计系统通常将同一引理的不同屈折形式视为彼此独立。通过明确考虑相关屈折形式的相互依赖性，可以更好地利用双语培训数据。我们提出了基于词的等价类构建分层词典模型。此外，我们还介绍了句子层次的重组转换，旨在同化相关句子中的词序。我们系统地调查了维持机器翻译质量所需的双语培训数据量。在资源匮乏的框架中，我们成功地测试了所建议的提高翻译质量的方法的组合：我们能够将双语培训数据量减少到原始语料库的10%以下，而翻译质量仅损失1.6%。从Verbmobil任务和Nespole！两个德语-英语语料库中可以看出翻译结果的改进！任务我们使用引理和形态标记将德语单词分解为层次表示，并使用MaxEnt模型将翻译模型中不同层次的表示结合起来。我们描述了一种结合德语中形态分裂动词的方法，并对英语和德语中的问题重新排序。"}
{"pid": "W93-0301", "zh_sum": "用于机器辅助翻译的强大双语单词对齐我们开发了一个名为Word\\u align的新程序，用于对齐平行文本，例如两种或多种语言的加拿大汉莎词典。该程序采用char\\u align（Church，1993）的输出，char\\u align是基于句子的对齐程序的一种稳健替代方案，并使用Brown el al.的模型2（Brown et al.，1993）版本应用单词级约束，该模型经过修改和扩展以处理稳健问题。Word\\u align是在Simard提供的加拿大Hansard的子集上进行测试的（Simard等人，1992年）。word\\u align加char\\u align的组合将方差（平均平方误差）减少了5倍，而char\\u align单独使用。更重要的是，由于word\\u align和char\\u align设计用于处理比Hansards更小、噪音更大的文本，因此有可能在商业翻译服务at&T Language Line Services成功部署这些程序，以帮助他们解决术语难题。我们表明，目标文本长度的知识对模型的性能并不重要。"}
{"pid": "P06-1005", "zh_sum": "我们提出了一种基于句法路径的代词消解方法。通过一个简单的自举过程，我们根据两个实体之间的解析树中的路径来了解代词和候选名词之间共指的可能性。该路径信息使我们能够处理以前具有挑战性的解析实例，并且还能够有力地解决传统的语法共指约束。高度相关的路径还允许挖掘精确的概率性别/数字信息。在支持向量机代词分辨分类器中，我们将统计知识与已知特征相结合。在多个数据集上观察到性能显著提高。给定一个自动解析的语料库，我们从每个解析树中提取一条依赖路径，该路径表示为连接代词和候选先行词的一系列节点和依赖标签，并从这些路径中收集统计信息，以确定由给定路径连接的代词和候选先行词是否相互关联。我们发现，学习的性别是他们代词分辨系统中最重要的特征。我们实现了最先进的名词性别分类性能，并将获得的名词性别数据库在线提供。我们从包含中间代词引理的路径构建了一个统计模型，但将名词、代词和反身代词的末端节点分别替换为名词、代词或代词自身。"}
{"pid": "H05-1079", "zh_sum": "用逻辑推理识别篇章蕴涵我们使用逻辑推理技术来识别篇章蕴涵。由于定理证明的性能高度依赖于不易获得的背景知识，我们结合了从自动推理中借用的建模技术，并证明了它是一种有效的鲁棒性方法来近似蕴涵。最后，我们使用机器学习将这些深层语义分析技术与简单的浅层词重叠相结合；鉴于目前的技术水平，得到的混合模型在RTE测试集上实现了高精度。我们的结果还表明，我们采用的不同技术在RTE语料库的某些子集上的表现非常不同，因此，将数据集的性质用作特征是有用的。通常情况下，缺乏足够的语言知识会导致推理失败，因此系统几乎对所有对都输出“无蕴涵”。我们的系统基于逻辑表示和自动定理证明，但仅使用WordNet（Fellbaum，1998）作为词汇知识资源。"}
{"pid": "W97-0109", "zh_sum": "基于语料库的基于语义词典的PP连接歧义消解本文研究了自然语言中两个重要的歧义：介词短语连接和词义歧义。我们提出了一种新的基于语义标记语料库的PP-依恋监督学习方法。由于不存在足够大的词义标注语料库，我们还提出了一种新的基于无监督上下文的词义消歧算法，该算法通过词义标注对PP连接的训练语料库进行修正。我们介绍了我们的方法的结果，并与其他方法进行了比较，评估了所获得的PP附着精度。我们开发了一种定制的显式WSD算法，作为其决策树系统的一部分。"}
